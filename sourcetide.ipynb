{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO6E1b7EIhKXZewNLGcjy+F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monacofj/sourcetide/blob/main/sourcetide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SourceTide - Analytics for collaborative development dynamics**\n",
        "---\n",
        "*Copyright (c) 2025, Monaco F. J. <monaco@usp.br> </br>\n",
        "This is free software distributed under the GNU General Public License vr. 3.0.*\n",
        "\n",
        "*Open Source/Science, CCOS-ICMC - University of São Paulo.*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4vaGsuiC5L-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# USER SETUP"
      ],
      "metadata": {
        "id": "4OT7pykvIwBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Repository"
      ],
      "metadata": {
        "id": "OEeLJm0UfNQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cellname : which-repo\n",
        "\n",
        "# Repository identification\n",
        "\n",
        "owner = \"fossguild\"  # <--- Replace with the actual owner username\n",
        "repository_name = \"naja\"  # <--- Replace with the actual repository name\n",
        "\n",
        "# GitHub Token\n",
        "#\n",
        "# To access the GitHub API, you'll need a personal access token. You can\n",
        "# generate one in your GitHub account settings under \"Developer settings\" >\n",
        "# \"Personal  access tokens\". Grant the token the necessary permissions\n",
        "# (e.g., `repo` scope for full control or more specific scopes like\n",
        "# `public_repo` for public repositories). In Colab, you can store your token\n",
        "# securely in the secrets manager under the key icon in the left panel.\n",
        "# Add a new secret with the name `GITHUB_TOKEN` and paste your token as\n",
        "# the value.\n"
      ],
      "metadata": {
        "id": "borUqKOx6vVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Period"
      ],
      "metadata": {
        "id": "pbsIQx1ZImvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cellname : period\n",
        "\n",
        "# Choose the period of analysis in weeks.\n",
        "\n",
        "num_weeks = 1"
      ],
      "metadata": {
        "id": "CFlxD0ttHmNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report"
      ],
      "metadata": {
        "id": "hrWp8pFDa4N0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cellname : report\n",
        "\n",
        "# Should an HTML report file be generated?\n",
        "# Answer with 'y', 'Y' (for Yes) or 'n', 'N' (for No)\n",
        "\n",
        "generate_report_value = 'y'\n",
        "\n",
        "# Base path where the report folder will be saved (Google Drive)\n",
        "report_path = 'My Drive/'\n"
      ],
      "metadata": {
        "id": "OnyMCQQSVi00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80e4097c"
      },
      "source": [
        "try:\n",
        "    repo = g.get_user(owner).get_repo(repository_name)\n",
        "    print(f\"Repository Name: {repo.name}\")\n",
        "    print(f\"Repository Description: {repo.description}\")\n",
        "    print(f\"Repository URL: {repo.html_url}\")\n",
        "    print(f\"Stars: {repo.stargazers_count}\")\n",
        "    print(f\"Forks: {repo.forks_count}\")\n",
        "    print(f\"Created At: {repo.created_at}\")\n",
        "    print(f\"Last Updated At: {repo.updated_at}\")\n",
        "\n",
        "    # The analysis_run_date is now defined in the 'Analysis Run Date' cell in USER SETUP.\n",
        "    # This cell will only print it.\n",
        "    print(f\"Analysis Run Date: {analysis_run_date}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error fetching repository information: {e}\")\n",
        "    # If you see an error here AFTER replacing the owner and repository_name,\n",
        "    # please copy and paste the full error message so I can assist you.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f69907cc"
      },
      "source": [
        "## Análise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "686c6fdb"
      },
      "source": [
        "# cellname : analysis-date\n",
        "from datetime import datetime\n",
        "\n",
        "# Define the single source of truth for the analysis run date\n",
        "analysis_run_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "print(f\"Analysis started at: {analysis_run_date}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Score"
      ],
      "metadata": {
        "id": "ZzMiAeA1c1GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cellname : score-parameters\n",
        "\n",
        "## Chosse the parameters to be used to compute the contribution score\n",
        "##\n",
        "## activity = A * (REMI>0)   +   (1-A) (RDMI>0)\n",
        "## reward   = R *  REMI      +   (1-R)  RDMI\n",
        "## score    = S * activity   +   (1-S)  reward\n",
        "##\n",
        "## Interpretation:\n",
        "##\n",
        "## * activity a binary weighted measure of minimal participation in both\n",
        "##            discussions and coding. The parameter A says how much each\n",
        "##            dimension is important: A=1 means only discussion counts;\n",
        "##            A=0 means only coding counts; A=0.5 both count equally.\n",
        "##            Activity varies from 0 to 1.\n",
        "##\n",
        "##  * reward  is a weighted measure of participation in both discussion\n",
        "##            and coding. The parameter R says how much each dimension is\n",
        "##            important: R=0 means only discussion counts; R=1 means only\n",
        "##            coding counts; R=0.5 both count equally.\n",
        "##            Reward valies from 0 to 1.\n",
        "##\n",
        "##  * score   is a weighted combination of activity and reward. The parameter\n",
        "##            S says which is more imporant: S=1 means that we're exclusively\n",
        "##            interested in the fact that there was any participation, and\n",
        "##            not in how intensive this participation was. S=0 means that the\n",
        "##            score is proportional to the participation intensity. S=0.5\n",
        "##            mean that half of the score is based on whether was any\n",
        "##            participation, and the other half is a bonification for the\n",
        "##            intensity of the work done.\n",
        "\n",
        "A_factor = 0.5\n",
        "R_factor = 0.5\n",
        "S_factor = 0.5\n"
      ],
      "metadata": {
        "id": "j4v_PomPLUcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONFIGURE TOOLS"
      ],
      "metadata": {
        "id": "s2db-k-U6qNk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idaGUgnAUmab"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import ipywidgets as widgets # Import ipywidgets to access the selector\n",
        "from IPython.display import display, Markdown\n",
        "from datetime import datetime\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Define the report_folder_name using repository_name and analysis_run_date\n",
        "# repository_name is from the 'which-repo' cell (borUqKOx6vVf)\n",
        "# analysis_run_date is from the 'Get repository info' cell (-zplV57Ea4LY)\n",
        "\n",
        "# Ensure analysis_run_date is available (defined in -zplV57Ea4LY)\n",
        "if 'analysis_run_date' not in globals():\n",
        "    # Fallback if analysis_run_date hasn't been set yet\n",
        "    current_date_str = datetime.now().strftime('%Y-%m-%d')\n",
        "else:\n",
        "    # Use the date from the analysis, formatted for folder names\n",
        "    # analysis_run_date is already a string '%Y-%m-%d %H:%M:%S', reformat for folder name\n",
        "    current_date_obj = datetime.strptime(analysis_run_date, '%Y-%m-%d %H:%M:%S')\n",
        "    current_date_str = current_date_obj.strftime('%Y-%m-%d')\n",
        "\n",
        "report_folder_name = f\"{repository_name}-{current_date_str}\"\n",
        "\n",
        "# Define the output folder path using report_path (from 'report' cell) and report_folder_name\n",
        "# Ensure report_path is defined in the 'report' cell (OnyMCQQSVi00)\n",
        "output_folder = os.path.join('/content/gdrive', report_path, report_folder_name)\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "print(f\"Output folder '{output_folder}' ensured to exist.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b498cb1c"
      },
      "source": [
        "# cellname : prepare-tools\n",
        "\n",
        "!pip --quiet install PyGithub\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a-Sh5RzUtFc"
      },
      "source": [
        "import plotly.io as pio\n",
        "import os\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "def save_plotly_figure_as_html(fig, filename):\n",
        "    \"\"\"\n",
        "    Saves a Plotly figure as an interactive HTML file in the designated output folder.\n",
        "\n",
        "    Args:\n",
        "        fig (plotly.graph_objects.Figure): The Plotly figure object to save.\n",
        "        filename (str): The desired filename (e.g., 'my_plot.html').\n",
        "    \"\"\"\n",
        "    global output_folder\n",
        "\n",
        "    # Ensure filename has .html extension\n",
        "    if not filename.lower().endswith('.html'):\n",
        "        filename = os.path.splitext(filename)[0] + '.html'\n",
        "\n",
        "    full_path = os.path.join(output_folder, filename)\n",
        "    pio.write_html(fig, full_path, auto_open=False) # auto_open=False to prevent browser from opening\n",
        "    print(f\"Figure saved to: {full_path}\")\n",
        "\n",
        "\n",
        "def save_fig(fig, filename):\n",
        "    \"\"\"\n",
        "    Conditionally saves a Plotly figure as an interactive HTML file\n",
        "    if report generation is enabled via the 'generate_report_value' variable.\n",
        "\n",
        "    Args:\n",
        "        fig (plotly.graph_objects.Figure): The Plotly figure object to save.\n",
        "        filename (str): The desired filename (e.g., 'my_plot.html').\n",
        "    \"\"\"\n",
        "    # Access the global variable 'generate_report_value' directly\n",
        "    if 'generate_report_value' in globals() and generate_report_value.lower() == 'y':\n",
        "        save_plotly_figure_as_html(fig, filename)\n",
        "    else:\n",
        "        print(f\"Report generation is not enabled. Skipping saving '{filename}'.\")\n",
        "\n",
        "print(\"Function 'save_fig' defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47bf3e6b"
      },
      "source": [
        "from datetime import timedelta\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Calculate the corresponding timedelta from num_weeks (defined in the 'period' cell)\n",
        "selected_timedelta = timedelta(weeks=num_weeks)\n",
        "\n",
        "# Create a string representation for the report\n",
        "selected_period_str = f'{num_weeks} week' if num_weeks == 1 else f'{num_weeks} weeks'\n",
        "\n",
        "# Display the selected period for confirmation\n",
        "display(Markdown(f'**Analysis Period: {selected_period_str}**'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83aa36a9"
      },
      "source": [
        "# cellname : token\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from github import Github\n",
        "\n",
        "# Retrieve the GitHub token from Colab secrets\n",
        "github_token = userdata.get('GITHUB_TOKEN')\n",
        "\n",
        "if github_token is None:\n",
        "    print(\"GitHub token not found. Please add it to Colab secrets with the name 'GITHUB_TOKEN'.\")\n",
        "else:\n",
        "    # Authenticate with the GitHub API\n",
        "    g = Github(github_token)\n",
        "    print(\"Successfully authenticated with GitHub API.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FETCH DEVELOPERS"
      ],
      "metadata": {
        "id": "uR0938TCI1RY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get repository info"
      ],
      "metadata": {
        "id": "HnFYV5TsfWld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    repo = g.get_user(owner).get_repo(repository_name)\n",
        "    print(f\"Repository Name: {repo.name}\")\n",
        "    print(f\"Repository Description: {repo.description}\")\n",
        "    print(f\"Repository URL: {repo.html_url}\")\n",
        "    print(f\"Stars: {repo.stargazers_count}\")\n",
        "    print(f\"Forks: {repo.forks_count}\")\n",
        "    print(f\"Created At: {repo.created_at}\")\n",
        "    print(f\"Last Updated At: {repo.updated_at}\")\n",
        "\n",
        "    # Capture and print the current analysis run date\n",
        "    from datetime import datetime # Ensure datetime is imported\n",
        "    analysis_run_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    print(f\"Analysis Run Date: {analysis_run_date}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error fetching repository information: {e}\")\n",
        "    # If you see an error here AFTER replacing the owner and repository_name,\n",
        "    # please copy and paste the full error message so I can assist you."
      ],
      "metadata": {
        "id": "-zplV57Ea4LY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07e973f9"
      },
      "source": [
        "## Get contributors\n",
        "All people that contributed to the project, even if not with repository access (i.e. including external contributors)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eab2221"
      },
      "source": [
        "raw_contributors = repo.get_contributors() # Fetch all contributors first\n",
        "\n",
        "filtered_contributors = []\n",
        "bot_logins_to_exclude = ['Copilot'] # Explicitly exclude Copilot as it caused 404 in previous cells\n",
        "\n",
        "for contributor in raw_contributors:\n",
        "    # Check if contributor is a bot based on type or common naming conventions\n",
        "    is_bot = (\n",
        "        contributor.type == 'Bot' or\n",
        "        contributor.login.endswith('[bot]') or\n",
        "        contributor.login in bot_logins_to_exclude\n",
        "    )\n",
        "\n",
        "    if not is_bot:\n",
        "        filtered_contributors.append(contributor)\n",
        "\n",
        "all_contributors = filtered_contributors # Update all_contributors with the filtered list\n",
        "\n",
        "print(f\"Found {len(all_contributors)} non-bot contributors.\")\n",
        "\n",
        "# Display the login, name, and number of contributions for the first few non-bot contributors\n",
        "print(\"\\nFirst 5 non-bot contributors:\")\n",
        "for i, contributor in enumerate(all_contributors):\n",
        "    if i >= 5:\n",
        "        break\n",
        "    # Get contributor name, handling cases where it might be None\n",
        "    contributor_name = contributor.name if contributor.name else \"N/A\"\n",
        "    # Use f-string formatting to align output\n",
        "    print(f\"- {contributor.login:<20}  {contributor_name:<30}    Contributions: {contributor.contributions}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d134c9a"
      },
      "source": [
        "## Get collaborators\n",
        "This is a list of everyone with access to the repostory, even those who never effectively contributed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5e2d235"
      },
      "source": [
        "collaborators = repo.get_collaborators()\n",
        "\n",
        "print(f\"Found {collaborators.totalCount} collaborators with access to the repository.\")\n",
        "\n",
        "print(\"\\nFirst 5 collaborators:\")\n",
        "for i, collaborator in enumerate(collaborators):\n",
        "    if i >= 5:\n",
        "        break\n",
        "    # Get collaborator name, handling cases where it might be None\n",
        "    collaborator_name = collaborator.name if collaborator.name else \"N/A\"\n",
        "\n",
        "    permissions_list = []\n",
        "    if collaborator.permissions.admin: permissions_list.append('admin')\n",
        "    if collaborator.permissions.push: permissions_list.append('push')\n",
        "    if collaborator.permissions.pull: permissions_list.append('pull')\n",
        "\n",
        "    print(f\"- {collaborator.login:<20}  Name: {collaborator_name:<30}  Permissions: {', '.join(permissions_list) if permissions_list else 'None'}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consolidate developers\n",
        "Create a dataframe with all collaborators (people with repository access), plus anyone else that has contributed (external contributors)."
      ],
      "metadata": {
        "id": "ni91UrakXzTx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0ac2a4c"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- 1. Prepare base information for all unique individuals (collaborators + external contributors) ---\n",
        "\n",
        "# Dictionary to store basic details for each unique person\n",
        "# Key: login (str), Value: {'name': str, 'access': str}\n",
        "person_base_info = {}\n",
        "\n",
        "# First, add all collaborators with 'yes' access status\n",
        "# This also populates their names\n",
        "collaborator_logins = set()\n",
        "for coll in collaborators:\n",
        "    collaborator_logins.add(coll.login)\n",
        "    person_base_info[coll.login] = {\n",
        "        'name': coll.name if coll.name else 'N/A',\n",
        "        'access': 'yes'\n",
        "    }\n",
        "\n",
        "# Now, add all_contributors who are NOT collaborators with 'no' access status\n",
        "for contr in all_contributors:\n",
        "    if contr.login not in person_base_info:\n",
        "        # This person is a contributor but not a direct collaborator\n",
        "        person_base_info[contr.login] = {\n",
        "            'name': contr.name if contr.name else 'N/A',\n",
        "            'access': 'no'\n",
        "        }\n",
        "    else:\n",
        "        # If a person is both a contributor and a collaborator, ensure their name is captured\n",
        "        # in case the contributor object had a better name than the collaborator object\n",
        "        if person_base_info[contr.login]['name'] == 'N/A' and contr.name:\n",
        "            person_base_info[contr.login]['name'] = contr.name\n",
        "\n",
        "# Convert this base info to a DataFrame\n",
        "df_people = pd.DataFrame.from_dict(person_base_info, orient='index')\n",
        "df_people.index.name = 'github_login'\n",
        "df_people = df_people.reset_index()\n",
        "\n",
        "print(\"Unified DataFrame of Collaborators and External Contributors:\")\n",
        "display(df_people.head())\n",
        "\n",
        "#print(\"\\nDataFrame Info:\")\n",
        "#df_people.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FETCH EVENTS"
      ],
      "metadata": {
        "id": "BvkieIyoJLuK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eb05bee"
      },
      "source": [
        "from datetime import timedelta\n",
        "\n",
        "# selected_timedelta is now calculated directly in the 'period' cell.\n",
        "# This cell now just confirms the value.\n",
        "\n",
        "print(f\"Corresponding timedelta for analysis: {selected_timedelta}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaef677d"
      },
      "source": [
        "num_core_devs = df_people[df_people['access'] == 'yes'].shape[0]\n",
        "num_externals = df_people[df_people['access'] == 'no'].shape[0]\n",
        "\n",
        "print(f\"* Number of collaborators (core devs): {num_core_devs}\")\n",
        "print(f\"* Number of contributors without repo access (externals): {num_externals}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cellname : here1\n",
        "\n",
        "# Initialize dictionaries for daily event counts\n",
        "daily_issues_raised_count = {}\n",
        "daily_issue_comment_count = {}\n",
        "daily_prs_opened_count = {}\n",
        "daily_prs_merged_count = {}\n",
        "daily_prs_closed_count = {}\n",
        "daily_additions = {}\n",
        "daily_deletions = {}"
      ],
      "metadata": {
        "id": "o88l1AcNLGJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Issues\n",
        "\n"
      ],
      "metadata": {
        "id": "6jRBVB1BfafK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd46d806"
      },
      "source": [
        "## Get issues open\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from github import Github\n",
        "from datetime import datetime, timedelta, timezone\n",
        "import pandas as pd\n",
        "\n",
        "# Calculate the start date based on the selected_timedelta\n",
        "# Ensure selected_timedelta is available from previous cells\n",
        "start_date = datetime.now(timezone.utc) - selected_timedelta\n",
        "\n",
        "# Dictionary to store issues opened count per user\n",
        "issues_opened_count = {}\n",
        "\n",
        "# Fetch all issues since the start_date. This fetches issues CREATED or UPDATED since start_date.\n",
        "issues = repo.get_issues(state='all', since=start_date)\n",
        "\n",
        "for issue in issues:\n",
        "    # Only count issues that were CREATED within the selected period\n",
        "    if issue.user and issue.created_at >= start_date:\n",
        "        login = issue.user.login\n",
        "        issues_opened_count[login] = issues_opened_count.get(login, 0) + 1\n",
        "\n",
        "        # NEW: Populate daily_issues_raised_count\n",
        "        date_key = issue.created_at.date()\n",
        "        daily_issues_raised_count[date_key] = daily_issues_raised_count.get(date_key, 0) + 1\n",
        "\n",
        "# Convert the dictionary to a pandas Series for merging\n",
        "issues_opened_series = pd.Series(issues_opened_count, name='issues_opened_count')\n",
        "\n",
        "# --- Fix for KeyError: Drop existing 'issues_opened_count' related columns before merging ---\n",
        "columns_to_drop = [col for col in df_people.columns if col.startswith('issues_opened_count')]\n",
        "if columns_to_drop:\n",
        "    df_people = df_people.drop(columns=columns_to_drop)\n",
        "\n",
        "df_people = df_people.set_index('github_login')\n",
        "df_people = df_people.merge(issues_opened_series, left_index=True, right_index=True, how='left')\n",
        "df_people = df_people.reset_index()\n",
        "\n",
        "# Fill NaN values with 0 for users who didn't open any issues in the period\n",
        "df_people['issues_opened_count'] = df_people['issues_opened_count'].fillna(0).astype(int)\n",
        "\n",
        "print(\"df_people DataFrame with 'issues_opened_count' column:\")\n",
        "display(df_people.head())\n",
        "\n",
        "print(\"\\nTop 4 people with most issues opened:\")\n",
        "top_issues_openers = df_people.sort_values(by='issues_opened_count', ascending=False).head(4)\n",
        "for index, row in top_issues_openers.iterrows():\n",
        "    print(f\"- {row['github_login']} ({row['name'] if row['name'] != 'N/A' else 'Name not available'}): {row['issues_opened_count']} issues opened\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57d02691"
      },
      "source": [
        "## Get issues reopened\n",
        "\n",
        "from datetime import datetime, timezone\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure selected_timedelta is available from previous cells\n",
        "start_date = datetime.now(timezone.utc) - selected_timedelta\n",
        "\n",
        "# Dictionary to store issues reopened count per user\n",
        "issues_reopened_count = {}\n",
        "\n",
        "# Fetch all issues that were created or updated since the start_date\n",
        "# This gives us a pool of issues that might have been reopened in the period\n",
        "issues_in_period = repo.get_issues(state='all', since=start_date)\n",
        "\n",
        "for issue in issues_in_period:\n",
        "    try:\n",
        "        # Get events for this specific issue (timeline events)\n",
        "        issue_timeline_events = issue.get_events()\n",
        "        for event in issue_timeline_events:\n",
        "            # Check if the event is a 'reopened' event, happened in the period, and has an actor\n",
        "            if event.event == 'reopened' and event.created_at >= start_date and event.actor:\n",
        "                login = event.actor.login\n",
        "                issues_reopened_count[login] = issues_reopened_count.get(login, 0) + 1\n",
        "\n",
        "                # NEW: Populate daily_issues_raised_count\n",
        "                date_key = event.created_at.date()\n",
        "                daily_issues_raised_count[date_key] = daily_issues_raised_count.get(date_key, 0) + 1\n",
        "    except Exception as e:\n",
        "        # Handle potential errors if issue events cannot be fetched (e.g., rate limits, deleted events)\n",
        "        print(f\"Warning: Could not fetch events for issue #{issue.number}: {e}\")\n",
        "\n",
        "# Convert the dictionary to a pandas Series for merging\n",
        "issues_reopened_series = pd.Series(issues_reopened_count, name='issues_reopened_count')\n",
        "\n",
        "# Merge the counts into df_people\n",
        "df_people = df_people.set_index('github_login')\n",
        "df_people = df_people.merge(issues_reopened_series, left_index=True, right_index=True, how='left')\n",
        "df_people = df_people.reset_index()\n",
        "\n",
        "# Fill NaN values with 0 for users who didn't reopen any issues in the period\n",
        "df_people['issues_reopened_count'] = df_people['issues_reopened_count'].fillna(0).astype(int)\n",
        "\n",
        "print(\"df_people DataFrame with 'issues_reopened_count' column:\")\n",
        "display(df_people.head())\n",
        "\n",
        "print(\"\\nTop 4 people with most issues reopened:\")\n",
        "top_reopeners = df_people.sort_values(by='issues_reopened_count', ascending=False).head(4)\n",
        "for index, row in top_reopeners.iterrows():\n",
        "    print(f\"- {row['github_login']} ({row['name'] if row['name'] != 'N/A' else 'Name not available'}): {row['issues_reopened_count']} issues reopened\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comments\n",
        "\n",
        "PR are treated as issues with code."
      ],
      "metadata": {
        "id": "HiYtDtHwgUYV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a826c89"
      },
      "source": [
        "from datetime import datetime, timezone\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure selected_timedelta is available from previous cells\n",
        "start_date = datetime.now(timezone.utc) - selected_timedelta\n",
        "\n",
        "# Dictionary to store IssueCommentEvent counts per user\n",
        "issue_comment_event_counts = {}\n",
        "\n",
        "# --- Process Issue Comments ---\n",
        "# Fetch all issues since the start_date (this method supports 'since')\n",
        "issues_in_period = repo.get_issues(state='all', since=start_date)\n",
        "\n",
        "for issue in issues_in_period:\n",
        "    # Fetch comments for each issue\n",
        "    comments = issue.get_comments()\n",
        "    for comment in comments:\n",
        "        # Ensure the comment itself was created within the period and has a user\n",
        "        if comment.user and comment.created_at >= start_date:\n",
        "            login = comment.user.login\n",
        "            issue_comment_event_counts[login] = issue_comment_event_counts.get(login, 0) + 1\n",
        "\n",
        "            # NEW: Populate daily_issue_comment_count\n",
        "            date_key = comment.created_at.date()\n",
        "            daily_issue_comment_count[date_key] = daily_issue_comment_count.get(date_key, 0) + 1\n",
        "\n",
        "# --- Process Pull Request Comments (which are also IssueCommentEvent conceptually) ---\n",
        "# Fetch all pull requests (this method does NOT support 'since')\n",
        "pulls_in_period = repo.get_pulls(state='all')\n",
        "\n",
        "for pull in pulls_in_period:\n",
        "    # Fetch comments for each pull request\n",
        "    comments = pull.get_comments()\n",
        "    for comment in comments:\n",
        "        # Ensure the comment itself was created within the period and has a user\n",
        "        if comment.user and comment.created_at >= start_date:\n",
        "            login = comment.user.login\n",
        "            issue_comment_event_counts[login] = issue_comment_event_counts.get(login, 0) + 1\n",
        "\n",
        "            # NEW: Populate daily_issue_comment_count\n",
        "            date_key = comment.created_at.date()\n",
        "            daily_issue_comment_count[date_key] = daily_issue_comment_count.get(date_key, 0) + 1\n",
        "\n",
        "# Convert the dictionary to a pandas Series for merging\n",
        "issue_comment_event_series = pd.Series(issue_comment_event_counts, name='issue_comment_event_count')\n",
        "\n",
        "# Merge the counts into df_people\n",
        "df_people = df_people.set_index('github_login')\n",
        "df_people = df_people.merge(issue_comment_event_series, left_index=True, right_index=True, how='left')\n",
        "df_people = df_people.reset_index()\n",
        "\n",
        "# Fill NaN values with 0 for users who didn't perform any IssueCommentEvent in the period\n",
        "df_people['issue_comment_event_count'] = df_people['issue_comment_event_count'].fillna(0).astype(int)\n",
        "\n",
        "print(\"df_people DataFrame with 'issue_comment_event_count' column:\")\n",
        "display(df_people.head())\n",
        "\n",
        "print(\"\\nTop 4 people with most IssueCommentEvents:\")\n",
        "top_commenters = df_people.sort_values(by='issue_comment_event_count', ascending=False).head(4)\n",
        "for index, row in top_commenters.iterrows():\n",
        "    print(f\"- {row['github_login']} ({row['name'] if row['name'] != 'N/A' else 'Name not available'}): {row['issue_comment_event_count']} IssueCommentEvents\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pull requests"
      ],
      "metadata": {
        "id": "QlBI3kXmjMKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timezone\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure selected_timedelta is available from previous cells\n",
        "start_date = datetime.now(timezone.utc) - selected_timedelta\n",
        "\n",
        "# Dictionary to store pull requests opened count per user\n",
        "pull_requests_opened_count = {}\n",
        "# Dictionary to store pull requests merged count per user\n",
        "pull_requests_merged_count = {}\n",
        "# Dictionary to store pull requests closed (including merged) count per user\n",
        "pull_requests_closed_count = {}\n",
        "\n",
        "# Fetch all pull requests (this method does NOT support 'since' for 'all' state directly, but can filter later)\n",
        "# It's generally better to iterate through all and filter by date.\n",
        "all_pulls = repo.get_pulls(state='all')\n",
        "\n",
        "for pull in all_pulls:\n",
        "    # Only count pull requests that were CREATED within the selected period\n",
        "    if pull.user and pull.created_at >= start_date:\n",
        "        login = pull.user.login\n",
        "        pull_requests_opened_count[login] = pull_requests_opened_count.get(login, 0) + 1\n",
        "\n",
        "        # NEW: Populate daily_prs_opened_count\n",
        "        date_key = pull.created_at.date()\n",
        "        daily_prs_opened_count[date_key] = daily_prs_opened_count.get(date_key, 0) + 1\n",
        "\n",
        "    # Count pull requests that were MERGED within the selected period\n",
        "    if pull.merged and pull.merged_at and pull.merged_at >= start_date:\n",
        "        if pull.merged_by:\n",
        "            login = pull.merged_by.login\n",
        "            pull_requests_merged_count[login] = pull_requests_merged_count.get(login, 0) + 1\n",
        "\n",
        "            # NEW: Populate daily_prs_merged_count\n",
        "            date_key = pull.merged_at.date()\n",
        "            daily_prs_merged_count[date_key] = daily_prs_merged_count.get(date_key, 0) + 1\n",
        "\n",
        "    # Count pull requests that were CLOSED (either merged or closed without merging) within the selected period\n",
        "    # Note: 'closed_at' is typically populated for both merged and explicitly closed PRs.\n",
        "    # We attribute the closure to the user who opened the PR for PI calculation.\n",
        "    if pull.user and pull.state == 'closed' and pull.closed_at and pull.closed_at >= start_date:\n",
        "        login = pull.user.login # User who created the PR\n",
        "        pull_requests_closed_count[login] = pull_requests_closed_count.get(login, 0) + 1\n",
        "\n",
        "        # NEW: Populate daily_prs_closed_count\n",
        "        date_key = pull.closed_at.date()\n",
        "        daily_prs_closed_count[date_key] = daily_prs_closed_count.get(date_key, 0) + 1\n",
        "\n",
        "# Convert the dictionaries to pandas Series for merging\n",
        "pull_requests_opened_series = pd.Series(pull_requests_opened_count, name='pull_requests_opened_count')\n",
        "pull_requests_merged_series = pd.Series(pull_requests_merged_count, name='pull_requests_merged_count')\n",
        "pull_requests_closed_series = pd.Series(pull_requests_closed_count, name='pull_requests_closed_count')\n",
        "\n",
        "\n",
        "# --- Drop existing 'pull_requests_opened_count', 'pull_requests_merged_count', 'pull_requests_closed_count' related columns before merging ---\n",
        "columns_to_drop_opened = [col for col in df_people.columns if col.startswith('pull_requests_opened_count')]\n",
        "if columns_to_drop_opened:\n",
        "    df_people = df_people.drop(columns=columns_to_drop_opened)\n",
        "\n",
        "columns_to_drop_merged = [col for col in df_people.columns if col.startswith('pull_requests_merged_count')]\n",
        "if columns_to_drop_merged:\n",
        "    df_people = df_people.drop(columns=columns_to_drop_merged)\n",
        "\n",
        "columns_to_drop_closed = [col for col in df_people.columns if col.startswith('pull_requests_closed_count')]\n",
        "if columns_to_drop_closed:\n",
        "    df_people = df_people.drop(columns=columns_to_drop_closed)\n",
        "\n",
        "\n",
        "df_people = df_people.set_index('github_login')\n",
        "\n",
        "# Merge counts\n",
        "df_people = df_people.merge(pull_requests_opened_series, left_index=True, right_index=True, how='left')\n",
        "df_people = df_people.merge(pull_requests_merged_series, left_index=True, right_index=True, how='left')\n",
        "df_people = df_people.merge(pull_requests_closed_series, left_index=True, right_index=True, how='left')\n",
        "\n",
        "df_people = df_people.reset_index()\n",
        "\n",
        "# Fill NaN values with 0\n",
        "df_people['pull_requests_opened_count'] = df_people['pull_requests_opened_count'].fillna(0).astype(int)\n",
        "df_people['pull_requests_merged_count'] = df_people['pull_requests_merged_count'].fillna(0).astype(int)\n",
        "df_people['pull_requests_closed_count'] = df_people['pull_requests_closed_count'].fillna(0).astype(int)\n",
        "\n",
        "print(\"df_people DataFrame with PR counts:\")\n",
        "display(df_people.head())\n",
        "\n",
        "print(\"\\nTop 4 people with most Pull Requests opened:\")\n",
        "top_pr_openers = df_people.sort_values(by='pull_requests_opened_count', ascending=False).head(4)\n",
        "for index, row in top_pr_openers.iterrows():\n",
        "    print(f\"- {row['github_login']} ({row['name'] if row['name'] != 'N/A' else 'Name not available'}): {row['pull_requests_opened_count']} PRs opened\")\n",
        "\n",
        "print(\"\\nTop 4 people with most Pull Requests merged:\")\n",
        "top_pr_mergers = df_people.sort_values(by='pull_requests_merged_count', ascending=False).head(4)\n",
        "for index, row in top_pr_mergers.iterrows():\n",
        "    print(f\"- {row['github_login']} ({row['name'] if row['name'] != 'N/A' else 'Name not available'}): {row['pull_requests_merged_count']} PRs merged\")\n",
        "\n",
        "print(\"\\nTop 4 people with most Pull Requests closed:\")\n",
        "top_pr_closed = df_people.sort_values(by='pull_requests_closed_count', ascending=False).head(4)\n",
        "for index, row in top_pr_closed.iterrows():\n",
        "    print(f\"- {row['github_login']} ({row['name'] if row['name'] != 'N/A' else 'Name not available'}): {row['pull_requests_closed_count']} PRs closed\")"
      ],
      "metadata": {
        "id": "oLvGLSeBjWaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code changes"
      ],
      "metadata": {
        "id": "k7PpvKkpqcca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timezone\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure selected_timedelta is available from previous cells\n",
        "start_date = datetime.now(timezone.utc) - selected_timedelta\n",
        "\n",
        "# Dictionaries to store additions and deletions per user\n",
        "user_additions = {}\n",
        "user_deletions = {}\n",
        "\n",
        "print(f\"Fetching code changes from merged and currently open Pull Requests since {start_date}...\")\n",
        "\n",
        "# Fetch all pull requests (open, closed, and all states to check merged status)\n",
        "all_pulls = repo.get_pulls(state='all')\n",
        "\n",
        "for pull in all_pulls:\n",
        "    process_pull = False\n",
        "    # Condition 1: PR was merged within the period\n",
        "    if pull.merged and pull.merged_at and pull.merged_at >= start_date:\n",
        "        process_pull = True\n",
        "    # Condition 2: PR is currently open\n",
        "    elif pull.state == 'open':\n",
        "        process_pull = True\n",
        "\n",
        "    if process_pull:\n",
        "        # Iterate through the commits of the relevant PR\n",
        "        try:\n",
        "            pr_commits = pull.get_commits()\n",
        "            for commit in pr_commits:\n",
        "                # Check if the commit's author exists\n",
        "                if commit.author and commit.author.login:\n",
        "                    login = commit.author.login\n",
        "                    try:\n",
        "                        full_commit = repo.get_commit(commit.sha)\n",
        "                        if full_commit.stats:\n",
        "                            # NEW: Check if commit date is within the period\n",
        "                            commit_datetime = full_commit.commit.author.date # This is already a timezone-aware datetime\n",
        "                            # Ensure comparison is done with timezone-aware objects\n",
        "                            if commit_datetime.astimezone(timezone.utc) >= start_date:\n",
        "                                commit_date_key = commit_datetime.date() # Get only the date part\n",
        "\n",
        "                                user_additions[login] = user_additions.get(login, 0) + full_commit.stats.additions\n",
        "                                user_deletions[login] = user_deletions.get(login, 0) + full_commit.stats.deletions\n",
        "\n",
        "                                # NEW: Populate daily_additions and daily_deletions\n",
        "                                daily_additions[commit_date_key] = daily_additions.get(commit_date_key, 0) + full_commit.stats.additions\n",
        "                                daily_deletions[commit_date_key] = daily_deletions.get(commit_date_key, 0) + full_commit.stats.deletions\n",
        "                    except Exception as e:\n",
        "                        print(f\"Warning: Could not fetch stats for commit {commit.sha} in PR #{pull.number} by {login}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not fetch commits for PR #{pull.number}: {e}\")\n",
        "\n",
        "# Convert dictionaries to pandas Series\n",
        "additions_series = pd.Series(user_additions, name='total_additions')\n",
        "deletions_series = pd.Series(user_deletions, name='total_deletions')\n",
        "\n",
        "# --- Drop existing columns before merging ---\n",
        "# This prevents duplicate columns if the cell is run multiple times\n",
        "columns_to_drop_add = [col for col in df_people.columns if col.startswith('total_additions')]\n",
        "if columns_to_drop_add:\n",
        "    df_people = df_people.drop(columns=columns_to_drop_add)\n",
        "\n",
        "columns_to_drop_del = [col for col in df_people.columns if col.startswith('total_deletions')]\n",
        "if columns_to_drop_del:\n",
        "    df_people = df_people.drop(columns=columns_to_drop_del)\n",
        "\n",
        "df_people = df_people.set_index('github_login')\n",
        "\n",
        "# Merge additions and deletions into df_people\n",
        "df_people = df_people.merge(additions_series, left_index=True, right_index=True, how='left')\n",
        "df_people = df_people.merge(deletions_series, left_index=True, right_index=True, how='left')\n",
        "\n",
        "df_people = df_people.reset_index()\n",
        "\n",
        "# Fill NaN values with 0 for users who had no additions or deletions in the period\n",
        "df_people['total_additions'] = df_people['total_additions'].fillna(0).astype(int)\n",
        "df_people['total_deletions'] = df_people['total_deletions'].fillna(0).astype(int)\n",
        "\n",
        "print(\"\\ndf_people DataFrame with 'total_additions' and 'total_deletions' columns:\")\n",
        "display(df_people.head())\n",
        "\n",
        "print(\"\\nTop 4 people with most additions:\")\n",
        "top_adders = df_people.sort_values(by='total_additions', ascending=False).head(4)\n",
        "for index, row in top_adders.iterrows():\n",
        "    print(f\"- {row['github_login']} ({row['name'] if row['name'] != 'N/A' else 'Name not available'}): {row['total_additions']} additions\")\n",
        "\n",
        "print(\"\\nTop 4 people with most deletions:\")\n",
        "top_deleters = df_people.sort_values(by='total_deletions', ascending=False).head(4)\n",
        "for index, row in top_deleters.iterrows():\n",
        "    print(f\"- {row['github_login']} ({row['name'] if row['name'] != 'N/A' else 'Name not available'}): {row['total_deletions']} deletions\")"
      ],
      "metadata": {
        "id": "S4jKZz3Lqd-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cellname : fix-count-scales\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 2. Define a list named other_event_cols\n",
        "other_event_cols = [\n",
        "    'issues_opened_count',\n",
        "    'issues_reopened_count',\n",
        "    'issue_comment_event_count',\n",
        "    'pull_requests_opened_count',\n",
        "    'pull_requests_merged_count'\n",
        "]\n",
        "\n",
        "# Ensure all columns exist, filter out non-existent ones if any\n",
        "existing_other_event_cols = [col for col in other_event_cols if col in df_people.columns]\n",
        "\n",
        "# 3. Calculate max_other_event_count\n",
        "if existing_other_event_cols:\n",
        "    max_other_event_count = df_people[existing_other_event_cols].max().max()\n",
        "else:\n",
        "    max_other_event_count = 0\n",
        "\n",
        "# Avoid division by zero if no other events\n",
        "if max_other_event_count == 0:\n",
        "    max_other_event_count = 1\n",
        "\n",
        "# 4. Calculate max_total_additions and max_total_deletions\n",
        "max_total_additions = df_people['total_additions'].max()\n",
        "max_total_deletions = df_people['total_deletions'].max()\n",
        "\n",
        "# Determine max_code_change as the maximum value between max_total_additions and max_total_deletions\n",
        "max_code_change = max(max_total_additions, max_total_deletions)\n",
        "\n",
        "# Calculate the scaling_ratio by dividing max_code_change by max_other_event_count\n",
        "# Handle the case where max_other_event_count is zero to avoid division errors.\n",
        "scaling_ratio = max_code_change / max_other_event_count if max_other_event_count > 0 else 0\n",
        "\n",
        "# Based on scaling_ratio, determine the chosen_factor:\n",
        "chosen_factor = 1 # Default value\n",
        "if max_code_change == 0: # If there are no code changes, factor is 1\n",
        "    chosen_factor = 1\n",
        "elif scaling_ratio <= 1:\n",
        "    chosen_factor = 1\n",
        "elif 1 < scaling_ratio <= 10:\n",
        "    chosen_factor = 10\n",
        "elif scaling_ratio > 10 and scaling_ratio <= 100:\n",
        "    chosen_factor = int(np.ceil(scaling_ratio / 10)) * 10\n",
        "elif scaling_ratio > 100:\n",
        "    chosen_factor = int(np.ceil(scaling_ratio / 100)) * 100\n",
        "\n",
        "# Create a scaled_label_suffix string\n",
        "if chosen_factor == 1:\n",
        "    scaled_label_suffix = ''\n",
        "else:\n",
        "    scaled_label_suffix = f' (x{chosen_factor})'\n",
        "\n",
        "# 5. Initialize new columns (or overwrite if they exist)\n",
        "df_people['scaled_additions'] = 0.0\n",
        "df_people['scaled_deletions'] = 0.0\n",
        "\n",
        "# Update the scaled_additions column in df_people\n",
        "df_people['scaled_additions'] = df_people['total_additions'] / chosen_factor\n",
        "\n",
        "# Update the scaled_deletions column in df_people\n",
        "df_people['scaled_deletions'] = df_people['total_deletions'] / chosen_factor\n",
        "\n",
        "# 8. Print the calculated values\n",
        "print(f\"Maximum count of other event types (for scaling): {max_other_event_count}\")\n",
        "print(f\"Maximum total additions: {max_total_additions}\")\n",
        "print(f\"Maximum total deletions: {max_total_deletions}\")\n",
        "print(f\"Maximum code change (additions or deletions): {max_code_change}\")\n",
        "if max_code_change > 0 and max_other_event_count > 0:\n",
        "    print(f\"Scaling ratio (max_code_change / max_other_event_count): {scaling_ratio:.4f}\")\n",
        "print(f\"Chosen factor for scaling code changes: {chosen_factor}\")\n",
        "print(f\"Scaled label suffix: '{scaled_label_suffix}'\")\n",
        "\n",
        "# 9. Display the head of the df_people DataFrame\n",
        "print(\"\\ndf_people DataFrame with scaled additions and deletions:\")\n",
        "display(df_people[['github_login', 'name', 'total_additions', 'scaled_additions', 'total_deletions', 'scaled_deletions']].head())\n",
        "\n",
        "# 10. Print top 4 contributors by scaled_additions\n",
        "print(\"\\nTop 4 people by scaled additions:\")\n",
        "top_scaled_adders = df_people.sort_values(by='scaled_additions', ascending=False).head(4)\n",
        "for index, row in top_scaled_adders.iterrows():\n",
        "    print(f\"- {row['github_login']} ({row['name'] if row['name'] != 'N/A' else 'Name not available'}): {row['scaled_additions']:.2f} scaled additions\")\n",
        "\n",
        "# 11. Print top 4 contributors by scaled_deletions\n",
        "print(\"\\nTop 4 people by scaled deletions:\")\n",
        "top_scaled_deleters = df_people.sort_values(by='scaled_deletions', ascending=False).head(4)\n",
        "for index, row in top_scaled_deleters.iterrows():\n",
        "    print(f\"- {row['github_login']} ({row['name'] if row['name'] != 'N/A' else 'Name not available'}): {row['scaled_deletions']:.2f} scaled deletions\")"
      ],
      "metadata": {
        "id": "j-NJ5SHRtkyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PROJECT OVERVIEW"
      ],
      "metadata": {
        "id": "kJyZNQbQyI_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Events (totals)\n",
        "\n",
        "* Show all events in the period (ommit devs that haven't generated events)"
      ],
      "metadata": {
        "id": "-JDHRPapvrUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cellname : show-all-events\n",
        "\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "# Diagnostic: Print all columns in df_people before processing for the plot\n",
        "print(\"Current columns in df_people before plotting:\")\n",
        "print(df_people.columns)\n",
        "\n",
        "# Define the columns that represent events, NOW USING SCALED ADDITIONS AND DELETIONS\n",
        "event_columns = [\n",
        "    'issues_opened_count',\n",
        "    'issues_reopened_count',\n",
        "    'issue_comment_event_count',\n",
        "    'pull_requests_opened_count',\n",
        "    'pull_requests_merged_count',\n",
        "    'scaled_additions',  # Use scaled additions\n",
        "    'scaled_deletions'   # Use scaled deletions\n",
        "]\n",
        "\n",
        "# Ensure all event columns exist in df_people\n",
        "existing_event_columns = [col for col in event_columns if col in df_people.columns]\n",
        "\n",
        "# Filter out developers with no activity in any of the tracked events for cleaner visualization\n",
        "df_plot = df_people[df_people[existing_event_columns].sum(axis=1) > 0].copy()\n",
        "\n",
        "# Rename columns for better plot labels (optional, but good practice)\n",
        "df_plot = df_plot.rename(columns={\n",
        "    'issues_opened_count': 'Issues Opened',\n",
        "    'issues_reopened_count': 'Issues Reopened',\n",
        "    'issue_comment_event_count': 'Comments (Issues/PRs)',\n",
        "    'pull_requests_opened_count': 'PRs Opened',\n",
        "    'pull_requests_merged_count': 'PRs Merged',\n",
        "    'scaled_additions': f'Lines Added{scaled_label_suffix}', # Updated label with suffix\n",
        "    'scaled_deletions': f'Lines Removed{scaled_label_suffix}' # Updated label with suffix\n",
        "})\n",
        "\n",
        "# Melt the DataFrame to a long format suitable for stacked bar charts\n",
        "# We use 'github_login' as the id_vars because we want one bar per developer\n",
        "df_melted = df_plot.melt(id_vars=['github_login', 'name'], value_vars=[col for col in df_plot.columns if col in [f'Lines Added{scaled_label_suffix}', f'Lines Removed{scaled_label_suffix}', 'Issues Opened', 'Issues Reopened', 'Comments (Issues/PRs)', 'PRs Opened', 'PRs Merged']],\n",
        "                         var_name='Event Type', value_name='Count')\n",
        "\n",
        "# Create the stacked bar chart using Plotly Express\n",
        "fig = px.bar(df_melted,\n",
        "             x='github_login',\n",
        "             y='Count',\n",
        "             color='Event Type',\n",
        "             title=f'Events per Contributor', # Updated title\n",
        "             labels={'github_login': 'Contributor', 'Count': 'Event Count'},\n",
        "             hover_name='name',\n",
        "             hover_data={'Event Type': True, 'Count': ':.2f', 'github_login': False},\n",
        "             text_auto='.2f') # Display text values automatically, formatted to 2 decimal places\n",
        "\n",
        "fig.update_layout(xaxis_title='Contributor',\n",
        "                  yaxis_title='Event Count',\n",
        "                  barmode='stack',\n",
        "                  legend_title='Event Type')\n",
        "\n",
        "# Set text color to white for all traces and position text inside\n",
        "fig.update_traces(textfont_color='white', textposition='inside')\n",
        "\n",
        "# Optional: Adjust x-axis to show only relevant developer names if too many\n",
        "# fig.update_xaxes(tickangle=45, tickfont=dict(size=10))\n",
        "\n",
        "fig.show() # Always show the figure\n",
        "\n",
        "# Save the figure to Google Drive if report generation is enabled\n",
        "save_fig(fig, 'all_primitive_events.html')\n"
      ],
      "metadata": {
        "id": "sls-rXnxrbjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Events categorized x time\n",
        "* All events, but aggregating Issues, Commands,PRs and total changes"
      ],
      "metadata": {
        "id": "6wTnhdODXxbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cellname : time-plot\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from datetime import date, timedelta, datetime, timezone\n",
        "\n",
        "# Ensure start_date and selected_timedelta are available (from previous cells)\n",
        "# Recalculate full date range for the period\n",
        "# It's important to use the same start_date as the event fetching to match the period.\n",
        "# The start_date for event fetching is a datetime with timezone.\n",
        "# We need to get the date part for comparison with dictionary keys.\n",
        "\n",
        "# The start_date for fetching events is already timezone-aware\n",
        "# start_date = datetime.now(timezone.utc) - selected_timedelta\n",
        "# Let's ensure 'start_date' is available, it comes from bd46d806\n",
        "if 'start_date' not in globals():\n",
        "    start_date = datetime.now(timezone.utc) - selected_timedelta # Fallback\n",
        "\n",
        "end_date_for_range = datetime.now(timezone.utc).date() # Current date\n",
        "start_date_for_range = start_date.date() # Date part of the fetching start_date\n",
        "\n",
        "all_dates = [start_date_for_range + timedelta(days=x) for x in range((end_date_for_range - start_date_for_range).days + 1)]\n",
        "\n",
        "# Create a DataFrame to store daily events\n",
        "df_daily_events = pd.DataFrame(all_dates, columns=['Date'])\n",
        "df_daily_events['Date'] = pd.to_datetime(df_daily_events['Date']) # Convert to datetime objects\n",
        "\n",
        "# Merge daily event counts\n",
        "# Fillna(0) for dates where no events occurred\n",
        "df_daily_events['Issues Raised'] = df_daily_events['Date'].apply(lambda d: daily_issues_raised_count.get(d.date(), 0))\n",
        "df_daily_events['Comments'] = df_daily_events['Date'].apply(lambda d: daily_issue_comment_count.get(d.date(), 0))\n",
        "df_daily_events['PRs Opened'] = df_daily_events['Date'].apply(lambda d: daily_prs_opened_count.get(d.date(), 0))\n",
        "df_daily_events['PRs Merged'] = df_daily_events['Date'].apply(lambda d: daily_prs_merged_count.get(d.date(), 0))\n",
        "df_daily_events['PRs Closed'] = df_daily_events['Date'].apply(lambda d: daily_prs_closed_count.get(d.date(), 0))\n",
        "\n",
        "# Apply scaling to additions and deletions for consistency with other plots\n",
        "# chosen_factor and scaled_label_suffix should be available from fix-count-scales (j-NJ5SHRtkyN)\n",
        "if 'chosen_factor' not in globals():\n",
        "    chosen_factor = 1 # Fallback if not defined (should be defined)\n",
        "if 'scaled_label_suffix' not in globals():\n",
        "    scaled_label_suffix = '' # Fallback\n",
        "\n",
        "# Calculate scaled additions and deletions\n",
        "df_daily_events[f'Lines Added{scaled_label_suffix}'] = df_daily_events['Date'].apply(lambda d: daily_additions.get(d.date(), 0) / chosen_factor)\n",
        "df_daily_events[f'Lines Removed{scaled_label_suffix}'] = df_daily_events['Date'].apply(lambda d: daily_deletions.get(d.date(), 0) / chosen_factor)\n",
        "\n",
        "# Combine 'Lines Added' and 'Lines Removed' into 'Lines Changed'\n",
        "df_daily_events[f'Lines Changed{scaled_label_suffix}'] = df_daily_events[f'Lines Added{scaled_label_suffix}'] + df_daily_events[f'Lines Removed{scaled_label_suffix}']\n",
        "\n",
        "# Melt the DataFrame for line chart visualization\n",
        "df_daily_melted = df_daily_events.melt(\n",
        "    id_vars=['Date'],\n",
        "    value_vars=[\n",
        "        'Issues Raised',\n",
        "        'Comments',\n",
        "        'PRs Opened',\n",
        "        'PRs Merged',\n",
        "        f'Lines Changed{scaled_label_suffix}' # Use combined category\n",
        "    ],\n",
        "    var_name='Event Type',\n",
        "    value_name='Count'\n",
        ")\n",
        "\n",
        "# Create the line chart using Plotly Express with spline for smoothing\n",
        "fig = px.line(\n",
        "    df_daily_melted,\n",
        "    x='Date',\n",
        "    y='Count',\n",
        "    color='Event Type',\n",
        "    title='Categorized events over time', # Changed title here\n",
        "    labels={'Count': 'Event Count', 'Date': 'Date'},\n",
        "    hover_data={'Count': ':.2f'},\n",
        "    line_shape='spline'\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='Count',\n",
        "    legend_title='Event Type',\n",
        "    hovermode='x unified' # Show all hover info for a given date\n",
        ")\n",
        "\n",
        "fig.update_xaxes(dtick=\"D1\", tickformat=\"%b %d\") # Daily ticks, format as \"Month Day\"\n",
        "\n",
        "fig.show()\n",
        "\n",
        "save_fig(fig, 'daily_events_time_series.html')\n",
        "\n",
        "print(\"df_daily_events DataFrame (first 5 rows):\")\n",
        "display(df_daily_events.head())"
      ],
      "metadata": {
        "id": "GWv7aCkB9y7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Events categorized\n"
      ],
      "metadata": {
        "id": "KblqqbT6v1A4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cellname : all-events-aggregate\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "# Create consolidated event columns\n",
        "df_consolidated = df_people.copy()\n",
        "df_consolidated['Issues'] = df_consolidated['issues_opened_count'] + df_consolidated['issues_reopened_count']\n",
        "df_consolidated['Comments'] = df_consolidated['issue_comment_event_count']\n",
        "df_consolidated['PRs Opened'] = df_consolidated['pull_requests_opened_count']\n",
        "df_consolidated['PRs Merged'] = df_consolidated['pull_requests_merged_count']\n",
        "\n",
        "# Recalculate 'Code Changes' using scaled additions and deletions and include the suffix\n",
        "df_consolidated[f'Code Changes{scaled_label_suffix}'] = df_consolidated['scaled_additions'] + df_consolidated['scaled_deletions']\n",
        "\n",
        "# Define the new consolidated event columns for plotting, now including the scaled suffix\n",
        "consolidated_event_columns = ['Issues', 'Comments', 'PRs Opened', 'PRs Merged', f'Code Changes{scaled_label_suffix}']\n",
        "\n",
        "# Filter out developers with no activity in any of the consolidated events\n",
        "df_plot_consolidated = df_consolidated[df_consolidated[consolidated_event_columns].sum(axis=1) > 0].copy()\n",
        "\n",
        "# Melt the DataFrame to a long format suitable for stacked bar charts\n",
        "df_melted_consolidated = df_plot_consolidated.melt(\n",
        "    id_vars=['github_login', 'name'],\n",
        "    value_vars=consolidated_event_columns,\n",
        "    var_name='Event Type',\n",
        "    value_name='Count'\n",
        ")\n",
        "\n",
        "# Rename 'Issues' to 'Issues Raised' for display in the plot legend\n",
        "df_melted_consolidated['Event Type'] = df_melted_consolidated['Event Type'].replace('Issues', 'Issues Raised')\n",
        "\n",
        "# Add a new column for text display, conditional on Count > 0 and formatted\n",
        "df_melted_consolidated['text_values'] = df_melted_consolidated.apply(lambda row: f\"{row['Count']:.2f}\" if row['Count'] > 0 and row['Event Type'] == f'Code Changes{scaled_label_suffix}' else (f\"{row['Count']:.0f}\" if row['Count'] > 0 else ''), axis=1)\n",
        "\n",
        "\n",
        "# Create the stacked bar chart using Plotly Express\n",
        "fig_consolidated = px.bar(\n",
        "    df_melted_consolidated,\n",
        "    x='github_login',\n",
        "    y='Count',\n",
        "    color='Event Type',\n",
        "    title=f'Categorized event count', # Updated title\n",
        "    labels={'github_login': 'Contributor', 'Count': 'Event Count'},\n",
        "    hover_name='name',\n",
        "    hover_data={'Event Type': True, 'Count': ':.2f', 'github_login': False}, # Format Count to 2 decimal places\n",
        "    text='text_values' # Use the new conditional text column\n",
        ")\n",
        "\n",
        "fig_consolidated.update_layout(\n",
        "    xaxis_title='Contributor',\n",
        "    yaxis_title='Event Count',\n",
        "    barmode='stack',\n",
        "    legend_title='Event Type'\n",
        ")\n",
        "\n",
        "# Adjust x-axis to show labels vertically\n",
        "fig_consolidated.update_xaxes(tickangle=90, tickfont=dict(size=10), showgrid=False)\n",
        "\n",
        "# Revert y-axis grid to default Plotly style, with lines every 5 units\n",
        "fig_consolidated.update_yaxes(showgrid=True, showticklabels=True, dtick=5, gridwidth=1, griddash=None)\n",
        "\n",
        "# Set text color to white for all traces and position text inside\n",
        "fig_consolidated.update_traces(textfont_color='white', textposition='inside')\n",
        "\n",
        "fig_consolidated.show()\n",
        "\n",
        "save_fig(fig_consolidated, 'all_aggregate_events.html')\n"
      ],
      "metadata": {
        "id": "3JF1OLKrv85u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Workload balance (WB) index"
      ],
      "metadata": {
        "id": "bVbGpk-gVHDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WB aggregate"
      ],
      "metadata": {
        "id": "97ufrvONyKjo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cc00a1d"
      },
      "source": [
        "# cellname : wbi\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Calculate aggregate PRs opened and closed for the entire project\n",
        "total_prs_opened = df_people['pull_requests_opened_count'].sum()\n",
        "total_prs_closed = df_people['pull_requests_closed_count'].sum()\n",
        "\n",
        "print(f\"Total PRs Opened in the period: {total_prs_opened}\")\n",
        "print(f\"Total PRs Closed in the period: {total_prs_closed}\")\n",
        "\n",
        "# --- Calculate the NEW Aggregate Progress Index (PI) (formerly PR Balance Index) ---\n",
        "# PI = (PRs Closed - PRs Opened) / (PRs Closed + PRs Opened)\n",
        "# Handle case where total_pr_activity is 0 to avoid division by zero\n",
        "total_pr_activity = total_prs_opened + total_prs_closed\n",
        "\n",
        "if total_pr_activity > 0:\n",
        "    # The new PI is the former PR Balance Index\n",
        "    wb_index = (total_prs_closed - total_prs_opened) / total_pr_activity\n",
        "    print(f\"Work Balance (WB): {wb_index:.2f}\\n\")\n",
        "else:\n",
        "    wb_index = 0.0 # Define a default if no PRs were opened and closed\n",
        "    print(\"No PR activity (opened or closed) in the period to calculate the Net Contribution Throughput (NCT).\\n\")\n",
        "\n",
        "# --- Horizontal Bar Chart for Net Contribution Throughput (NCT) ---\n",
        "\n",
        "# Determine marker color dynamically\n",
        "#bar_color_pi = '#AEC6CF'\n",
        "if wb_index < 0:\n",
        "    bar_color_pi = \"#dd5500\"\n",
        "elif wb_index > 0:\n",
        "    bar_color_pi = \"#00aa88\"\n",
        "\n",
        "fig_horizontal_bar = go.Figure()\n",
        "\n",
        "# Add background shapes for the ranges (reverting y0 and y1 for taller colored segments)\n",
        "fig_horizontal_bar.update_layout(\n",
        "    shapes=[\n",
        "        # More vivid Red for [-1, -0.5]\n",
        "        dict(type='rect', xref='x', yref='y', x0=-1, y0=-0.5, x1=-0.5, y1=0.5, fillcolor='#FF6347', opacity=0.3, layer='below', line_width=0),\n",
        "        # More vivid Salmon for [-0.5, 0]\n",
        "        dict(type='rect', xref='x', yref='y', x0=-0.5, y0=-0.5, x1=0, y1=0.5, fillcolor='#FFA07A', opacity=0.3, layer='below', line_width=0),\n",
        "        # More vivid Light Green for [0, 0.5]\n",
        "        dict(type='rect', xref='x', yref='y', x0=0, y0=-0.5, x1=0.5, y1=0.5, fillcolor='#90EE90', opacity=0.3, layer='below', line_width=0),\n",
        "        # More vivid Darker Green for [0.5, 1]\n",
        "        dict(type='rect', xref='x', yref='y', x0=0.5, y0=-0.5, x1=1, y1=0.5, fillcolor='#3CB371', opacity=0.3, layer='below', line_width=0)\n",
        "    ]\n",
        ")\n",
        "\n",
        "fig_horizontal_bar.add_trace(go.Bar(\n",
        "    x=[wb_index],\n",
        "    y=['NCT'],\n",
        "    orientation='h',\n",
        "    base=0,\n",
        "    marker_color=bar_color_pi,\n",
        "    text=f'{wb_index:.2f}',\n",
        "    textposition='inside',\n",
        "    textfont=dict(color='white'),\n",
        "    width=0.4 # Explicitly setting the width (height for horizontal bar) of the blue indicator bar\n",
        "))\n",
        "\n",
        "fig_horizontal_bar.update_layout(\n",
        "    title_text='Workload Balance (WB)', # Updated title\n",
        "    xaxis_range=[-1, 1], # Set x-axis range strictly from -1 to 1\n",
        "    xaxis_title='← Accumulating | Clearing →', # Updated x-axis title\n",
        "    yaxis_visible=False, # Hide y-axis\n",
        "    yaxis_showticklabels=False, # Hide y-axis tick labels\n",
        "    height=150, # Set a reasonable height for the bar chart\n",
        "    width=800, # Set a reasonable width\n",
        "    margin=dict(l=20, r=20, t=50, b=20), # Adjust margins\n",
        "    title_x=0.5 # Center the title\n",
        ")\n",
        "\n",
        "fig_horizontal_bar.show()\n",
        "\n",
        "save_fig(fig_horizontal_bar, 'workload_balance.html')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WB evolution"
      ],
      "metadata": {
        "id": "9rjlbt-wybGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from datetime import date, timedelta, datetime, timezone\n",
        "\n",
        "# Ensure start_date and selected_timedelta are available\n",
        "if 'start_date' not in globals():\n",
        "    start_date = datetime.now(timezone.utc) - timedelta(weeks=1) # Fallback to 1 week if not defined\n",
        "\n",
        "end_date_for_range = datetime.now(timezone.utc).date() # Current date\n",
        "start_date_for_range = start_date.date() # Date part of the fetching start_date\n",
        "\n",
        "all_dates = [start_date_for_range + timedelta(days=x) for x in range((end_date_for_range - start_date_for_range).days + 1)]\n",
        "\n",
        "daily_wb_data = []\n",
        "\n",
        "for current_date in all_dates:\n",
        "    # Get daily PRs opened and closed from the dictionaries\n",
        "    daily_opened = daily_prs_opened_count.get(current_date, 0)\n",
        "    daily_closed = daily_prs_closed_count.get(current_date, 0)\n",
        "\n",
        "    # Calculate daily WB\n",
        "    total_daily_pr_activity = daily_opened + daily_closed\n",
        "    if total_daily_pr_activity > 0:\n",
        "        daily_wb = (daily_closed - daily_opened) / total_daily_pr_activity\n",
        "    else:\n",
        "        daily_wb = 0.0 # No PR activity for the day\n",
        "\n",
        "    daily_wb_data.append({'Date': current_date, 'Daily WB': daily_wb})\n",
        "\n",
        "df_daily_wb = pd.DataFrame(daily_wb_data)\n",
        "df_daily_wb['Date'] = pd.to_datetime(df_daily_wb['Date'])\n",
        "\n",
        "# Create the smoothed line chart for daily WB\n",
        "fig = px.line(\n",
        "    df_daily_wb,\n",
        "    x='Date',\n",
        "    y='Daily WB',\n",
        "    title='Daily Workload Balance (WB) over Time',\n",
        "    labels={'Daily WB': 'Workload Balance (WB)', 'Date': 'Date'},\n",
        "    hover_data={'Daily WB': ':.2f'},\n",
        "    line_shape='spline'\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='Workload Balance (WB)',\n",
        "    hovermode='x unified',\n",
        "    yaxis_range=[-1.1, 1.1]\n",
        ")\n",
        "\n",
        "fig.update_xaxes(dtick=\"D1\", tickformat=\"%b %d\") # Daily ticks, format as \"Month Day\"\n",
        "\n",
        "# Get the last date for the arrow placement\n",
        "last_date = df_daily_wb['Date'].iloc[-1]\n",
        "\n",
        "# Add an annotation for the 'Clearing' arrow (pointing up)\n",
        "fig.add_annotation(\n",
        "    x=last_date,\n",
        "    y=0.75,  # Arrowhead at y=0.75 (shortened length)\n",
        "    ax=last_date,\n",
        "    ay=0.05,    # Arrow tail slightly above y=0\n",
        "    showarrow=True,\n",
        "    arrowhead=3,\n",
        "    arrowsize=1.5,\n",
        "    arrowwidth=1,\n",
        "    arrowcolor='green',\n",
        "    text=\"\", # No text for the arrow itself\n",
        "    xref='x',\n",
        "    yref='y',\n",
        "    axref='x',\n",
        "    ayref='y',\n",
        "    xshift=10 # Shift arrow slightly to the right\n",
        ")\n",
        "\n",
        "# Add a separate annotation for the text \"Clearing\"\n",
        "fig.add_annotation(\n",
        "    x=last_date,\n",
        "    y=0.95,  # Position the text higher, near y=1\n",
        "    text='Clearing',\n",
        "    showarrow=False, # No arrow for the text\n",
        "    yanchor='bottom', # Anchor the bottom of the text to its y-position\n",
        "    font=dict(color='green'), # Set text color to green\n",
        "    xref='x',\n",
        "    yref='y',\n",
        "    xshift=10 # Shift text slightly to the right\n",
        ")\n",
        "\n",
        "# Add an annotation for the 'Accumulating' arrow (pointing down)\n",
        "fig.add_annotation(\n",
        "    x=last_date,\n",
        "    y=-0.75, # Arrowhead at y=-0.75\n",
        "    ax=last_date,\n",
        "    ay=-0.05, # Arrow tail slightly below y=0\n",
        "    showarrow=True,\n",
        "    arrowhead=3, # Plotly automatically orients arrowhead=3 based on direction\n",
        "    arrowsize=1.5,\n",
        "    arrowwidth=1,\n",
        "    arrowcolor='red',\n",
        "    text=\"\", # No text for the arrow itself\n",
        "    xref='x',\n",
        "    yref='y',\n",
        "    axref='x',\n",
        "    ayref='y',\n",
        "    xshift=10 # Shift arrow slightly to the right\n",
        ")\n",
        "\n",
        "# Add a separate annotation for the text \"Accumulating\"\n",
        "fig.add_annotation(\n",
        "    x=last_date,\n",
        "    y=-0.85, # Position the text below the arrow head (-0.75)\n",
        "    text='Accumulating',\n",
        "    showarrow=False, # No arrow for the text\n",
        "    yanchor='top', # Anchor the top of the text to its y-position\n",
        "    font=dict(color='red'), # Set text color to red\n",
        "    xref='x',\n",
        "    yref='y',\n",
        "    xshift=10 # Shift text slightly to the right\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "save_fig(fig, 'daily_wb_line_chart.html')\n",
        "\n",
        "print(\"Daily Workload Balance (WB) chart generated.\")"
      ],
      "metadata": {
        "id": "MJWlWzJ5Yw0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resolution balance (RB) index"
      ],
      "metadata": {
        "id": "hH3TiyA4TPhE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RB aggregate"
      ],
      "metadata": {
        "id": "yf67EpJ1yfNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cellname : ebi\n",
        "\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Calculate total issues raised (opened + reopened)\n",
        "total_issues_raised = df_people['issues_opened_count'].sum() + df_people['issues_reopened_count'].sum()\n",
        "\n",
        "# Get total PRs submitted (using 'pull_requests_opened_count' as proxy for submitted)\n",
        "total_prs_submitted = df_people['pull_requests_opened_count'].sum()\n",
        "\n",
        "print(f\"Total Issues Raised in the period: {total_issues_raised}\")\n",
        "print(f\"Total PRs Submitted in the period: {total_prs_submitted}\")\n",
        "\n",
        "# --- Calculate the Submitted Index (SI) ---\n",
        "# SI = (PRs Submitted - Issues Raised) / (PRs Submitted + Issues Raised)\n",
        "# Handle case where total_activity is 0 to avoid division by zero\n",
        "total_activity_for_si = total_prs_submitted + total_issues_raised\n",
        "\n",
        "if total_activity_for_si > 0:\n",
        "    rb_index = (total_prs_submitted - total_issues_raised) / total_activity_for_si\n",
        "    print(f\"Resolution Balance (RB): {rb_index:.2f}\\n\")\n",
        "else:\n",
        "    rb_index = 0.0 # Define a default if no relevant activity\n",
        "    print(\"No PRs submitted or issues raised in the period to calculate the Submitted Index (SI).\\n\")\n",
        "\n",
        "# --- Horizontal Bar Chart for Submitted Index (SI) ---\n",
        "\n",
        "# Determine indicator bar color based on index value, as originally designed\n",
        "if rb_index < 0:\n",
        "    bar_color_si = \"#888800\"\n",
        "elif rb_index > 0:\n",
        "    bar_color_si = \"#0088aa\"\n",
        "else:\n",
        "    bar_color_si = \"#AEC6CF\" # Default grayish-blue if index is 0\n",
        "\n",
        "fig_submitted_bar = go.Figure()\n",
        "\n",
        "# Add background shapes for the ranges (reusing vivid colors from previous notebook state)\n",
        "fig_submitted_bar.update_layout(\n",
        "    shapes=[\n",
        "        # More vivid Red for [-1, -0.5] -> Changed to Yellow\n",
        "        dict(type='rect', xref='x', yref='y', x0=-1, y0=-0.5, x1=-0.5, y1=0.5, fillcolor='#FFDD33', opacity=0.3, layer='below', line_width=0),\n",
        "        # More vivid Salmon for [-0.5, 0] -> Changed to Lighter Yellow\n",
        "        dict(type='rect', xref='x', yref='y', x0=-0.5, y0=-0.5, x1=0, y1=0.5, fillcolor='#FFEE99', opacity=0.3, layer='below', line_width=0),\n",
        "        # More vivid Light Green for [0, 0.5] -> Changed to Light Blue\n",
        "        dict(type='rect', xref='x', yref='y', x0=0, y0=-0.5, x1=0.5, y1=0.5, fillcolor='#ADD8E6', opacity=0.3, layer='below', line_width=0),\n",
        "        # More vivid Darker Green for [0.5, 1] -> Changed to Darker Blue\n",
        "        dict(type='rect', xref='x', yref='y', x0=0.5, y0=-0.5, x1=1, y1=0.5, fillcolor='#4682B4', opacity=0.3, layer='below', line_width=0)\n",
        "    ]\n",
        ")\n",
        "\n",
        "fig_submitted_bar.add_trace(go.Bar(\n",
        "    x=[rb_index],\n",
        "    y=['SI'],\n",
        "    orientation='h',\n",
        "    base=0,\n",
        "    marker_color=bar_color_si,\n",
        "    text=f'{rb_index:.2f}',\n",
        "    textposition='inside',\n",
        "    textfont=dict(color='white'), # White text color\n",
        "    width=0.4 # Height of the indicator bar\n",
        "))\n",
        "\n",
        "fig_submitted_bar.update_layout(\n",
        "    title_text='Resolution Balance (RB)', # Updated title\n",
        "    xaxis_range=[-1, 1], # Set x-axis range strictly from -1 to 1\n",
        "    xaxis_title='← Planning | Implementing →', # Updated x-axis title\n",
        "    yaxis_visible=False, # Hide y-axis\n",
        "    yaxis_showticklabels=False, # Hide y-axis tick labels\n",
        "    height=150, # Set a reasonable height for the bar chart\n",
        "    width=800, # Set a reasonable width\n",
        "    margin=dict(l=20, r=20, t=50, b=20), # Adjust margins\n",
        "    title_x=0.5 # Center the title\n",
        ")\n",
        "\n",
        "fig_submitted_bar.show()\n",
        "\n",
        "save_fig(fig_submitted_bar, 'resolution_balance.html')\n"
      ],
      "metadata": {
        "id": "UhCJQCVaHK7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RB evolution"
      ],
      "metadata": {
        "id": "tvjZ5MxXyoOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from datetime import date, timedelta, datetime, timezone\n",
        "\n",
        "# Ensure start_date and selected_timedelta are available (from previous cells)\n",
        "if 'start_date' not in globals():\n",
        "    start_date = datetime.now(timezone.utc) - timedelta(weeks=1) # Fallback to 1 week if not defined\n",
        "\n",
        "end_date_for_range = datetime.now(timezone.utc).date() # Current date\n",
        "start_date_for_range = start_date.date() # Date part of the fetching start_date\n",
        "\n",
        "all_dates = [start_date_for_range + timedelta(days=x) for x in range((end_date_for_range - start_date_for_range).days + 1)]\n",
        "\n",
        "daily_rb_data = []\n",
        "\n",
        "for current_date in all_dates:\n",
        "    # Get daily issues raised and PRs opened from the dictionaries\n",
        "    daily_issues_raised = daily_issues_raised_count.get(current_date, 0)\n",
        "    daily_prs_opened = daily_prs_opened_count.get(current_date, 0)\n",
        "\n",
        "    # Calculate daily RB\n",
        "    total_daily_activity_for_rb = daily_issues_raised + daily_prs_opened\n",
        "    if total_daily_activity_for_rb > 0:\n",
        "        daily_rb = (daily_prs_opened - daily_issues_raised) / total_daily_activity_for_rb\n",
        "    else:\n",
        "        daily_rb = 0.0 # No relevant activity for the day\n",
        "\n",
        "    daily_rb_data.append({'Date': current_date, 'Daily RB': daily_rb})\n",
        "\n",
        "df_daily_rb = pd.DataFrame(daily_rb_data)\n",
        "df_daily_rb['Date'] = pd.to_datetime(df_daily_rb['Date'])\n",
        "\n",
        "# Create the smoothed line chart for daily RB\n",
        "fig = px.line(\n",
        "    df_daily_rb,\n",
        "    x='Date',\n",
        "    y='Daily RB',\n",
        "    title='Daily Resolution Balance (RB) over Time',\n",
        "    labels={'Daily RB': 'Resolution Balance (RB)', 'Date': 'Date'},\n",
        "    hover_data={'Daily RB': ':.2f'},\n",
        "    line_shape='spline'\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='Resolution Balance (RB)',\n",
        "    hovermode='x unified',\n",
        "    yaxis_range=[-1.1, 1.1]\n",
        ")\n",
        "\n",
        "fig.update_xaxes(dtick=\"D1\", tickformat=\"%b %d\") # Daily ticks, format as \"Month Day\"\n",
        "\n",
        "# Get the last date for the arrow placement\n",
        "last_date = df_daily_rb['Date'].iloc[-1]\n",
        "\n",
        "# Add an annotation for the 'Implementing' arrow (pointing up)\n",
        "fig.add_annotation(\n",
        "    x=last_date,\n",
        "    y=0.75,  # Arrowhead at y=0.75\n",
        "    ax=last_date,\n",
        "    ay=0.05,    # Arrow tail slightly above y=0\n",
        "    showarrow=True,\n",
        "    arrowhead=3,\n",
        "    arrowsize=1.5,\n",
        "    arrowwidth=1,\n",
        "    arrowcolor='blue', # Blue for implementing\n",
        "    text=\"\", # No text for the arrow itself\n",
        "    xref='x',\n",
        "    yref='y',\n",
        "    axref='x',\n",
        "    ayref='y',\n",
        "    xshift=10 # Shift arrow slightly to the right\n",
        ")\n",
        "\n",
        "# Add a separate annotation for the text \"Implementing\"\n",
        "fig.add_annotation(\n",
        "    x=last_date,\n",
        "    y=0.95,  # Position the text higher, near y=1\n",
        "    text='Implementing',\n",
        "    showarrow=False, # No arrow for the text\n",
        "    yanchor='bottom', # Anchor the bottom of the text to its y-position\n",
        "    font=dict(color='blue'), # Set text color to blue\n",
        "    xref='x',\n",
        "    yref='y',\n",
        "    xshift=10 # Shift text slightly to the right\n",
        ")\n",
        "\n",
        "# Add an annotation for the 'Planning' arrow (pointing down)\n",
        "fig.add_annotation(\n",
        "    x=last_date,\n",
        "    y=-0.75, # Arrowhead at y=-0.75\n",
        "    ax=last_date,\n",
        "    ay=-0.05, # Arrow tail slightly below y=0\n",
        "    showarrow=True,\n",
        "    arrowhead=3, # Plotly automatically orients arrowhead=3 based on direction\n",
        "    arrowsize=1.5,\n",
        "    arrowwidth=1,\n",
        "    arrowcolor='orange', # Orange for planning\n",
        "    text=\"\", # No text for the arrow itself\n",
        "    xref='x',\n",
        "    yref='y',\n",
        "    axref='x',\n",
        "    ayref='y',\n",
        "    xshift=10 # Shift arrow slightly to the right\n",
        ")\n",
        "\n",
        "# Add a separate annotation for the text \"Planning\"\n",
        "fig.add_annotation(\n",
        "    x=last_date,\n",
        "    y=-0.85, # Position the text below the arrow head (-0.75)\n",
        "    text='Planning',\n",
        "    showarrow=False, # No arrow for the text\n",
        "    yanchor='top', # Anchor the top of the text to its y-position\n",
        "    font=dict(color='orange'), # Set text color to orange\n",
        "    xref='x',\n",
        "    yref='y',\n",
        "    xshift=10 # Shift text slightly to the right\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "save_fig(fig, 'daily_rb_line_chart.html')\n",
        "\n",
        "print(\"Daily Resolution Balance (RB) chart generated.\")"
      ],
      "metadata": {
        "id": "U8EI1T-8xsZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Develoment tide chart : WB x RB"
      ],
      "metadata": {
        "id": "iKfdKtcHTfb0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8vEIYLBZ9Pn"
      },
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "\n",
        "# 2. Create a Pandas DataFrame named df_metrics with 'EB' (submitted_index) and 'WB' (aggregate_pi)\n",
        "# Ensure rb_index and wb_index are available from previous cells\n",
        "df_metrics = pd.DataFrame({\n",
        "    'RB': [rb_index],\n",
        "    'WB': [wb_index]\n",
        "})\n",
        "\n",
        "# 1. Calculate the polar coordinates r (radius) and theta_degrees (angle in degrees)\n",
        "#    from the EB and WB values in df_metrics.\n",
        "df_metrics['r'] = np.sqrt(df_metrics['RB']**2 + df_metrics['WB']**2)\n",
        "df_metrics['theta_degrees'] = np.degrees(np.arctan2(df_metrics['RB'], df_metrics['WB']))\n",
        "\n",
        "# 2. Initialize the figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add a separate trace for the line from the origin to the point (added first to be in the background)\n",
        "fig.add_trace(go.Scatterpolar(\n",
        "    r=[0, df_metrics['r'].iloc[0]], # Draw line from origin (r=0) to the point's r\n",
        "    theta=[df_metrics['theta_degrees'].iloc[0], df_metrics['theta_degrees'].iloc[0]], # Maintain same angle\n",
        "    mode='lines',\n",
        "    line=dict(width=1, color='black', dash='dot'), # Updated line style: thin, black, dotted\n",
        "    name='Handle from Origin'\n",
        "))\n",
        "\n",
        "# Add the trace for the marker and its text (added second to be on top)\n",
        "fig.add_trace(go.Scatterpolar(\n",
        "    r=df_metrics['r'],\n",
        "    theta=df_metrics['theta_degrees'],\n",
        "    mode='markers+text', # Restored mode to markers+text\n",
        "    marker=dict(symbol='circle-open-dot', size=10, color='black', line=dict(width=2, color='black')), # Reduced size to 10 and line width to 2\n",
        "    text=[f'RB: {df_metrics['RB'].iloc[0]:.2f}<br>WB: {df_metrics['WB'].iloc[0]:.2f}'],\n",
        "    textposition='top center',\n",
        "    name='Current State'\n",
        "))\n",
        "\n",
        "# 3. Update the layout of the figure to configure the polar axes\n",
        "fig.update_layout(\n",
        "    title_text='Project tide chart (WB x RB)', # Updated title\n",
        "    title_x=0.5,\n",
        "    height=600,\n",
        "    width=800,\n",
        "    polar=dict(\n",
        "        radialaxis=dict(range=[0, np.sqrt(2) * 1.05]), # Adjusted range to accommodate sqrt(2) and a bit more for padding\n",
        "        angularaxis=dict(\n",
        "            rotation=90, # To align 0 degrees with 'Up' (+WB)\n",
        "            direction='clockwise', # For clockwise angle increase\n",
        "            tickvals=[0, 90, 180, 270],\n",
        "            ticktext=['Clearing (+WB)', 'Implementing (+RB)', 'Accumulating (-WB)', 'Planning (-RB)']\n",
        "        )\n",
        "    ),\n",
        "    showlegend=False # Hide legend as it's a single point and handle\n",
        ")\n",
        "\n",
        "# 6. Display the plot\n",
        "fig.show()\n",
        "\n",
        "save_fig(fig, 'wb_rb_map.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cellname : wb-rb-time\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "from datetime import date, timedelta, datetime, timezone\n",
        "\n",
        "# Ensure start_date and selected_timedelta are available (from previous cells)\n",
        "if 'start_date' not in globals():\n",
        "    start_date = datetime.now(timezone.utc) - timedelta(weeks=1) # Fallback to 1 week if not defined\n",
        "\n",
        "end_date_for_range = datetime.now(timezone.utc).date() # Current date\n",
        "start_date_for_range = start_date.date() # Date part of the fetching start_date\n",
        "\n",
        "all_dates = [start_date_for_range + timedelta(days=x) for x in range((end_date_for_range - start_date_for_range).days + 1)]\n",
        "\n",
        "daily_metrics_wb_rb = []\n",
        "\n",
        "for current_date in all_dates:\n",
        "    # Get daily WB and RB from the dataframes (assume df_daily_wb and df_daily_rb are available)\n",
        "    # Ensure current_date is compared with the date part of the DataFrame's 'Date' column\n",
        "    daily_wb_val = df_daily_wb.loc[df_daily_wb['Date'].dt.date == current_date, 'Daily WB'].iloc[0] if current_date in df_daily_wb['Date'].dt.date.values else 0.0\n",
        "    daily_rb_val = df_daily_rb.loc[df_daily_rb['Date'].dt.date == current_date, 'Daily RB'].iloc[0] if current_date in df_daily_rb['Date'].dt.date.values else 0.0\n",
        "\n",
        "    # Calculate norm (r) and normalize it by sqrt(2) to be in [0, 1] range\n",
        "    r_raw = np.sqrt(daily_wb_val**2 + daily_rb_val**2)\n",
        "    norm_normalized = r_raw / np.sqrt(2) if np.sqrt(2) > 0 else 0.0\n",
        "\n",
        "    # Calculate standard angle (theta) in degrees using np.arctan2(y, x)\n",
        "    standard_angle_degrees = np.degrees(np.arctan2(daily_rb_val, daily_wb_val))\n",
        "\n",
        "    # --- Apply Plotly's angular axis transformation to match polar diagram's display ---\n",
        "    # This transformation converts the standard [-180, 180] angle to Plotly's [0, 360] scale\n",
        "    # with rotation=90 and direction='clockwise'.\n",
        "    # The formula is (standard_angle_0_360 - 90 + 360) % 360\n",
        "    # First, convert standard_angle_degrees to [0, 360) range\n",
        "    standard_angle_0_360 = (standard_angle_degrees + 360) % 360\n",
        "\n",
        "    # Then apply the transformation for Plotly's axis (rotation=90, clockwise direction)\n",
        "    # Plotly's 0 is standard 90\n",
        "    # Plotly's 90 is standard 180\n",
        "    # Plotly's 180 is standard 270 (or -90)\n",
        "    # Plotly's 270 is standard 0\n",
        "    # The transformation is (standard_angle_0_360 - 90 + 360) % 360 to map to the tickvals.\n",
        "    # Let's verify: std 0 -> (0-90+360)%360 = 270 (Clearing is at 270). OK\n",
        "    #              std 90 -> (90-90+360)%360 = 0 (Implementing is at 0). OK\n",
        "    #              std 180 -> (180-90+360)%360 = 90 (Accumulating is at 90). OK\n",
        "    #              std 270 -> (270-90+360)%360 = 180 (Planning is at 180). OK\n",
        "    plotly_mapped_angle = (standard_angle_0_360 - 90 + 360) % 360\n",
        "\n",
        "    daily_metrics_wb_rb.append({\n",
        "        'Date': current_date,\n",
        "        'WB': daily_wb_val,\n",
        "        'RB': daily_rb_val,\n",
        "        'Norm (0-1)': norm_normalized,\n",
        "        'Angle (degrees)': plotly_mapped_angle # Use the transformed angle\n",
        "    })\n",
        "\n",
        "df_daily_wb_rb_evolution = pd.DataFrame(daily_metrics_wb_rb)\n",
        "df_daily_wb_rb_evolution['Date'] = pd.to_datetime(df_daily_wb_rb_evolution['Date'])\n",
        "\n",
        "# Create subplots with secondary y-axis\n",
        "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "# Add traces for WB, RB, Norm (primary y-axis)\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=df_daily_wb_rb_evolution['Date'],\n",
        "    y=df_daily_wb_rb_evolution['WB'],\n",
        "    mode='lines',\n",
        "    name='WB (Workload Balance)',\n",
        "    line=dict(shape='spline'),\n",
        "    hovertemplate='<b>Date:</b> %{x}<br><b>WB:</b> %{y:.2f}<extra></extra>'\n",
        "), secondary_y=False)\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=df_daily_wb_rb_evolution['Date'],\n",
        "    y=df_daily_wb_rb_evolution['RB'],\n",
        "    mode='lines',\n",
        "    name='RB (Resolution Balance)',\n",
        "    line=dict(shape='spline'),\n",
        "    hovertemplate='<b>Date:</b> %{x}<br><b>RB:</b> %{y:.2f}<extra></extra>'\n",
        "), secondary_y=False)\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=df_daily_wb_rb_evolution['Date'],\n",
        "    y=df_daily_wb_rb_evolution['Norm (0-1)'],\n",
        "    mode='lines',\n",
        "    name='Magnitude (Norm normalized)',\n",
        "    line=dict(shape='spline'),\n",
        "    hovertemplate='<b>Date:</b> %{x}<br><b>Magnitude:</b> %{y:.2f}<extra></extra>'\n",
        "), secondary_y=False)\n",
        "\n",
        "# Add trace for Angle (secondary y-axis)\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=df_daily_wb_rb_evolution['Date'],\n",
        "    y=df_daily_wb_rb_evolution['Angle (degrees)'],\n",
        "    mode='lines',\n",
        "    name='Angle (degrees)',\n",
        "    line=dict(shape='spline', dash='dot', color='purple'),\n",
        "    hovertemplate='<b>Date:</b> %{x}<br><b>Angle:</b> %{y:.1f} degrees<extra></extra>'\n",
        "), secondary_y=True)\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title_text='Daily Evolution of WB, RB, Magnitude and Angle',\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='WB, RB, Magnitude (Range -1 to 1)', # Corrected title to reflect single scale range\n",
        "    yaxis2_title='Angle (degrees)',\n",
        "    legend_title='Metric',\n",
        "    hovermode='x unified',\n",
        "    height=600\n",
        ")\n",
        "\n",
        "fig.update_xaxes(dtick=\"D1\", tickformat=\"%b %d\") # Daily ticks, format as \"Month Day\"\n",
        "\n",
        "# Adjust y-axis ranges for clarity\n",
        "fig.update_yaxes(range=[-1.1, 1.1], secondary_y=False) # WB, RB, Norm range\n",
        "fig.update_yaxes(range=[0, 360], secondary_y=True, dtick=45) # Angle range (0 to 360 degrees), with 45-degree ticks\n",
        "\n",
        "fig.show()\n",
        "save_fig(fig, 'wb_rb_time_evolution.html')\n",
        "\n",
        "print(\"Daily WB, RB, Norm and Angle evolution chart generated.\")"
      ],
      "metadata": {
        "id": "baBIYotSy9Wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c66825d1"
      },
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go # Import go for update_traces\n",
        "import numpy as np\n",
        "\n",
        "# Ensure df_daily_wb and df_daily_rb are available from previous cells\n",
        "# Merge the daily WB and RB dataframes on 'Date'\n",
        "df_wb_rb_temporal = pd.merge(df_daily_wb, df_daily_rb, on='Date', how='inner')\n",
        "\n",
        "# Add an identifier for the start and end points for better visualization\n",
        "df_wb_rb_temporal['Point Type'] = 'Intermediate'\n",
        "if not df_wb_rb_temporal.empty:\n",
        "    df_wb_rb_temporal.loc[df_wb_rb_temporal.index[0], 'Point Type'] = 'Start'\n",
        "    df_wb_rb_temporal.loc[df_wb_rb_temporal.index[-1], 'Point Type'] = 'End'\n",
        "\n",
        "# Add a sequential number for each point\n",
        "df_wb_rb_temporal['Point Number'] = range(1, len(df_wb_rb_temporal) + 1)\n",
        "\n",
        "# --- Calculate polar coordinates (r, theta) for all points ---\n",
        "# r (radius) = distance from center\n",
        "df_wb_rb_temporal['r'] = np.sqrt(df_wb_rb_temporal['Daily WB']**2 + df_wb_rb_temporal['Daily RB']**2)\n",
        "# theta (angle) = angle in degrees\n",
        "# Adjust atan2 to place +WB at 0 degrees and +RB at 90 degrees\n",
        "df_wb_rb_temporal['theta_degrees'] = np.degrees(np.arctan2(df_wb_rb_temporal['Daily RB'], df_wb_rb_temporal['Daily WB']))\n",
        "\n",
        "# Initialize figure\n",
        "fig = go.Figure()\n",
        "\n",
        "if not df_wb_rb_temporal.empty:\n",
        "    # Add the main continuous line for all points (drawn first so points are on top)\n",
        "    fig.add_trace(go.Scatterpolar(\n",
        "        r=df_wb_rb_temporal['r'],\n",
        "        theta=df_wb_rb_temporal['theta_degrees'],\n",
        "        mode='lines', # Only lines for the main path\n",
        "        line=dict(shape='spline', color='blue', width=2), # Smooth blue line\n",
        "        name='Daily Evolution',\n",
        "        hoverinfo='none', # No hover for the line itself\n",
        "        showlegend=False\n",
        "    ))\n",
        "\n",
        "    # --- Add the Global Project point ---\n",
        "    # Ensure df_metrics (containing global rb_index, wb_index, r, theta_degrees) is available\n",
        "    if 'df_metrics' in globals() and not df_metrics.empty:\n",
        "        global_r = df_metrics['r'].iloc[0]\n",
        "        global_theta_degrees = df_metrics['theta_degrees'].iloc[0]\n",
        "        global_wb = df_metrics['WB'].iloc[0]\n",
        "        global_rb = df_metrics['RB'].iloc[0]\n",
        "\n",
        "        fig.add_trace(go.Scatterpolar(\n",
        "            r=[global_r],\n",
        "            theta=[global_theta_degrees],\n",
        "            mode='markers',\n",
        "            marker=dict(symbol='circle-dot', size=12, color='black', line=dict(width=2, color='white')),\n",
        "            name='Project', # Changed from 'Project Global' to 'Project'\n",
        "            hoverinfo='text',\n",
        "            text=f\"Project<br>WB: {global_wb:.2f}<br>RB: {global_rb:.2f}\" # Changed text too\n",
        "        ))\n",
        "\n",
        "    # Add markers for all points with their point number as text\n",
        "    fig.add_trace(go.Scatterpolar(\n",
        "        r=df_wb_rb_temporal['r'],\n",
        "        theta=df_wb_rb_temporal['theta_degrees'],\n",
        "        mode='markers+text', # Markers and text (point numbers)\n",
        "        marker=dict(symbol='circle', size=8, color='rgba(0,0,255,0.5)', line=dict(width=1, color='blue')),\n",
        "        text=df_wb_rb_temporal['Point Number'], # Show point number as text\n",
        "        textposition='top center',\n",
        "        name='Daily Points',\n",
        "        hoverinfo='text',\n",
        "        texttemplate='<b>%{text}</b>', # Make point number text bold\n",
        "        hovertext=df_wb_rb_temporal.apply(lambda row:\n",
        "            f\"Date: {row['Date'].strftime('%Y-%m-%d')}<br>\" +\n",
        "            f\"WB: {row['Daily WB']:.2f}<br>\" +\n",
        "            f\"RB: {row['Daily RB']:.2f}<br>\" +\n",
        "            f\"Point: {row['Point Number']}\", axis=1),\n",
        "        showlegend=False # No legend for generic points, only start/end\n",
        "    ))\n",
        "\n",
        "    # Add specific trace for the 'Start' point to highlight it\n",
        "    start_point = df_wb_rb_temporal[df_wb_rb_temporal['Point Type'] == 'Start']\n",
        "    if not start_point.empty:\n",
        "        fig.add_trace(go.Scatterpolar(\n",
        "            r=start_point['r'],\n",
        "            theta=start_point['theta_degrees'],\n",
        "            mode='markers',\n",
        "            marker=dict(symbol='circle', size=15, color='green', line=dict(width=2, color='black')),\n",
        "            name='Start Point',\n",
        "            hoverinfo='text',\n",
        "            text=start_point.apply(lambda row:\n",
        "                f\"Date: {row['Date'].strftime('%Y-%m-%d')}<br>\" +\n",
        "                f\"WB: {row['Daily WB']:.2f}<br>\" +\n",
        "                f\"RB: {row['Daily RB']:.2f}<br>\" +\n",
        "                f\"Point: {row['Point Number']}\", axis=1)\n",
        "        ))\n",
        "\n",
        "    # Add specific trace for the 'End' point to highlight it\n",
        "    end_point = df_wb_rb_temporal[df_wb_rb_temporal['Point Type'] == 'End']\n",
        "    if not end_point.empty:\n",
        "        fig.add_trace(go.Scatterpolar(\n",
        "            r=end_point['r'],\n",
        "            theta=end_point['theta_degrees'],\n",
        "            mode='markers',\n",
        "            marker=dict(symbol='diamond', size=15, color='red', line=dict(width=2, color='black')),\n",
        "            name='End Point',\n",
        "            hoverinfo='text',\n",
        "            text=end_point.apply(lambda row:\n",
        "                f\"Date: {row['Date'].strftime('%Y-%m-%d')}<br>\" +\n",
        "                f\"WB: {row['Daily WB']:.2f}<br>\" +\n",
        "                f\"RB: {row['Daily RB']:.2f}<br>\" +\n",
        "                f\"Point: {row['Point Number']}\", axis=1)\n",
        "        ))\n",
        "\n",
        "# --- Update layout for polar coordinates ---\n",
        "fig.update_layout(\n",
        "    title_text='Project Tide Phase Diagram (WB vs RB Evolution)',\n",
        "    height=700, # Reduced height\n",
        "    width=800, # Reduced width\n",
        "    showlegend=True,\n",
        "    legend=dict(\n",
        "        orientation='h',\n",
        "        yanchor='bottom',\n",
        "        y=1.02, # Position above the plot\n",
        "        xanchor='right',\n",
        "        x=1\n",
        "    ),\n",
        "    polar=dict(\n",
        "        domain=dict(x=[0.1, 0.9], y=[0.1, 0.9]), # Shrink the polar plot itself to create more empty space\n",
        "        radialaxis=dict(\n",
        "            range=[0, np.sqrt(2) * 1.05], # Adjust range to accommodate sqrt(2) and a bit more for padding\n",
        "            dtick=0.2 # Set tick interval to 0.2\n",
        "        ),\n",
        "        angularaxis=dict(\n",
        "            rotation=90, # To align 0 degrees with 'Up' (+WB)\n",
        "            direction='clockwise', # For clockwise angle increase\n",
        "            tickvals=[0, 90, 180, 270],\n",
        "            ticktext=['Clearing (+WB)', 'Implementing (+RB)', 'Accumulating (-WB)', 'Planning (-RB)']\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "# Removed x/y axis specific updates as they are now handled by polar layout\n",
        "\n",
        "fig.show()\n",
        "\n",
        "save_fig(fig, 'wb_rb_temporal_evolution.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PR completion-time in the period"
      ],
      "metadata": {
        "id": "ttI06KxRMSFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cellname : pr-close-time\n",
        "\n",
        "from datetime import datetime, timezone\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# Ensure selected_timedelta and repo are available from previous cells\n",
        "end_date = datetime.now(timezone.utc)\n",
        "start_date = end_date - selected_timedelta\n",
        "\n",
        "pr_close_times = []\n",
        "\n",
        "# Initialize counters for the new categories\n",
        "opened_before_closed_in_period_count = 0\n",
        "opened_and_closed_in_period_count = 0\n",
        "opened_in_period_still_open_count = 0\n",
        "opened_before_still_open_count = 0\n",
        "\n",
        "print(f\"Analyzing PRs relative to the period: {start_date.strftime('%Y-%m-%d %H:%M:%S')} to {end_date.strftime('%Y-%m-%d %H:%M:%S')}...\")\n",
        "\n",
        "# Fetch ALL pull requests (both open and closed) for comprehensive analysis\n",
        "all_pulls = repo.get_pulls(state='all')\n",
        "\n",
        "for pull in all_pulls:\n",
        "    # Condition for PRs closed within the period\n",
        "    if pull.closed_at and start_date <= pull.closed_at <= end_date:\n",
        "        time_to_close = pull.closed_at - pull.created_at\n",
        "        pr_data = {\n",
        "            'number': pull.number,\n",
        "            'title': pull.title,\n",
        "            'creator': pull.user.login if pull.user else 'N/A',\n",
        "            'created_at': pull.created_at,\n",
        "            'closed_at': pull.closed_at,\n",
        "            'time_to_close_days': time_to_close.total_seconds() / (24 * 3600)\n",
        "        }\n",
        "\n",
        "        # Category: Opened before period and closed in period\n",
        "        if pull.created_at < start_date:\n",
        "            opened_before_closed_in_period_count += 1\n",
        "            pr_data['Category'] = 'Opened before period & closed in period'\n",
        "            pr_close_times.append(pr_data)\n",
        "        # Category: Opened and closed in period\n",
        "        elif start_date <= pull.created_at <= end_date:\n",
        "            opened_and_closed_in_period_count += 1\n",
        "            pr_data['Category'] = 'Opened and closed in period'\n",
        "            pr_close_times.append(pr_data)\n",
        "\n",
        "    # Category: Opened in period and not yet closed\n",
        "    elif start_date <= pull.created_at <= end_date and pull.closed_at is None:\n",
        "        opened_in_period_still_open_count += 1\n",
        "\n",
        "    # Category: Opened before period and still open\n",
        "    elif pull.created_at < start_date and pull.closed_at is None:\n",
        "        opened_before_still_open_count += 1\n",
        "\n",
        "print(\"\\n--- PR Categorization for the Period ---\")\n",
        "print(f\"* PRs opened before period and closed in period: {opened_before_closed_in_period_count}\")\n",
        "print(f\"* PRs opened and closed in period: {opened_and_closed_in_period_count}\")\n",
        "print(f\"* PRs opened in period and not closed: {opened_in_period_still_open_count}\")\n",
        "print(f\"* PRs opened before period and still open: {opened_before_still_open_count}\")\n",
        "\n",
        "df_pr_close_times = pd.DataFrame(pr_close_times)\n",
        "\n",
        "if not df_pr_close_times.empty:\n",
        "    print(f\"\\nFound {len(df_pr_close_times)} PRs closed within the period for detailed analysis.\")\n",
        "    display(df_pr_close_times.head())\n",
        "\n",
        "    print(\"\\n--- Summary of PR Close Times (Closed in Period) ---\")\n",
        "    print(f\"Average time to close PR: {df_pr_close_times['time_to_close_days'].mean():.2f} days\")\n",
        "    print(f\"Median time to close PR: {df_pr_close_times['time_to_close_days'].median():.2f} days\")\n",
        "    print(f\"Minimum time to close PR: {df_pr_close_times['time_to_close_days'].min():.2f} days\")\n",
        "    print(f\"Maximum time to close PR: {df_pr_close_times['time_to_close_days'].max():.2f} days\")\n",
        "\n",
        "    # Calculate nbins to ensure 1-day intervals\n",
        "    max_days = df_pr_close_times['time_to_close_days'].max()\n",
        "    nbins_calculated = int(max_days) + 1 if max_days > 0 else 1 # Ensure at least 1 bin if data exists\n",
        "\n",
        "    # Plotting a histogram of close times with color distinction\n",
        "    fig = px.histogram(df_pr_close_times,\n",
        "                       x='time_to_close_days',\n",
        "                       nbins=nbins_calculated, # Use calculated nbins for 1-day intervals\n",
        "                       color='Category', # Use the new Category column for color\n",
        "                       barmode='stack', # Changed to stack mode\n",
        "                       title='PR completion-time histogram by Category',\n",
        "                       labels={'time_to_close_days': 'Time to Close (Days)', 'count': 'Number of PRs'})\n",
        "    fig.update_traces(opacity=0.75) # Adjust opacity for better visibility of overlaid bars\n",
        "    fig.update_layout(xaxis=dict(dtick=1)) # Set x-axis ticks to 1-unit intervals\n",
        "    fig.show()\n",
        "    save_fig(fig, 'pr_close_times_histogram.html')\n",
        "else:\n",
        "    print(\"\\nNo PRs were closed within the selected period for detailed time analysis.\")"
      ],
      "metadata": {
        "id": "20O31kzZMXxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "import plotly.express as px # Import plotly.express to use its color sequences if needed\n",
        "\n",
        "# Ensure the df_pr_close_times DataFrame is not empty\n",
        "if not df_pr_close_times.empty:\n",
        "    # Define colors for categories (using plotly express default sequence for consistency)\n",
        "    colors = px.colors.qualitative.Plotly # Or choose specific colors: ['blue', 'orange']\n",
        "    color_map = {\n",
        "        'Opened and closed in period': colors[0], # e.g., blue\n",
        "        'Opened before period & closed in period': colors[1] # e.g., orange\n",
        "    }\n",
        "\n",
        "    # Create the figure\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Calculate quartiles for each category\n",
        "    category_quartiles = df_pr_close_times.groupby('Category')['time_to_close_days'].quantile([0.25, 0.5, 0.75]).unstack()\n",
        "    category_quartiles.columns = ['Q1', 'Median', 'Q3']\n",
        "\n",
        "    # Iterate over unique categories and create a trace for each\n",
        "    for i, (category_name, color_val) in enumerate(color_map.items()):\n",
        "        df_category = df_pr_close_times[df_pr_close_times['Category'] == category_name]\n",
        "        if not df_category.empty:\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=df_category['number'],\n",
        "                y=df_category['time_to_close_days'],\n",
        "                mode='markers',\n",
        "                marker=dict(\n",
        "                    size=10,\n",
        "                    opacity=0.8,\n",
        "                    line=dict(width=1, color='DarkSlateGrey'),\n",
        "                    color=color_val # Set color explicitly for this trace\n",
        "                ),\n",
        "                name=category_name, # Set the name for the legend entry\n",
        "                hoverinfo='text',\n",
        "                # Custom hover text to display detailed PR information\n",
        "                text=df_category.apply(lambda row:\n",
        "                    f\"PR #{row['number']}<br>\" +\n",
        "                    f\"Title: {row['title']}<br>\" +\n",
        "                    f\"Creator: {row['creator']}<br>\" +\n",
        "                    f\"Created: {row['created_at'].strftime('%Y-%m-%d %H:%M:%S')}<br>\" +\n",
        "                    f\"Closed: {row['closed_at'].strftime('%Y-%m-%d %H:%M:%S')}<br>\" +\n",
        "                    f\"Time to Close: {row['time_to_close_days']:.2f} days<br>\" +\n",
        "                    f\"Category: {row['Category']}\", axis=1)\n",
        "            ))\n",
        "\n",
        "            # Add a transparent rectangle for IQR (Q1 to Q3) for this category\n",
        "            if category_name in category_quartiles.index:\n",
        "                q1 = category_quartiles.loc[category_name, 'Q1']\n",
        "                q3 = category_quartiles.loc[category_name, 'Q3']\n",
        "                median = category_quartiles.loc[category_name, 'Median']\n",
        "\n",
        "                fig.add_shape(type='rect',\n",
        "                              xref='paper', yref='y',\n",
        "                              x0=0, x1=0.75, # Spans the constrained plot area width\n",
        "                              y0=q1, y1=q3,\n",
        "                              fillcolor=color_val,\n",
        "                              opacity=0.15, # Transparent fill\n",
        "                              layer='below',\n",
        "                              line_width=0)\n",
        "\n",
        "                # Add median line for this category\n",
        "                fig.add_hline(y=median,\n",
        "                              line_dash='dash',\n",
        "                              line_color=color_val,\n",
        "                              line_width=1)\n",
        "\n",
        "                # Define x-position for annotations\n",
        "                x_pos_base = 0.75 + 0.01 # Start just outside the new x-axis domain\n",
        "\n",
        "                # Annotation for Median\n",
        "                fig.add_annotation(\n",
        "                    xref='paper', yref='y',\n",
        "                    x=x_pos_base, y=median,\n",
        "                    text=f\"Median: {median:.2f} days\",\n",
        "                    showarrow=False,\n",
        "                    font=dict(color=color_val, size=12), # Increased font size to 12\n",
        "                    xanchor='left', yanchor='middle'\n",
        "                )\n",
        "\n",
        "\n",
        "    # Update layout for title and axis labels\n",
        "    fig.update_layout(\n",
        "        title='PR completion time with IQR by Category',\n",
        "        xaxis_title='Pull Request Number',\n",
        "        yaxis_title='Time to Close (Days)',\n",
        "        height=600,\n",
        "        width=1000, # Increased canvas width\n",
        "        hovermode='closest',\n",
        "        showlegend=True, # Show legend to differentiate categories\n",
        "        legend_title='PR Category',\n",
        "        legend=dict(\n",
        "            orientation='h', # Horizontal orientation for bottom placement\n",
        "            yanchor='bottom',   # Anchor to the bottom\n",
        "            y=-0.3,             # Position further below the plot area\n",
        "            xanchor='center',\n",
        "            x=0.5              # Center horizontally\n",
        "        ),\n",
        "        xaxis=dict(\n",
        "            domain=[0, 0.75] # Constrain x-axis to 75% of the figure width\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # --- NEW: Add annotations for Q1, Median, Q3 below the legend ---\n",
        "    # Adjust height to accommodate these new annotations\n",
        "    fig.update_layout(height=800) # Increase figure height here\n",
        "\n",
        "    # Starting Y position for the first annotation block, below the legend (-0.3)\n",
        "    annotation_start_y_pos = -0.45 # Relative to paper coordinates\n",
        "\n",
        "    for i, (category_name, color_val) in enumerate(color_map.items()):\n",
        "        if category_name in category_quartiles.index:\n",
        "            q1 = category_quartiles.loc[category_name, 'Q1']\n",
        "            median = category_quartiles.loc[category_name, 'Median']\n",
        "            q3 = category_quartiles.loc[category_name, 'Q3']\n",
        "\n",
        "            # Format the text for the annotation\n",
        "            # Using HTML for bold category name\n",
        "            annotation_text = f\"<b>{category_name}</b><br>\" \\\n",
        "                              f\"Q1: {q1:.2f} days<br>\" \\\n",
        "                              f\"Median: {median:.2f} days<br>\" \\\n",
        "                              f\"Q3: {q3:.2f} days\"\n",
        "\n",
        "            fig.add_annotation(\n",
        "                xref='paper', yref='paper',\n",
        "                x=0.5, # Center horizontally below the legend\n",
        "                y=annotation_start_y_pos - (0.15 * i), # Stack vertically, adjust 0.15 for spacing between blocks\n",
        "                text=annotation_text,\n",
        "                showarrow=False,\n",
        "                font=dict(color=color_val, size=10),\n",
        "                xanchor='center', yanchor='top' # Anchor the top of the text block to the y position\n",
        "            )\n",
        "\n",
        "    # Display the plot\n",
        "    fig.show()\n",
        "    save_fig(fig, 'pr_close_time_scatter_plot.html')\n",
        "\n",
        "    # Print quartile values for each category\n",
        "    print(\"\\n--- Quartile Values per PR Category ---\")\n",
        "    for category_name in category_quartiles.index:\n",
        "        q1_val = category_quartiles.loc[category_name, 'Q1']\n",
        "        median_val = category_quartiles.loc[category_name, 'Median']\n",
        "        q3_val = category_quartiles.loc[category_name, 'Q3']\n",
        "        print(f\"  Category: {category_name}\")\n",
        "        print(f\"    Q1: {q1_val:.2f} days\")\n",
        "        print(f\"    Median: {median_val:.2f} days\")\n",
        "        print(f\"    Q3: {q3_val:.2f} days\")\n",
        "\n",
        "else:\n",
        "    print(\"No PRs were opened and closed within the selected period to plot.\")"
      ],
      "metadata": {
        "id": "jMcVH9HESHeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "# Data from the previous cell's execution\n",
        "pr_category_data = {\n",
        "    'Category': [\n",
        "        'Opened before period & closed in period',\n",
        "        'Opened & closed in period',\n",
        "        'Opened in period & not closed',\n",
        "        'Opened before period & still open'\n",
        "    ],\n",
        "    'Count': [\n",
        "        opened_before_closed_in_period_count,\n",
        "        opened_and_closed_in_period_count,\n",
        "        opened_in_period_still_open_count,\n",
        "        opened_before_still_open_count\n",
        "    ]\n",
        "}\n",
        "\n",
        "df_pr_categories = pd.DataFrame(pr_category_data)\n",
        "\n",
        "# Filter out categories with 0 count to avoid empty slices\n",
        "df_pr_categories = df_pr_categories[df_pr_categories['Count'] > 0]\n",
        "\n",
        "if df_pr_categories.empty:\n",
        "    print(\"No PR activity found to categorize for the pie chart.\")\n",
        "else:\n",
        "    fig = px.pie(\n",
        "        df_pr_categories,\n",
        "        values='Count',\n",
        "        names='Category', # Use the raw category name for slice identification\n",
        "        title='Distribution of PR status',\n",
        "        hole=0.3, # Optional: creates a donut chart\n",
        "        color_discrete_sequence=px.colors.qualitative.Pastel # Use a pastel color sequence\n",
        "    )\n",
        "\n",
        "    fig.update_traces(\n",
        "        textposition='outside',\n",
        "        textinfo='percent+label+value',\n",
        "        textfont_color='black'\n",
        "        # Optional: rotate labels if they overlap\n",
        "        # insidetextorientation='radial'\n",
        "    )\n",
        "    fig.update_layout(\n",
        "        showlegend=True,\n",
        "        legend=dict(\n",
        "            orientation='h', # Horizontal orientation for bottom placement\n",
        "            yanchor='bottom',   # Anchor to the bottom\n",
        "            y=-0.3,             # Position further below the plot area\n",
        "            xanchor='center',\n",
        "            x=0.5              # Center horizontally\n",
        "        )\n",
        "    )\n",
        "    fig.show()\n",
        "    save_fig(fig, 'pr_categories_pie_chart.html')\n"
      ],
      "metadata": {
        "id": "74vrX9wPN94D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEVELOPERS OVERVIEW"
      ],
      "metadata": {
        "id": "-PiwsbYFHJr3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Engagement"
      ],
      "metadata": {
        "id": "AV-VrWYcVCkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Engament chart"
      ],
      "metadata": {
        "id": "K1gq1rrxJjgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cellname : engagement-plot\n",
        "\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "# Calculate the 'Discussion' metric\n",
        "df_plot_discussion = df_people.copy()\n",
        "df_plot_discussion['Discussion'] = df_plot_discussion['issues_opened_count'] + df_plot_discussion['issue_comment_event_count']\n",
        "\n",
        "# Filter out developers who have no activity in either PRs Opened or Discussion\n",
        "df_plot_discussion = df_plot_discussion[\n",
        "    (df_plot_discussion['pull_requests_opened_count'] > 0) |\n",
        "    (df_plot_discussion['Discussion'] > 0)\n",
        "].copy()\n",
        "\n",
        "# --- Aggregate data for unique (PRs Opened, Discussion) coordinates ---\n",
        "aggregated_df = df_plot_discussion.groupby(['pull_requests_opened_count', 'Discussion']).agg(\n",
        "    github_logins=('github_login', lambda x: ', '.join(x)),\n",
        "    names=('name', lambda x: ', '.join(x)),\n",
        "    num_devs=('github_login', 'count')\n",
        ").reset_index()\n",
        "\n",
        "# Create a unique key for each aggregated point for coloring with distinct discrete colors\n",
        "# This key will also be used initially as the legend name before being replaced by logins\n",
        "aggregated_df['point_category'] = 'PRs ' + aggregated_df['pull_requests_opened_count'].astype(str) + ', Discussion ' + aggregated_df['Discussion'].astype(str)\n",
        "\n",
        "# Add a new column to aggregated_df to determine marker style\n",
        "aggregated_df['marker_type'] = aggregated_df['num_devs'].apply(lambda x: 'Single Contributor' if x == 1 else 'Multiple Contributors')\n",
        "\n",
        "# Define a custom color sequence using only Dark24\n",
        "custom_color_sequence = px.colors.qualitative.Dark24\n",
        "\n",
        "# Create the scatter plot with aggregated data\n",
        "fig = px.scatter(\n",
        "    aggregated_df,\n",
        "    x='pull_requests_opened_count',\n",
        "    y='Discussion',\n",
        "    size='num_devs', # Size of marker based on number of developers at this point\n",
        "    color='point_category', # Use the categorical key for distinct colors\n",
        "    color_discrete_sequence=custom_color_sequence, # Use the custom, extended palette\n",
        "    symbol='marker_type', # Use the new column to determine marker symbol (solid vs. outlined)\n",
        "    symbol_map={'Single Contributor': 'circle', 'Multiple Contributors': 'circle-open'}, # Explicitly map styles\n",
        "    text=None, # Removed text labels from directly on the bubbles\n",
        "    title='Engagement chart',\n",
        "    labels={\n",
        "        'pull_requests_opened_count': 'PRs Opened',\n",
        "        'Discussion': 'Discussion (Issues Opened + Comments)',\n",
        "        'num_devs': 'Number of Contributors',\n",
        "        'github_logins': 'Contributors',\n",
        "        'point_category': 'Contributors' # This will be the initial legend title, replaced later\n",
        "    },\n",
        "    hover_name='names', # Show aggregated names on hover\n",
        "    hover_data={\n",
        "        'github_logins': True, # Also show aggregated logins on hover\n",
        "        'num_devs': True,\n",
        "        'pull_requests_opened_count': True,\n",
        "        'Discussion': True,\n",
        "        'names': False,\n",
        "        'point_category': False\n",
        "    }\n",
        ")\n",
        "\n",
        "# The update_traces for textposition will now do nothing as text=None\n",
        "# fig.update_traces(textposition='middle right', textfont_size=10)\n",
        "\n",
        "# Increase border thickness for 'circle-open' symbols, inheriting color\n",
        "fig.update_traces(marker=dict(line=dict(width=8, color='black')), selector=dict(symbol='circle-open'))\n",
        "\n",
        "# Create a mapping from point_category to github_logins for legend renaming\n",
        "category_to_logins_map = aggregated_df.set_index('point_category')['github_logins'].to_dict()\n",
        "\n",
        "# Function to extract the base point_category from the full trace name\n",
        "def get_base_point_category(full_trace_name):\n",
        "    # The trace name will be 'point_category_value, marker_type_value' if both color and symbol are used\n",
        "    # We want to split at the last comma to get 'point_category_value'\n",
        "    parts = full_trace_name.rsplit(', ', 1)\n",
        "    if len(parts) > 1 and (parts[-1] == 'Single Contributor' or parts[-1] == 'Multiple Contributors'):\n",
        "        return parts[0]\n",
        "    return full_trace_name # Fallback if name doesn't match expected pattern (e.g., if only color is used)\n",
        "\n",
        "# Update the legend entry names to show github_logins\n",
        "fig.for_each_trace(lambda trace: trace.update(name=category_to_logins_map[get_base_point_category(trace.name)]))\n",
        "\n",
        "fig.update_layout(\n",
        "    height=1200, # Increased height to accommodate the legend\n",
        "    xaxis_title='PRs Opened',\n",
        "    yaxis_title='Discussion (Issues Opened + Comments)',\n",
        "    legend_title='Contributors',\n",
        "    legend=dict(\n",
        "        orientation='v', # Vertical orientation\n",
        "        yanchor='top', # Anchor to the top\n",
        "        y=-0.2, # Position below the chart (adjust as needed)\n",
        "        xanchor='left',\n",
        "        x=0\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "save_fig(fig, 'engagement_plot.html')\n"
      ],
      "metadata": {
        "id": "hRXTbQz0J7LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Engagement magnitude (EM) index"
      ],
      "metadata": {
        "id": "NSNiOp8IJqrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cellname :em-agregado\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- 1. Calculate Project-wide totals for Discussion and PRs Opened ---\n",
        "# Summing across all individuals in df_people\n",
        "total_project_discussion = df_people['issues_opened_count'].sum() + df_people['issue_comment_event_count'].sum()\n",
        "total_project_prs_opened = df_people['pull_requests_opened_count'].sum()\n",
        "\n",
        "print(f\"Total Project Discussion (Issues Opened + Comments): {total_project_discussion}\")\n",
        "print(f\"Total Project PRs Opened: {total_project_prs_opened}\")\n",
        "\n",
        "# --- 2. Retrieve max individual values for normalization ---\n",
        "# These values (max_discussion, max_prs_opened) refer to the highest individual contribution.\n",
        "# Calculate them from df_people\n",
        "df_people_discussion_temp = df_people['issues_opened_count'] + df_people['issue_comment_event_count']\n",
        "max_discussion = df_people_discussion_temp.max()\n",
        "max_prs_opened = df_people['pull_requests_opened_count'].max()\n",
        "\n",
        "# Handle division by zero if no activity of a certain type\n",
        "if max_discussion == 0: max_discussion = 1\n",
        "if max_prs_opened == 0: max_prs_opened = 1\n",
        "\n",
        "# --- 3. Calculate count of active contributors for proper scaling ---\n",
        "# An 'active' contributor for EM is one with >0 discussion or >0 PRs opened\n",
        "active_contributors_for_em_count = df_people[\n",
        "    (df_people_discussion_temp > 0) | (df_people['pull_requests_opened_count'] > 0)\n",
        "].shape[0]\n",
        "\n",
        "# Ensure at least one active contributor to avoid division by zero later\n",
        "if active_contributors_for_em_count == 0: active_contributors_for_em_count = 1\n",
        "\n",
        "\n",
        "# --- 4. Normalize Project totals using individual maximums ---\n",
        "# These are 'project totals relative to the best individual performance'\n",
        "normalized_project_discussion = total_project_discussion / max_discussion\n",
        "normalized_project_prs_opened = total_project_prs_opened / max_prs_opened\n",
        "\n",
        "print(f\"Normalized Project Discussion (relative to max individual): {normalized_project_discussion:.2f}\")\n",
        "print(f\"Normalized Project PRs Opened (relative to max individual): {normalized_project_prs_opened:.2f}\")\n",
        "\n",
        "# --- 5. Calculate Project EM (raw magnitude) ---\n",
        "project_em_raw = np.sqrt(normalized_project_discussion**2 + normalized_project_prs_opened**2)\n",
        "\n",
        "# --- 6. Scale Project EM to be between 0 and 1 ---\n",
        "# The maximum possible value for project_em_raw, if all 'active_contributors_for_em_count'\n",
        "# were performing at 'max_discussion' and 'max_prs_opened' level, would be\n",
        "# active_contributors_for_em_count * sqrt(max_discussion^2/max_discussion^2 + max_prs_opened^2/max_prs_opened^2)\n",
        "# = active_contributors_for_em_count * sqrt(1+1) = active_contributors_for_em_count * sqrt(2)\n",
        "max_theoretical_project_em_raw = active_contributors_for_em_count * np.sqrt(2)\n",
        "\n",
        "if max_theoretical_project_em_raw > 0:\n",
        "    project_em = project_em_raw / max_theoretical_project_em_raw\n",
        "else:\n",
        "    project_em = 0.0 # If no activity, project EM is 0\n",
        "\n",
        "print(f\"\\nProject Engagement Magnitude (EM) (0-1 scaled): {project_em:.2f}\")"
      ],
      "metadata": {
        "id": "H27fmKkNGTum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cellname : engagement-index\n",
        "\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Calculate the 'Discussion' metric if not already present\n",
        "df_bci = df_people.copy()\n",
        "df_bci['Discussion'] = df_bci['issues_opened_count'] + df_bci['issue_comment_event_count']\n",
        "\n",
        "# Filter out developers with BCI equal to 0 (no contributions in these categories)\n",
        "df_em_plot = df_bci[((df_bci['Discussion'] > 0) | (df_bci['pull_requests_opened_count'] > 0))].copy()\n",
        "\n",
        "# --- Normalize coordinates (Discussion, PRs Opened) between 0 and 1 ---\n",
        "max_discussion = df_em_plot['Discussion'].max()\n",
        "max_prs_opened = df_em_plot['pull_requests_opened_count'].max()\n",
        "\n",
        "df_em_plot['Normalized_Discussion'] = 0\n",
        "if max_discussion > 0:\n",
        "    df_em_plot['Normalized_Discussion'] = (df_em_plot['Discussion'] / max_discussion)\n",
        "\n",
        "df_em_plot['Normalized_PRs_Opened'] = 0\n",
        "if max_prs_opened > 0:\n",
        "    df_em_plot['Normalized_PRs_Opened'] = (df_em_plot['pull_requests_opened_count'] / max_prs_opened)\n",
        "\n",
        "# Calculate the Engagement Magnitude Index (EM) using normalized values\n",
        "# EM = sqrt(Normalized_Discussion^2 + Normalized_PRs_Opened^2)\n",
        "df_em_plot['EM'] = np.sqrt(df_em_plot['Normalized_Discussion']**2 + df_em_plot['Normalized_PRs_Opened']**2)\n",
        "\n",
        "# Normalize EM by its theoretical maximum of sqrt(2) to scale it between 0 and 1\n",
        "if not df_em_plot['EM'].empty:\n",
        "    df_em_plot['EM'] = df_em_plot['EM'] / np.sqrt(2)\n",
        "\n",
        "# Sort the DataFrame by EM in descending order\n",
        "df_em_plot = df_em_plot.sort_values(by='EM', ascending=False)\n",
        "\n",
        "# Calculate the average EM for all contributors\n",
        "average_em = df_em_plot['EM'].mean()\n",
        "\n",
        "# Ensure project_em is available from the 'em-agregado' cell (H27fmKkNGTum)\n",
        "# Fallback if it hasn't been run or is not defined\n",
        "if 'project_em' not in globals():\n",
        "    # Re-calculate project_em if not available (should ideally come from H27fmKkNGTum)\n",
        "    total_project_discussion = df_people['issues_opened_count'].sum() + df_people['issue_comment_event_count'].sum()\n",
        "    total_project_prs_opened = df_people['pull_requests_opened_count'].sum()\n",
        "\n",
        "    max_discussion_fallback = df_em_plot['Discussion'].max() if df_em_plot['Discussion'].max() > 0 else 1\n",
        "    max_prs_opened_fallback = df_em_plot['pull_requests_opened_count'].max() if df_em_plot['pull_requests_opened_count'].max() > 0 else 1\n",
        "\n",
        "    normalized_project_discussion_fallback = total_project_discussion / max_discussion_fallback\n",
        "    normalized_project_prs_opened_fallback = total_project_prs_opened / max_prs_opened_fallback\n",
        "\n",
        "    project_em_raw_fallback = np.sqrt(normalized_project_discussion_fallback**2 + normalized_project_prs_opened_fallback**2)\n",
        "    project_em = project_em_raw_fallback / np.sqrt(2)\n",
        "\n",
        "# Create the bar chart for EM\n",
        "fig = px.bar(\n",
        "    df_em_plot,\n",
        "    x='github_login',\n",
        "    y='EM',\n",
        "    title='Engagement magnitude (EM) index',\n",
        "    labels={\n",
        "        'github_login': 'Contributor',\n",
        "        'EM': 'EM'\n",
        "    },\n",
        "    hover_name='name',\n",
        "    hover_data={\n",
        "        'github_login': False,\n",
        "        'name': True,\n",
        "        'Discussion': True,\n",
        "        'pull_requests_opened_count': True,\n",
        "        'Normalized_Discussion': ':.2f',\n",
        "        'Normalized_PRs_Opened': ':.2f',\n",
        "        'EM': ':.2f' # Format EM to 2 decimal places in hover\n",
        "    },\n",
        "    text_auto='.1f' # Format EM to 1 decimal place on top of bars\n",
        ")\n",
        "\n",
        "# Adjust x-axis to show labels vertically\n",
        "fig.update_xaxes(tickangle=90, tickfont=dict(size=10))\n",
        "fig.update_yaxes(rangemode='tozero') # Ensure y-axis starts at zero\n",
        "\n",
        "# Optional: adjust text position if needed (default 'auto' for bar usually puts it on top)\n",
        "fig.update_traces(textposition='outside')\n",
        "\n",
        "# Removed horizontal line for the average EM of all contributors as requested\n",
        "# fig.add_hline(y=average_em, line_width=1, line_dash=\"dash\", line_color=\"gray\",\n",
        "#               annotation_text=f\"Average EM: {average_em:.2f}\",\n",
        "#               annotation_position=\"bottom right\",\n",
        "#               annotation_font_color=\"gray\")\n",
        "\n",
        "# Add horizontal line for the aggregated project EM\n",
        "fig.add_hline(y=project_em, line_width=1, line_dash=\"dash\", line_color=\"black\",\n",
        "              annotation_text=f\"Project EM: {project_em:.2f}\",\n",
        "              annotation_position=\"top right\",\n",
        "              annotation_font_color=\"black\")\n",
        "\n",
        "fig.show()\n",
        "save_fig(fig, 'em_index.html')\n",
        "\n",
        "print(\"Figure saved to: /content/gdrive/My Drive/naja-2025-11-27/em_index.html\")"
      ],
      "metadata": {
        "id": "MqCBGuYoRC60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "REMI is the norm of the two-dimensional contribution vector whose coordinates are the developer’s Discussion score and the number of PRs opened. In practice, EMI captures the overall intensity of a developer’s participatory and initiatory actions by combining conversational engagement with concrete technical initiation into a single magnitude."
      ],
      "metadata": {
        "id": "COPik5GMUQbl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delivery"
      ],
      "metadata": {
        "id": "wC_CR5-1jLym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delivery chart"
      ],
      "metadata": {
        "id": "gSdZmkWKJ-Mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cellname : dm-agregado\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- 1. Calculate Project-wide totals for Code Changes and PRs Merged ---\n",
        "# Summing across all individuals in df_people\n",
        "total_project_code_changes = df_people['total_additions'].sum() + df_people['total_deletions'].sum()\n",
        "total_project_prs_merged = df_people['pull_requests_merged_count'].sum()\n",
        "\n",
        "print(f\"Total Project Code Changes (Additions + Deletions): {total_project_code_changes}\")\n",
        "print(f\"Total Project PRs Merged: {total_project_prs_merged}\")\n",
        "\n",
        "# --- 2. Retrieve max individual values for normalization ---\n",
        "# These values (max_code_changes, max_prs_merged) refer to the highest individual contribution.\n",
        "# Calculate them from df_people\n",
        "df_people_code_changes_temp = df_people['total_additions'] + df_people['total_deletions']\n",
        "max_code_changes = df_people_code_changes_temp.max()\n",
        "max_prs_merged = df_people['pull_requests_merged_count'].max()\n",
        "\n",
        "# Handle division by zero if no activity of a certain type\n",
        "if max_code_changes == 0: max_code_changes = 1\n",
        "if max_prs_merged == 0: max_prs_merged = 1\n",
        "\n",
        "# --- 3. Calculate count of active contributors for proper scaling ---\n",
        "# An 'active' contributor for DM is one with >0 code changes or >0 PRs merged\n",
        "active_contributors_for_dm_count = df_people[\n",
        "    (df_people_code_changes_temp > 0) | (df_people['pull_requests_merged_count'] > 0)\n",
        "].shape[0]\n",
        "\n",
        "# Ensure at least one active contributor to avoid division by zero later\n",
        "if active_contributors_for_dm_count == 0: active_contributors_for_dm_count = 1\n",
        "\n",
        "# --- 4. Normalize Project totals using individual maximums ---\n",
        "normalized_project_code_changes = total_project_code_changes / max_code_changes\n",
        "normalized_project_prs_merged = total_project_prs_merged / max_prs_merged\n",
        "\n",
        "print(f\"Normalized Project Code Changes (relative to max individual): {normalized_project_code_changes:.2f}\")\n",
        "print(f\"Normalized Project PRs Merged (relative to max individual): {normalized_project_prs_merged:.2f}\")\n",
        "\n",
        "# --- 5. Calculate Project DM (raw magnitude) ---\n",
        "project_dm_raw = np.sqrt(normalized_project_code_changes**2 + normalized_project_prs_merged**2)\n",
        "\n",
        "# --- 6. Scale Project DM to be between 0 and 1 ---\n",
        "# The maximum possible value for project_dm_raw, if all 'active_contributors_for_dm_count'\n",
        "# were performing at 'max_code_changes' and 'max_prs_merged' level, would be\n",
        "# active_contributors_for_dm_count * sqrt(max_code_changes^2/max_code_changes^2 + max_prs_merged^2/max_prs_merged^2)\n",
        "# = active_contributors_for_dm_count * sqrt(1+1) = active_contributors_for_dm_count * sqrt(2)\n",
        "max_theoretical_project_dm_raw = active_contributors_for_dm_count * np.sqrt(2)\n",
        "\n",
        "if max_theoretical_project_dm_raw > 0:\n",
        "    project_dm = project_dm_raw / max_theoretical_project_dm_raw\n",
        "else:\n",
        "    project_dm = 0.0 # If no activity, project DM is 0\n",
        "\n",
        "print(f\"\\nProject Delivery Magnitude (DM) (0-1 scaled): {project_dm:.2f}\")"
      ],
      "metadata": {
        "id": "hxkget_HZ2JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cellname : deliery-plot\n",
        "\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "# Calculate the 'Code Changes' metric\n",
        "df_plot_code_changes = df_people.copy()\n",
        "df_plot_code_changes['Code Changes'] = df_plot_code_changes['total_additions'] + df_plot_code_changes['total_deletions']\n",
        "\n",
        "# Filter out developers who have no activity in either PRs Merged or Code Changes\n",
        "df_plot_code_changes = df_plot_code_changes[\n",
        "    (df_plot_code_changes['pull_requests_merged_count'] > 0) |\n",
        "    (df_plot_code_changes['Code Changes'] > 0)\n",
        "].copy()\n",
        "\n",
        "# --- Aggregate data for unique (PRs merged, Code Changes) coordinates ---\n",
        "aggregated_df = df_plot_code_changes.groupby(['pull_requests_merged_count', 'Code Changes']).agg(\n",
        "    github_logins=('github_login', lambda x: ', '.join(x)),\n",
        "    names=('name', lambda x: ', '.join(x)),\n",
        "    num_devs=('github_login', 'count')\n",
        ").reset_index()\n",
        "\n",
        "# Create a unique key for each aggregated point for coloring with distinct discrete colors\n",
        "aggregated_df['point_category'] = 'PRs Merged ' + aggregated_df['pull_requests_merged_count'].astype(str) + ', Changes ' + aggregated_df['Code Changes'].astype(str)\n",
        "\n",
        "# Add a new column to aggregated_df to determine marker style\n",
        "# Updated: 'circle' for single, 'circle-open' for multiple. 'circle-dot' would be third if needed.\n",
        "aggregated_df['marker_type'] = aggregated_df['num_devs'].apply(lambda x: 'Single Contributor' if x == 1 else 'Multiple Contributors')\n",
        "\n",
        "# Define a custom color sequence using only Dark24\n",
        "custom_color_sequence = px.colors.qualitative.Dark24\n",
        "\n",
        "# Create the scatter plot with aggregated data (axes swapped)\n",
        "fig = px.scatter(\n",
        "    aggregated_df,\n",
        "    x='Code Changes', # Swapped axis\n",
        "    y='pull_requests_merged_count', # Swapped axis\n",
        "    size='num_devs', # Size of marker based on number of developers at this point\n",
        "    color='point_category', # Use the categorical key for distinct colors\n",
        "    color_discrete_sequence=custom_color_sequence, # Use the custom, extended palette\n",
        "    symbol='marker_type', # Use the new column to determine marker symbol (solid vs. outlined)\n",
        "    symbol_map={'Single Contributor': 'circle', 'Multiple Contributors': 'circle-open'}, # Updated symbol_map\n",
        "    text=None, # Removed text labels from directly on the bubbles\n",
        "    title='Delivery chart',\n",
        "    labels={\n",
        "        'Code Changes': 'Code Changes (Additions + Deletions)', # Updated label\n",
        "        'pull_requests_merged_count': 'PRs Merged', # Updated label\n",
        "        'num_devs': 'Number of Contributors',\n",
        "        'github_logins': 'Contributors',\n",
        "        'point_category': 'Contributors' # This will be the initial legend title, replaced later\n",
        "    },\n",
        "    hover_name='names', # Show aggregated names on hover\n",
        "    hover_data={\n",
        "        'github_logins': True, # Also show aggregated logins on hover\n",
        "        'num_devs': True,\n",
        "        'pull_requests_merged_count': True,\n",
        "        'Code Changes': True,\n",
        "        'names': False, # Don't duplicate hover_name in hover_data\n",
        "        'point_category': False # Don't show this in hover data, as labels are clear\n",
        "    }\n",
        ")\n",
        "\n",
        "# Apply marker styling to ensure proportionality with small base size\n",
        "# sizeref: smaller value makes markers larger. Larger value makes markers smaller.\n",
        "# Let's target a max size of ~10 pixels for the largest 'num_devs' (reduced from 20)\n",
        "max_num_devs_val = aggregated_df['num_devs'].max()\n",
        "if max_num_devs_val > 0:\n",
        "    target_max_size = 10 # pixels for the largest bubble (reduced)\n",
        "    sizeref_val = max_num_devs_val / target_max_size\n",
        "else:\n",
        "    sizeref_val = 1 # avoid division by zero if all num_devs are 0 or 1\n",
        "\n",
        "fig.update_traces(\n",
        "    marker=dict(\n",
        "        sizemode='diameter', # Scale by diameter\n",
        "        sizeref=sizeref_val, # Reference value for scaling\n",
        "        sizemin=2,           # Minimum size in pixels for the smallest 'num_devs' (reduced from 4)\n",
        "        line=dict(width=1, color='DarkSlateGrey') # Default line for all\n",
        "    ),\n",
        "    selector=dict(mode='markers') # Apply to all scatter markers\n",
        ")\n",
        "\n",
        "# If specific line widths for symbols are desired, they can be added here\n",
        "# For example, to make the 'circle-open' slightly thicker outline\n",
        "fig.update_traces(marker=dict(line=dict(width=2)), selector=dict(symbol='circle-open'))\n",
        "\n",
        "\n",
        "# Create a mapping from point_category to github_logins for legend renaming\n",
        "category_to_logins_map = aggregated_df.set_index('point_category')['github_logins'].to_dict()\n",
        "\n",
        "# Function to extract the base point_category from the full trace name\n",
        "def get_base_point_category(full_trace_name):\n",
        "    # The trace name will be 'point_category_value, marker_type_value' if both color and symbol are used\n",
        "    # We want to split at the last comma to get 'point_category_value'\n",
        "    parts = full_trace_name.rsplit(', ', 1)\n",
        "    if len(parts) > 1 and (parts[-1] == 'Single Contributor' or parts[-1] == 'Multiple Contributors'):\n",
        "        return parts[0]\n",
        "    return full_trace_name # Fallback if name doesn't match expected pattern (e.g., if only color is used)\n",
        "\n",
        "# Update the legend entry names to show github_logins\n",
        "fig.for_each_trace(lambda trace: trace.update(name=category_to_logins_map[get_base_point_category(trace.name)]))\n",
        "\n",
        "fig.update_layout(\n",
        "    height=800, # Reduced height\n",
        "    xaxis_title='Code Changes (Additions + Deletions)', # Swapped axis title\n",
        "    yaxis_title='PRs Merged',\n",
        "    legend_title='Contributors',\n",
        "    legend=dict(\n",
        "        orientation='v', # Vertical orientation\n",
        "        yanchor='top', # Anchor to the top\n",
        "        y=-0.2, # Position below the chart (adjust as needed)\n",
        "        xanchor='left',\n",
        "        x=0\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "save_fig(fig, 'delivery_plot.html')\n"
      ],
      "metadata": {
        "id": "7HpAzvtSjkiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delivery magnitude (DM) index\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DMKVzGH0KFNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cellname : delivery-index\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Calculate the 'Code Changes' metric if not already present\n",
        "df_dm = df_people.copy()\n",
        "df_dm['Code Changes'] = df_dm['total_additions'] + df_dm['total_deletions']\n",
        "\n",
        "# Filter out developers with no contributions in these categories\n",
        "df_dm_plot = df_dm[((df_dm['Code Changes'] > 0) | (df_dm['pull_requests_merged_count'] > 0))].copy()\n",
        "\n",
        "# --- Normalize coordinates (Code Changes, PRs Merged) between 0 and 1 ---\n",
        "max_code_changes = df_dm_plot['Code Changes'].max()\n",
        "max_prs_merged = df_dm_plot['pull_requests_merged_count'].max()\n",
        "\n",
        "df_dm_plot['Normalized_Code_Changes'] = 0\n",
        "if max_code_changes > 0:\n",
        "    df_dm_plot['Normalized_Code_Changes'] = (df_dm_plot['Code Changes'] / max_code_changes)\n",
        "\n",
        "df_dm_plot['Normalized_PRs_Merged'] = 0\n",
        "if max_prs_merged > 0:\n",
        "    df_dm_plot['Normalized_PRs_Merged'] = (df_dm_plot['pull_requests_merged_count'] / max_prs_merged)\n",
        "\n",
        "# Calculate the Delivery Magnitude Index (DM) using normalized values\n",
        "# DM = sqrt(Normalized_Code_Changes^2 + Normalized_PRs_Merged^2)\n",
        "df_dm_plot['DM'] = np.sqrt(df_dm_plot['Normalized_Code_Changes']**2 + df_dm_plot['Normalized_PRs_Merged']**2)\n",
        "\n",
        "# Normalize the final index value so that it fits in [0, 1]\n",
        "# The maximum possible value for DM is sqrt(1^2 + 1^2) = sqrt(2)\n",
        "if not df_dm_plot['DM'].empty:\n",
        "    df_dm_plot['DM'] = df_dm_plot['DM'] / np.sqrt(2)\n",
        "\n",
        "# Sort the DataFrame by DM in descending order\n",
        "df_dm_plot = df_dm_plot.sort_values(by='DM', ascending=False)\n",
        "\n",
        "# Ensure project_dm is available from the 'dm-agregado' cell (hxkget_HZ2JM)\n",
        "# Fallback if it hasn't been run or is not defined\n",
        "if 'project_dm' not in globals():\n",
        "    # Re-calculate project_dm if not available (should ideally come from hxkget_HZ2JM)\n",
        "    total_project_code_changes = df_people['total_additions'].sum() + df_people['total_deletions'].sum()\n",
        "    total_project_prs_merged = df_people['pull_requests_merged_count'].sum()\n",
        "\n",
        "    max_code_changes_fallback = df_dm_plot['Code Changes'].max() if df_dm_plot['Code Changes'].max() > 0 else 1\n",
        "    max_prs_merged_fallback = df_dm_plot['pull_requests_merged_count'].max() if df_dm_plot['pull_requests_merged_count'].max() > 0 else 1\n",
        "\n",
        "    normalized_project_code_changes_fallback = total_project_code_changes / max_code_changes_fallback\n",
        "    normalized_project_prs_merged_fallback = total_project_prs_merged / max_prs_merged_fallback\n",
        "\n",
        "    project_dm_raw_fallback = np.sqrt(normalized_project_code_changes_fallback**2 + normalized_project_prs_merged_fallback**2)\n",
        "    active_contributors_for_dm_count = df_people[\n",
        "        (df_people['total_additions'] + df_people['total_deletions'] > 0) | (df_people['pull_requests_merged_count'] > 0)\n",
        "    ].shape[0]\n",
        "    if active_contributors_for_dm_count == 0: active_contributors_for_dm_count = 1\n",
        "    project_dm = project_dm_raw_fallback / (active_contributors_for_dm_count * np.sqrt(2))\n",
        "\n",
        "# Create the bar chart for DM\n",
        "fig = px.bar(\n",
        "    df_dm_plot,\n",
        "    x='github_login',\n",
        "    y='DM',\n",
        "    title='Delivery magnitude (DM)',\n",
        "    labels={\n",
        "        'github_login': 'Contributor',\n",
        "        'DM': 'DM'\n",
        "    },\n",
        "    hover_name='name',\n",
        "    hover_data={\n",
        "        'github_login': False,\n",
        "        'name': True,\n",
        "        'Code Changes': True,\n",
        "        'pull_requests_merged_count': True,\n",
        "        'Normalized_Code_Changes': ':.2f',\n",
        "        'Normalized_PRs_Merged': ':.2f',\n",
        "        'DM': ':.2f' # Format to 2 decimal places in hover\n",
        "    },\n",
        "    text_auto='.1f' # Format to 1 decimal place on top of bars\n",
        ")\n",
        "\n",
        "# Adjust x-axis to show labels vertically\n",
        "fig.update_xaxes(tickangle=90, tickfont=dict(size=10))\n",
        "fig.update_yaxes(rangemode='tozero') # Ensure y-axis starts at zero\n",
        "\n",
        "# Adjust text position\n",
        "fig.update_traces(textposition='outside')\n",
        "\n",
        "# Add horizontal line for the aggregated project DM\n",
        "fig.add_hline(y=project_dm, line_width=1, line_dash=\"dash\", line_color=\"black\",\n",
        "              annotation_text=f\"Project DM: {project_dm:.2f}\",\n",
        "              annotation_position=\"top right\",\n",
        "              annotation_font_color=\"black\")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "save_fig(fig, 'dm_index.html')\n",
        "\n",
        "print(\"Figure saved to: /content/gdrive/My Drive/naja-2025-11-28/dm_index.html\")"
      ],
      "metadata": {
        "id": "LpOgEhI0l-YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Relative Delivery Magnitude Index (RDMI) is the norm of the two-dimensional delivery vector whose components are the developer’s code changes and the number of PRs merged. Conceptually, CDI represents the overall strength of a developer’s effective code delivery by combining the volume of code produced with the number of contributions successfully integrated into the project."
      ],
      "metadata": {
        "id": "k_rbDzl2qGkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Engagement-Delivery Map"
      ],
      "metadata": {
        "id": "p_vKg9PGqfOx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9240627d"
      },
      "source": [
        "#cellname : em-dm-chart\n",
        "\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np # Ensure numpy is imported\n",
        "\n",
        "# Ensure df_em_plot and df_dm_plot are available and contain the calculated indices\n",
        "# If this cell is run independently, ensure these DFs are regenerated or loaded\n",
        "\n",
        "# Select relevant columns from each DataFrame\n",
        "df_em = df_em_plot[['github_login', 'name', 'EM']]\n",
        "df_dm = df_dm_plot[['github_login', 'name', 'DM']]\n",
        "\n",
        "# Merge the two DataFrames on 'github_login'\n",
        "# Use an outer merge to include all contributors who appear in either index calculation\n",
        "df_combined_indices = pd.merge(df_em, df_dm,\n",
        "                               how='outer',\n",
        "                               on='github_login',\n",
        "                               suffixes=('_em', '_dm'))\n",
        "\n",
        "# Handle cases where a contributor might not have activity for one of the indices\n",
        "# (e.g., no PRs opened/comments for EM, or no code changes/PRs merged for DM)\n",
        "df_combined_indices['EM'] = df_combined_indices['EM'].fillna(0)\n",
        "df_combined_indices['DM'] = df_combined_indices['DM'].fillna(0)\n",
        "\n",
        "# Resolve potential duplicate 'name' columns if a simple merge was used.\n",
        "# We prefer the 'name' from the EM calculation, or the DM if the EM name is null/NA\n",
        "df_combined_indices['name'] = df_combined_indices['name_em'].fillna(df_combined_indices['name_dm'])\n",
        "\n",
        "# Drop the redundant name columns\n",
        "df_combined_indices = df_combined_indices.drop(columns=['name_em', 'name_dm'], errors='ignore')\n",
        "\n",
        "# Filter out contributors with zero in both indices for a cleaner plot, unless they are all zeros\n",
        "# Check if there's any non-zero value at all to decide on filtering\n",
        "if (df_combined_indices['EM'].sum() > 0) or (df_combined_indices['DM'].sum() > 0):\n",
        "    df_combined_indices_plot = df_combined_indices[\n",
        "        (df_combined_indices['EM'] > 0) |\n",
        "        (df_combined_indices['DM'] > 0)\n",
        "    ].copy()\n",
        "else:\n",
        "    df_combined_indices_plot = df_combined_indices.copy() # Keep all if all are zero\n",
        "\n",
        "# --- Add the aggregated project EM and DM point ---\n",
        "# Ensure project_em and project_dm are available\n",
        "if 'project_em' not in globals():\n",
        "    # Fallback if not defined (should come from em-agregado cell)\n",
        "    project_em = 0.0\n",
        "if 'project_dm' not in globals():\n",
        "    # Fallback if not defined (should come from dm-agregado cell)\n",
        "    project_dm = 0.0\n",
        "\n",
        "# Create a DataFrame for the aggregated point\n",
        "df_aggregated_point = pd.DataFrame({\n",
        "    'github_login': ['Project_Aggregated'],\n",
        "    'name': ['Project'], # Changed text here\n",
        "    'EM': [project_em],\n",
        "    'DM': [project_dm]\n",
        "})\n",
        "\n",
        "# Concatenate with the existing DataFrame\n",
        "df_combined_indices_plot = pd.concat([df_combined_indices_plot, df_aggregated_point], ignore_index=True)\n",
        "\n",
        "# Create the scatter plot\n",
        "fig = px.scatter(\n",
        "    df_combined_indices_plot,\n",
        "    x='DM', # DM on X-axis\n",
        "    y='EM', # EM on Y-axis\n",
        "    text=None, # Removed github_login as text on points\n",
        "    hover_name='name', # Show full name on hover\n",
        "    hover_data={\n",
        "        'github_login': True, # Keep github_login in hover\n",
        "        'name': False,       # Don't duplicate hover_name\n",
        "        'EM': ':.2f',       # Format EM in hover\n",
        "        'DM': ':.2f' # Format DM in hover\n",
        "    },\n",
        "    title='Contribution chart (EM vs DM)',\n",
        "    labels={\n",
        "        'EM': 'Engagement Magnitude (EM)',\n",
        "        'DM': 'Delivery Magnitude (DM)'\n",
        "    },\n",
        "    # Removed 'size' from px.scatter to control it via update_traces\n",
        "    color='github_login', # Assign a unique color to each contributor and the aggregated point\n",
        "    symbol='github_login' # Assign a unique symbol to each contributor and the aggregated point\n",
        ")\n",
        "\n",
        "# Set a fixed, slightly larger size for all individual markers\n",
        "fig.update_traces(marker=dict(size=10))\n",
        "\n",
        "# Apply specific styling for the aggregated point\n",
        "fig.update_traces(\n",
        "    marker=dict(symbol='hexagon-dot', size=20, color='black', line=dict(width=2, color='white')),\n",
        "    selector=dict(name='Project_Aggregated')\n",
        ")\n",
        "\n",
        "# Add text label for the aggregated point\n",
        "fig.add_annotation(\n",
        "    x=project_dm,\n",
        "    y=project_em,\n",
        "    text=\"Project\", # Changed text here\n",
        "    showarrow=False,\n",
        "    xshift=0,\n",
        "    yshift=15,\n",
        "    font=dict(color='black', size=12, weight='bold')\n",
        ")\n",
        "\n",
        "# Add lines for quadrants\n",
        "fig.add_hline(y=0.5, line_dash='dash', line_color='gray', annotation_text=\"Average EM\")\n",
        "fig.add_vline(x=0.5, line_dash='dash', line_color='gray', annotation_text=\"Average DM\")\n",
        "\n",
        "# Update layout for better aesthetics and legend position\n",
        "fig.update_layout(\n",
        "    height=900, # Increased height to accommodate the legend below\n",
        "    width=900,\n",
        "    xaxis=dict(range=[-0.05, 1.05]), # Extend range slightly beyond 0-1\n",
        "    yaxis=dict(range=[-0.05, 1.05]),\n",
        "    showlegend=True, # Ensure legend is shown\n",
        "    legend=dict(\n",
        "        orientation='v', # Vertical orientation\n",
        "        yanchor='top',   # Anchor to the top of the legend container\n",
        "        y=1,             # Position at the top right (y=1) of the plot area\n",
        "        xanchor='right',\n",
        "        x=1             # Position slightly outside the plot area to the right\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "save_fig(fig, 'engagement_delivery_map.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Add REMI e RDMI to dataframe"
      ],
      "metadata": {
        "id": "gcLPjI8UM-B2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Select only the necessary columns from df_combined_indices\n",
        "df_indices_to_merge = df_combined_indices[['github_login', 'EM', 'DM']]\n",
        "\n",
        "# Ensure there are no existing BCI or Code_Delivery_Index columns in df_people\n",
        "# that would cause suffixing issues during merge. This is a more robust way to clean up.\n",
        "cols_to_remove = [col for col in df_people.columns if col.startswith('BCI') or col.startswith('Code_Delivery_Index') or col.startswith('EM') or col.startswith('DM')]\n",
        "if cols_to_remove:\n",
        "    df_people = df_people.drop(columns=cols_to_remove)\n",
        "\n",
        "# Merge with df_people to add EM and DM cleanly\n",
        "df_people = pd.merge(df_people, df_indices_to_merge,\n",
        "                     on='github_login',\n",
        "                     how='left') # Use left merge to keep all rows from df_people\n",
        "\n",
        "# Fill NaN values for EM and DM with 0 for contributors who had no activity\n",
        "# These lines should now succeed because the merge would have added the columns.\n",
        "df_people['EM'] = df_people['EM'].fillna(0.0)\n",
        "df_people['DM'] = df_people['DM'].fillna(0.0)\n",
        "\n",
        "print(\"df_people DataFrame com EM e DM adicionados:\")\n",
        "display(df_people.head())\n"
      ],
      "metadata": {
        "id": "gsoPL6-CMvcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1LgYy-TfM7mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d481b19"
      },
      "source": [
        "## Engagement-Delivery distribution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Engagement (EM) and Delivery (DM) stacked"
      ],
      "metadata": {
        "id": "qokwH3OJcEWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "# Select relevant columns from df_people\n",
        "df_plot_indices = df_people[['github_login', 'name', 'EM', 'DM']].copy()\n",
        "\n",
        "# Filter out developers who have zero for both EM and DM\n",
        "df_plot_indices_filtered = df_plot_indices[\n",
        "    (df_plot_indices['EM'] > 0) | (df_plot_indices['DM'] > 0)\n",
        "].copy()\n",
        "\n",
        "if df_plot_indices_filtered.empty:\n",
        "    print(\"No contributors with non-zero EM or DM scores found to plot.\")\n",
        "else:\n",
        "    # Calculate total score for sorting\n",
        "    df_plot_indices_filtered['total_score'] = df_plot_indices_filtered['EM'] + df_plot_indices_filtered['DM']\n",
        "\n",
        "    # Sort df_plot_indices_filtered by total_score in descending order\n",
        "    df_plot_indices_filtered = df_plot_indices_filtered.sort_values(by='total_score', ascending=False)\n",
        "\n",
        "    # Melt the DataFrame to a long format suitable for stacked bar charts\n",
        "    df_melted_indices = df_plot_indices_filtered.melt(\n",
        "        id_vars=['github_login', 'name', 'total_score'], # Include total_score for potential hover or sorting\n",
        "        value_vars=['EM', 'DM'],\n",
        "        var_name='Metric',\n",
        "        value_name='Value'\n",
        "    )\n",
        "\n",
        "    # Rename metrics for better readability in the plot\n",
        "    df_melted_indices['Metric'] = df_melted_indices['Metric'].replace({\n",
        "        'EM': 'EM (Engagement)',\n",
        "        'DM': 'DM (Delivery)'\n",
        "    })\n",
        "\n",
        "    # Custom order for metrics so EM is 'em baixo' and DM is 'acima'\n",
        "    metric_order = ['EM (Engagement)', 'DM (Delivery)']\n",
        "    df_melted_indices['Metric'] = pd.Categorical(df_melted_indices['Metric'], categories=metric_order, ordered=True)\n",
        "\n",
        "    # Get the sorted list of github_login for xaxis_categoryarray based on total_score\n",
        "    sorted_github_logins = df_plot_indices_filtered['github_login'].tolist()\n",
        "\n",
        "    # Create the stacked bar chart using Plotly Express\n",
        "    fig = px.bar(\n",
        "        df_melted_indices,\n",
        "        x='github_login',\n",
        "        y='Value',\n",
        "        color='Metric',\n",
        "        barmode='stack', # This creates stacked bars for each github_login\n",
        "        title='Contribution rank',\n",
        "        labels={\n",
        "            'github_login': 'Contributor',\n",
        "            'Value': 'Score',\n",
        "            'Metric': 'Index Type'\n",
        "        },\n",
        "        hover_name='name',\n",
        "        hover_data={\n",
        "            'github_login': True, # Keep github_login in hover for context\n",
        "            'name': False,       # Don't duplicate hover_name\n",
        "            'Metric': True,\n",
        "            'Value': ':.2f'      # Format Value to 2 decimal places in hover\n",
        "        },\n",
        "        text_auto='.2f' # Display text values automatically\n",
        "    )\n",
        "\n",
        "    # Ensure project_em and project_dm are available\n",
        "    if 'project_em' not in globals():\n",
        "        project_em = 0.0 # Fallback\n",
        "    if 'project_dm' not in globals():\n",
        "        project_dm = 0.0 # Fallback\n",
        "\n",
        "    # Add horizontal line for aggregated EM (blue)\n",
        "    fig.add_hline(y=project_em, line_width=1, line_dash=\"dash\", line_color=\"blue\",\n",
        "                  annotation_text=f\"Project EM: {project_em:.2f}\",\n",
        "                  annotation_position=\"top left\",\n",
        "                  annotation_font_color=\"blue\")\n",
        "\n",
        "    # Add horizontal line for aggregated DM (red) at its own value\n",
        "    fig.add_hline(y=project_dm, line_width=1, line_dash=\"dash\", line_color=\"red\",\n",
        "                  annotation_text=f\"Project DM: {project_dm:.2f}\",\n",
        "                  annotation_position=\"bottom right\", # Position below the line\n",
        "                  annotation_font_color=\"red\")\n",
        "\n",
        "    # Add horizontal line for aggregated EM + DM (total project score) (black)\n",
        "    fig.add_hline(y=project_em + project_dm, line_width=2, line_dash=\"dash\", line_color=\"black\",\n",
        "                  annotation_text=f\"Project Total (EM+DM): {project_em + project_dm:.2f}\",\n",
        "                  annotation_position=\"top right\", # Position above the line\n",
        "                  annotation_font_color=\"black\")\n",
        "\n",
        "\n",
        "    # Refine plot aesthetics\n",
        "    fig.update_layout(\n",
        "        xaxis_title='Contributor',\n",
        "        yaxis_title='Score',\n",
        "        legend_title='Index Type',\n",
        "        xaxis_tickangle=90, # Rotate x-axis labels for readability\n",
        "        bargap=0.1, # Gap between groups of bars (not very relevant for stacked, but good practice)\n",
        "        height=600, # Adjust plot height\n",
        "        xaxis_categoryorder='array', # Preserve the order from the DataFrame\n",
        "        xaxis_categoryarray=sorted_github_logins\n",
        "    )\n",
        "\n",
        "    fig.update_yaxes(rangemode='tozero') # Ensure y-axis starts at zero\n",
        "\n",
        "    # Set text color to white for all traces and position text inside\n",
        "    fig.update_traces(textfont_color='white', textposition='inside')\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "    save_fig(fig, 'contribution_em_dm_plot.html')"
      ],
      "metadata": {
        "id": "qhmDWfUzRaD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contribution score"
      ],
      "metadata": {
        "id": "uvG4IsTyRvzC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27c9c567"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Calculate 'activity': A_factor * (EM>0) + (1-A_factor) * (DM>0)\n",
        "# Convert boolean to integer (True=1, False=0) for multiplication\n",
        "df_people['activity'] = (df_people['EM'] > 0).astype(int) * A_factor + \\\n",
        "                        (df_people['DM'] > 0).astype(int) * (1 - A_factor)\n",
        "\n",
        "# 2. Calculate 'reward' using the formula: R_factor * EM + (1-R_factor) * DM\n",
        "df_people['reward'] = (R_factor * df_people['EM']) + ((1 - R_factor) * df_people['DM'])\n",
        "\n",
        "# 3. Calculate the 'score' column as S_factor * activity + (1 - S_factor) * reward (range 0-1)\n",
        "df_people['score'] = S_factor * df_people['activity'] + (1 - S_factor) * df_people['reward']\n",
        "\n",
        "# Display the head of the df_people DataFrame to show the newly added 'activity', 'reward', and 'score' columns.\n",
        "display(df_people.head())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contribution score (CS) plot"
      ],
      "metadata": {
        "id": "oXVlUy7xLyYT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c1ea2ce"
      },
      "source": [
        "# cellname : cs-plot\n",
        "\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots # Import make_subplots\n",
        "\n",
        "# Ensure 'activity', 'reward', and 'score' columns are already calculated in df_people\n",
        "\n",
        "# --- Calculate Project-level Activity, Reward, and Score ---\n",
        "# project_em and project_dm are already calculated globally (0-1 scaled)\n",
        "# A_factor and R_factor are from the score-parameters cell\n",
        "\n",
        "# Ensure project_em and project_dm are numerical (they are floats from numpy, but good practice to check)\n",
        "project_em_val = project_em if pd.notna(project_em) else 0.0\n",
        "project_dm_val = project_dm if pd.notna(project_dm) else 0.0\n",
        "\n",
        "project_activity = A_factor * (1 if project_em_val > 0 else 0) + (1 - A_factor) * (1 if project_dm_val > 0 else 0)\n",
        "project_reward = R_factor * project_em_val + (1 - R_factor) * project_dm_val\n",
        "project_score = S_factor * project_activity + (1 - S_factor) * project_reward\n",
        "\n",
        "print(f\"Calculated Project Activity: {project_activity:.2f}\")\n",
        "print(f\"Calculated Project Reward: {project_reward:.2f}\")\n",
        "print(f\"Calculated Project Score: {project_score:.2f}\")\n",
        "\n",
        "# Calculate the four individual components of the score\n",
        "df_people['score_component_activity_em_present'] = S_factor * A_factor * (df_people['EM'] > 0).astype(int)\n",
        "df_people['score_component_activity_dm_present'] = S_factor * (1 - A_factor) * (df_people['DM'] > 0).astype(int)\n",
        "df_people['score_component_reward_em_magnitude'] = (1 - S_factor) * R_factor * df_people['EM']\n",
        "df_people['score_component_reward_dm_magnitude'] = (1 - S_factor) * (1 - R_factor) * df_people['DM']\n",
        "\n",
        "# 1. Create a new DataFrame, df_contribution_plot, using the score components\n",
        "df_contribution_plot = df_people[[\n",
        "    'github_login', 'name', 'score',\n",
        "    'score_component_activity_em_present',\n",
        "    'score_component_activity_dm_present',\n",
        "    'score_component_reward_em_magnitude',\n",
        "    'score_component_reward_dm_magnitude'\n",
        "]].copy()\n",
        "\n",
        "# 2. Filter df_contribution_plot to include only contributors where score > 0\n",
        "df_contribution_plot = df_contribution_plot[df_contribution_plot['score'] > 0].copy()\n",
        "\n",
        "# 3. Melt df_contribution_plot into a long format suitable for stacking\n",
        "df_melted_score = df_contribution_plot.melt(\n",
        "    id_vars=['github_login', 'name', 'score'],\n",
        "    value_vars=[\n",
        "        'score_component_activity_em_present',\n",
        "        'score_component_activity_dm_present',\n",
        "        'score_component_reward_em_magnitude',\n",
        "        'score_component_reward_dm_magnitude'\n",
        "    ],\n",
        "    var_name='Component Type',\n",
        "    value_name='Value'\n",
        ")\n",
        "\n",
        "# --- Add Project-level score as a bar ---\n",
        "project_score_component_activity_em_present = S_factor * A_factor * (1 if project_em_val > 0 else 0)\n",
        "project_score_component_activity_dm_present = S_factor * (1 - A_factor) * (1 if project_dm_val > 0 else 0)\n",
        "project_score_component_reward_em_magnitude = (1 - S_factor) * R_factor * project_em_val\n",
        "project_score_component_reward_dm_magnitude = (1 - S_factor) * (1 - R_factor) * project_dm_val\n",
        "\n",
        "project_data_for_bar = {\n",
        "    'github_login': ['Project'] * 4, # 4 components\n",
        "    'name': ['Project'] * 4,\n",
        "    'score': [project_score] * 4,\n",
        "    'Component Type': [\n",
        "        'score_component_activity_em_present',\n",
        "        'score_component_activity_dm_present',\n",
        "        'score_component_reward_em_magnitude',\n",
        "        'score_component_reward_dm_magnitude'\n",
        "    ],\n",
        "    'Value': [\n",
        "        project_score_component_activity_em_present,\n",
        "        project_score_component_activity_dm_present,\n",
        "        project_score_component_reward_em_magnitude,\n",
        "        project_score_component_reward_dm_magnitude\n",
        "    ]\n",
        "}\n",
        "df_project_score_bar = pd.DataFrame(project_data_for_bar)\n",
        "\n",
        "# Concatenate project data to the melted DataFrame\n",
        "df_melted_score = pd.concat([df_melted_score, df_project_score_bar], ignore_index=True)\n",
        "\n",
        "# Concatenate project total score to the df_contribution_plot for text display\n",
        "df_contribution_plot = pd.concat([df_contribution_plot, pd.DataFrame([{\n",
        "    'github_login': 'Project',\n",
        "    'name': 'Project',\n",
        "    'score': project_score,\n",
        "    'score_component_activity_em_present': project_score_component_activity_em_present,\n",
        "    'score_component_activity_dm_present': project_score_component_activity_dm_present,\n",
        "    'score_component_reward_em_magnitude': project_score_component_reward_em_magnitude,\n",
        "    'score_component_reward_dm_magnitude': project_score_component_reward_dm_magnitude\n",
        "}])], ignore_index=True)\n",
        "\n",
        "# 4. Rename component types for better plot labels\n",
        "df_melted_score['Component Type'] = df_melted_score['Component Type'].replace({\n",
        "    'score_component_activity_em_present': 'Activity (EM Present)',\n",
        "    'score_component_activity_dm_present': 'Activity (DM Present)',\n",
        "    'score_component_reward_em_magnitude': 'Reward (EM Magnitude)',\n",
        "    'score_component_reward_dm_magnitude': 'Reward (DM Magnitude)'\n",
        "})\n",
        "\n",
        "# 5. Sort the melted DataFrame for individuals first, then append 'Project'\n",
        "# Separate individual contributors from the project data for sorting\n",
        "df_melted_score_individuals = df_melted_score[df_melted_score['github_login'] != 'Project'].copy()\n",
        "df_project_melted_score = df_melted_score[df_melted_score['github_login'] == 'Project'].copy()\n",
        "\n",
        "# Sort individual contributors by total score in descending order\n",
        "df_melted_score_individuals = df_melted_score_individuals.sort_values(by='score', ascending=False)\n",
        "\n",
        "# Get the sorted list of github_login for individuals\n",
        "sorted_individual_logins = df_melted_score_individuals['github_login'].unique().tolist()\n",
        "\n",
        "# Add a blank space as a separator for better visual spacing\n",
        "separator_label = '  ' # Using two spaces for a slightly wider string, which translates to a wider category\n",
        "\n",
        "# Append separator and 'Project' to the end of the sorted list\n",
        "sorted_github_logins_for_score = sorted_individual_logins + [separator_label, 'Project']\n",
        "\n",
        "# Create empty data for the separator for df_melted_score\n",
        "df_separator_melted = pd.DataFrame({\n",
        "    'github_login': [separator_label] * len(component_order),\n",
        "    'name': [separator_label] * len(component_order),\n",
        "    'score': [0.0] * len(component_order),\n",
        "    'Component Type': component_order,\n",
        "    'Value': [0.0] * len(component_order)\n",
        "})\n",
        "df_melted_score = pd.concat([df_melted_score_individuals, df_separator_melted, df_project_melted_score]).set_index('github_login').loc[sorted_github_logins_for_score].reset_index()\n",
        "\n",
        "# Create empty data for the separator for df_contribution_plot (for text labels)\n",
        "df_separator_contribution = pd.DataFrame([{\n",
        "    'github_login': separator_label,\n",
        "    'name': separator_label,\n",
        "    'score': 0.0,\n",
        "    'score_component_activity_em_present': 0.0,\n",
        "    'score_component_activity_dm_present': 0.0,\n",
        "    'score_component_reward_em_magnitude': 0.0,\n",
        "    'score_component_reward_dm_magnitude': 0.0\n",
        "}])\n",
        "df_contribution_plot = pd.concat([df_contribution_plot_individuals, df_separator_contribution, df_project_contribution_plot]).set_index('github_login').loc[sorted_github_logins_for_score].reset_index()\n",
        "\n",
        "# Define custom colors for the components\n",
        "# Two shades of blue for activity components, two shades of green for reward components\n",
        "component_colors = {\n",
        "    'Activity (EM Present)': '#2A52BE',  # Darker Blue\n",
        "    'Activity (DM Present)': '#6495ED',  # Slightly darker CornflowerBlue\n",
        "    'Reward (EM Magnitude)': '#228B22',  # Darker Green (ForestGreen)\n",
        "    'Reward (DM Magnitude)': '#66CDAA'   # Slightly darker MediumAquamarine\n",
        "}\n",
        "\n",
        "# --- Removed the 'project_component_colors' dictionary ---\n",
        "# Define component order for consistent stacking\n",
        "component_order = [\n",
        "    'Activity (EM Present)',\n",
        "    'Activity (DM Present)',\n",
        "    'Reward (EM Magnitude)',\n",
        "    'Reward (DM Magnitude)'\n",
        "]\n",
        "df_melted_score['Component Type'] = pd.Categorical(df_melted_score['Component Type'], categories=component_order, ordered=True)\n",
        "\n",
        "# 6. Create a stacked bar chart using Plotly Express\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add bar traces for individual score components\n",
        "for component in component_order:\n",
        "    df_component = df_melted_score[df_melted_score['Component Type'] == component]\n",
        "\n",
        "    # Determine color based on whether it's a project bar or individual bar\n",
        "    current_colors = []\n",
        "    for login in df_component['github_login']:\n",
        "        if login == separator_label: # Make separator invisible\n",
        "            current_colors.append('rgba(0,0,0,0)')\n",
        "        else:\n",
        "            # Always use the standard component colors for all bars, including 'Project'\n",
        "            current_colors.append(component_colors[component])\n",
        "\n",
        "    fig.add_trace(go.Bar(\n",
        "        x=df_component['github_login'],\n",
        "        y=df_component['Value'],\n",
        "        name=component,\n",
        "        marker_color=current_colors,\n",
        "        text=[f'{v:.2f}' if v > 0 else '' for v in df_component['Value']], # Add text values, hide for 0\n",
        "        hovertemplate=\n",
        "            \"<b>%{customdata[1]}</b> (%{x})<br>\" +\n",
        "            \"Component Type: %{name}<br>\" +\n",
        "            \"Value: %{y:.2f}<br>\" +\n",
        "            \"Total Score: %{customdata[0]:.2f}<extra></extra>\", # customdata[0] for total score\n",
        "        customdata=df_component[['score', 'name']].values, # Pass score and name for hover as a numpy array\n",
        "        showlegend=True,\n",
        "    ))\n",
        "\n",
        "\n",
        "# Add total score as text on top of each bar (primary y-axis)\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=df_contribution_plot['github_login'],\n",
        "    y=df_contribution_plot['score'],\n",
        "    mode='text',\n",
        "    text=[f'{s:.2f}' if s > 0 else '' for s in df_contribution_plot['score']], # Format total score to 2 decimal places, hide for 0\n",
        "    textposition='top center',\n",
        "    textfont=dict(color='black', size=10),\n",
        "    showlegend=False,\n",
        "    hoverinfo='none',\n",
        "))\n",
        "\n",
        "\n",
        "# f. Adjust the x-axis to show labels vertically and preserve the order from the sorted DataFrame\n",
        "custom_tick_text = [login if login != separator_label else '' for login in sorted_github_logins_for_score]\n",
        "\n",
        "fig.update_xaxes(tickangle=-90, tickfont=dict(size=10),\n",
        "                 categoryorder='array',\n",
        "                 categoryarray=sorted_github_logins_for_score,\n",
        "                 ticktext=custom_tick_text, # Use the custom tick text\n",
        "                 tickvals=sorted_github_logins_for_score) # Keep tickvals for positioning\n",
        "\n",
        "# g. Update y-axes properties\n",
        "fig.update_yaxes(\n",
        "    rangemode='tozero',\n",
        "    title_text='Score Component Value',\n",
        "    range=[0, 1.1] # Adjusted range to 0-1.1\n",
        ")\n",
        "\n",
        "\n",
        "# Update layout for title and overall settings\n",
        "fig.update_layout(\n",
        "    title_text='Contribution score breakdown (with Project Total)', # Updated title\n",
        "    barmode='stack', # This is important for go.Bar traces to stack\n",
        "    height=700,\n",
        "    bargap=0.3, # Increased gap between bars to create more separation\n",
        "    legend=dict(\n",
        "        orientation='v', # Vertical orientation\n",
        "        yanchor='middle', # Anchor to the middle\n",
        "        y=0.5,             # Position vertically centered (0.5 means middle of plot area)\n",
        "        xanchor='left',\n",
        "        x=1.02             # Position slightly outside the plot area to the right\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display the Value on top of the bars in white text, positioned inside (for components)\n",
        "fig.update_traces(textfont_color='white', textposition='inside', selector=dict(type='bar'))\n",
        "\n",
        "fig.show()\n",
        "\n",
        "print(\"Overall Score calculation and stacked bar chart generation complete.\")\n",
        "save_fig(fig, 'score_plot.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cellname : score-evolution\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from datetime import date, timedelta, datetime, timezone\n",
        "import numpy as np\n",
        "\n",
        "# Ensure start_date and selected_timedelta are available (from previous cells)\n",
        "# start_date for event fetching is a datetime with timezone.\n",
        "# We need to get the date part for comparison with dictionary keys.\n",
        "\n",
        "# The start_date for fetching events is already timezone-aware\n",
        "# start_date is defined in bd46d806 or earlier cells\n",
        "if 'start_date' not in globals():\n",
        "    start_date = datetime.now(timezone.utc) - timedelta(weeks=1) # Fallback to 1 week if not defined\n",
        "\n",
        "end_date_for_range = datetime.now(timezone.utc).date() # Current date\n",
        "start_date_for_range = start_date.date() # Date part of the fetching start_date\n",
        "\n",
        "all_dates = [start_date_for_range + timedelta(days=x) for x in range((end_date_for_range - start_date_for_range).days + 1)]\n",
        "\n",
        "daily_project_metrics = []\n",
        "\n",
        "# Ensure max_discussion, max_prs_opened, max_code_changes, max_prs_merged are available\n",
        "# from previous cells like 'em-agregado' and 'dm-agregado'\n",
        "# Fallback values if not found (should be defined if previous cells were run)\n",
        "if 'max_discussion' not in globals() or max_discussion == 0: max_discussion_val = 1\n",
        "else: max_discussion_val = max_discussion\n",
        "\n",
        "if 'max_prs_opened' not in globals() or max_prs_opened == 0: max_prs_opened_val = 1\n",
        "else: max_prs_opened_val = max_prs_opened\n",
        "\n",
        "if 'max_code_changes' not in globals() or max_code_changes == 0: max_code_changes_val = 1\n",
        "else: max_code_changes_val = max_code_changes\n",
        "\n",
        "if 'max_prs_merged' not in globals() or max_prs_merged == 0: max_prs_merged_val = 1\n",
        "else: max_prs_merged_val = max_prs_merged\n",
        "\n",
        "# Ensure A_factor, R_factor, S_factor are available\n",
        "if 'A_factor' not in globals(): A_factor = 0.5\n",
        "if 'R_factor' not in globals(): R_factor = 0.5\n",
        "if 'S_factor' not in globals(): S_factor = 0.5\n",
        "\n",
        "# Defensive check: Ensure daily_additions and daily_deletions are dictionaries\n",
        "# This guards against cases where prior cells might not have been run or\n",
        "# if they were inadvertently reassigned as integers.\n",
        "if not isinstance(daily_additions, dict):\n",
        "    daily_additions = {}\n",
        "if not isinstance(daily_deletions, dict):\n",
        "    daily_deletions = {}\n",
        "\n",
        "# Fallback values for theoretical maximums, if not defined globally\n",
        "if 'active_contributors_for_em_count' not in globals() or active_contributors_for_em_count == 0:\n",
        "    active_contributors_for_em_count_val = 1\n",
        "else:\n",
        "    active_contributors_for_em_count_val = active_contributors_for_em_count\n",
        "\n",
        "if 'max_theoretical_project_em_raw' not in globals() or max_theoretical_project_em_raw == 0:\n",
        "    max_theoretical_project_em_raw_val = active_contributors_for_em_count_val * np.sqrt(2)\n",
        "else:\n",
        "    max_theoretical_project_em_raw_val = max_theoretical_project_em_raw\n",
        "\n",
        "if 'active_contributors_for_dm_count' not in globals() or active_contributors_for_dm_count == 0:\n",
        "    active_contributors_for_dm_count_val = 1\n",
        "else:\n",
        "    active_contributors_for_dm_count_val = active_contributors_for_dm_count\n",
        "\n",
        "if 'max_theoretical_project_dm_raw' not in globals() or max_theoretical_project_dm_raw == 0:\n",
        "    max_theoretical_project_dm_raw_val = active_contributors_for_dm_count_val * np.sqrt(2)\n",
        "else:\n",
        "    max_theoretical_project_dm_raw_val = max_theoretical_project_dm_raw\n",
        "\n",
        "\n",
        "for current_date in all_dates:\n",
        "    # Get daily event counts\n",
        "    daily_issues_raised = daily_issues_raised_count.get(current_date, 0)\n",
        "    daily_issue_comments = daily_issue_comment_count.get(current_date, 0)\n",
        "    daily_prs_opened = daily_prs_opened_count.get(current_date, 0)\n",
        "    daily_prs_merged = daily_prs_merged_count.get(current_date, 0)\n",
        "\n",
        "    # Use distinct variable names to avoid overwriting the global dictionaries\n",
        "    current_day_additions = daily_additions.get(current_date, 0)\n",
        "    current_day_deletions = daily_deletions.get(current_date, 0)\n",
        "\n",
        "    # Calculate daily total discussion and code changes\n",
        "    daily_total_discussion = daily_issues_raised + daily_issue_comments\n",
        "    daily_total_code_changes = current_day_additions + current_day_deletions\n",
        "\n",
        "    # Normalize daily EM components by global individual maximums\n",
        "    daily_normalized_discussion = daily_total_discussion / max_discussion_val\n",
        "    daily_normalized_prs_opened = daily_prs_opened / max_prs_opened_val\n",
        "\n",
        "    # Calculate daily EM (scaled 0-1) using the theoretical project maximum\n",
        "    daily_em_raw = np.sqrt(daily_normalized_discussion**2 + daily_normalized_prs_opened**2)\n",
        "    daily_em = 0.0\n",
        "    if max_theoretical_project_em_raw_val > 0:\n",
        "        daily_em = daily_em_raw / max_theoretical_project_em_raw_val\n",
        "    # Ensure it's capped at 1.0 to handle extreme cases, though with correct scaling it should be <= 1\n",
        "    daily_em = min(daily_em, 1.0)\n",
        "\n",
        "    # Normalize daily DM components by global individual maximums\n",
        "    daily_normalized_code_changes = daily_total_code_changes / max_code_changes_val\n",
        "    daily_normalized_prs_merged = daily_prs_merged / max_prs_merged_val\n",
        "\n",
        "    # Calculate daily DM (scaled 0-1) using the theoretical project maximum\n",
        "    daily_dm = 0.0\n",
        "    daily_dm_raw = np.sqrt(daily_normalized_code_changes**2 + daily_normalized_prs_merged**2)\n",
        "    if max_theoretical_project_dm_raw_val > 0:\n",
        "        daily_dm = daily_dm_raw / max_theoretical_project_dm_raw_val\n",
        "    # Ensure it's capped at 1.0\n",
        "    daily_dm = min(daily_dm, 1.0)\n",
        "\n",
        "    # Calculate daily Activity\n",
        "    daily_activity = A_factor * (1 if daily_em > 0 else 0) + (1 - A_factor) * (1 if daily_dm > 0 else 0)\n",
        "\n",
        "    # Calculate daily Reward\n",
        "    daily_reward = R_factor * daily_em + (1 - R_factor) * daily_dm\n",
        "\n",
        "    # Calculate daily Contribution Score (CS)\n",
        "    daily_cs = S_factor * daily_activity + (1 - S_factor) * daily_reward\n",
        "\n",
        "    # Calculate the four individual components of the daily score\n",
        "    daily_score_component_activity_em_present = S_factor * A_factor * (1 if daily_em > 0 else 0)\n",
        "    daily_score_component_activity_dm_present = S_factor * (1 - A_factor) * (1 if daily_dm > 0 else 0)\n",
        "    daily_score_component_reward_em_magnitude = (1 - S_factor) * R_factor * daily_em\n",
        "    daily_score_component_reward_dm_magnitude = (1 - S_factor) * (1 - R_factor) * daily_dm\n",
        "\n",
        "    daily_project_metrics.append({\n",
        "        'Date': current_date,\n",
        "        'Activity (EM Present)': daily_score_component_activity_em_present,\n",
        "        'Activity (DM Present)': daily_score_component_activity_dm_present,\n",
        "        'Reward (EM Magnitude)': daily_score_component_reward_em_magnitude,\n",
        "        'Reward (DM Magnitude)': daily_score_component_reward_dm_magnitude,\n",
        "        'Score': daily_cs # Changed 'CS' to 'Score'\n",
        "    })\n",
        "\n",
        "df_daily_project_metrics = pd.DataFrame(daily_project_metrics)\n",
        "df_daily_project_metrics['Date'] = pd.to_datetime(df_daily_project_metrics['Date'])\n",
        "\n",
        "# Melt the DataFrame for line chart visualization\n",
        "df_daily_melted_metrics = df_daily_project_metrics.melt(\n",
        "    id_vars=['Date'],\n",
        "    value_vars=['Activity (EM Present)', 'Activity (DM Present)', 'Reward (EM Magnitude)', 'Reward (DM Magnitude)', 'Score'], # Changed 'CS' to 'Score'\n",
        "    var_name='Metric',\n",
        "    value_name='Value'\n",
        ")\n",
        "\n",
        "# Create the smoothed line chart using Plotly Express\n",
        "fig = px.line(\n",
        "    df_daily_melted_metrics,\n",
        "    x='Date',\n",
        "    y='Value',\n",
        "    color='Metric',\n",
        "    title='Daily Evolution of Project Metrics',\n",
        "    labels={'Value': 'Metric Value', 'Date': 'Date'},\n",
        "    hover_data={'Value': ':.2f'},\n",
        "    line_shape='spline'\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='Metric Value',\n",
        "    legend_title='Metric',\n",
        "    hovermode='x unified'\n",
        "    # Removed yaxis_range=[0, 1.1] to allow automatic adjustment\n",
        ")\n",
        "\n",
        "fig.update_xaxes(dtick=\"D1\", tickformat=\"%b %d\") # Daily ticks, format as \"Month Day\"\n",
        "\n",
        "fig.show()\n",
        "\n",
        "save_fig(fig, 'daily_project_metrics_evolution.html')\n",
        "\n",
        "print(\"Daily project metrics evolution chart generated.\")"
      ],
      "metadata": {
        "id": "5JBXWhbW40MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Score percentiles"
      ],
      "metadata": {
        "id": "owgCNExyL772"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "# Filter out contributors with a score of 0 for a more meaningful quartile analysis\n",
        "df_active_scores = df_people[df_people['score'] > 0].copy()\n",
        "\n",
        "if df_active_scores.empty:\n",
        "    print(\"No contributors with a score > 0 found to plot quartiles.\")\n",
        "else:\n",
        "    # Create a box plot for the 'score'\n",
        "    fig = px.box(\n",
        "        df_active_scores,\n",
        "        y='score',\n",
        "        title='Contribution score percentiles',\n",
        "        labels={\n",
        "            'score': 'Contribution Score'\n",
        "        },\n",
        "        hover_data={'github_login': True, 'name': True, 'score': ':.2f'}\n",
        "    )\n",
        "\n",
        "    # Customize layout for better readability\n",
        "    fig.update_layout(\n",
        "        yaxis_title='Score',\n",
        "        yaxis_range=[0, 1], # Ensure y-axis covers the full score range from 0 to 1\n",
        "        yaxis=dict(dtick=0.1), # Set grid lines every 0.1 units\n",
        "        boxmode='overlay' # Overlay points if multiple boxes (not applicable for single score column)\n",
        "    )\n",
        "\n",
        "    # Add individual data points as a scatter plot overlay (optional, for detail)\n",
        "    fig.add_trace(px.scatter(\n",
        "        df_active_scores,\n",
        "        y='score',\n",
        "        hover_name='name',\n",
        "        hover_data={'github_login': True, 'name': True, 'score': ':.2f'}\n",
        "    ).data[0])\n",
        "\n",
        "    fig.show()\n",
        "    save_fig(fig, 'score_percentiles_box_plot.html')\n",
        "\n",
        "    print(\"Box plot for contributor scores generated.\")\n",
        "\n",
        "    # Print quartile values with explanations\n",
        "    q1 = df_active_scores['score'].quantile(0.25)\n",
        "    median = df_active_scores['score'].quantile(0.50)\n",
        "    q3 = df_active_scores['score'].quantile(0.75)\n",
        "    max_score = df_active_scores['score'].max()\n",
        "    min_score = df_active_scores['score'].min()\n",
        "\n",
        "    print(\"\\n--- Contributor Score Quartiles ---\")\n",
        "    print(f\"* The lowest active score is: {min_score:.2f}\")\n",
        "    print(f\"* 25% of contributors have a score up to: {q1:.2f}\")\n",
        "    print(f\"* 50% of contributors (the median) have a score up to: {median:.2f}\")\n",
        "    print(f\"* 75% of contributors have a score up to: {q3:.2f}\")\n",
        "    print(f\"* The highest active score is: {max_score:.2f}\")\n"
      ],
      "metadata": {
        "id": "ZwrwQ2Fc8zQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Score histogram"
      ],
      "metadata": {
        "id": "LTb600wcMC9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "# Filter out contributors with a score of 0 for a more meaningful histogram\n",
        "df_active_scores_hist = df_people[df_people['score'] > 0].copy()\n",
        "\n",
        "if df_active_scores_hist.empty:\n",
        "    print(\"No contributors with a score > 0 found to plot a histogram.\")\n",
        "else:\n",
        "    # Create a histogram for the 'score'\n",
        "    fig = px.histogram(\n",
        "        df_active_scores_hist,\n",
        "        x='score',\n",
        "        nbins=20, # Adjust number of bins as needed\n",
        "        title='Contribution score histogram',\n",
        "        labels={\n",
        "            'score': 'Contribution Score',\n",
        "            'count': 'Number of Contributors'\n",
        "        },\n",
        "        hover_data={'name': True, 'score': ':.2f'}\n",
        "    )\n",
        "\n",
        "    # Customize layout for better readability\n",
        "    fig.update_layout(\n",
        "        xaxis_title='Contribution Score',\n",
        "        yaxis_title='Number of Contributors',\n",
        "        xaxis_range=[0, 1], # Ensure x-axis covers the full score range\n",
        "        bargap=0.1 # Gap between bars\n",
        "    )\n",
        "\n",
        "    # Calculate mean and median\n",
        "    mean_score = df_active_scores_hist['score'].mean()\n",
        "    median_score = df_active_scores_hist['score'].median()\n",
        "\n",
        "    # Add mean line\n",
        "    fig.add_vline(x=mean_score, line_width=2, line_dash=\"dash\", line_color=\"red\",\n",
        "                  annotation_text=f\"Mean: {mean_score:.2f}\",\n",
        "                  annotation_position=\"top right\")\n",
        "\n",
        "    # Add median line\n",
        "    fig.add_vline(x=median_score, line_width=2, line_dash=\"dot\", line_color=\"green\",\n",
        "                  annotation_text=f\"Median: {median_score:.2f}\",\n",
        "                  annotation_position=\"top left\")\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "    print(\"Histogram for contributor scores generated.\")\n",
        "    save_fig(fig, 'score_histogram.html')\n"
      ],
      "metadata": {
        "id": "QoHhfca4-9Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Score proportions"
      ],
      "metadata": {
        "id": "fwIf_NaqMKyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cellname : scopre-pizza\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Categorize the 'score' column with new names\n",
        "def categorize_score(score):\n",
        "    if score == 0:\n",
        "        return 'Zero (score = 0)'\n",
        "    elif 0 < score < 0.5:\n",
        "        return 'Low (0 < score < 0.5)'\n",
        "    else:\n",
        "        return 'Good (0.5 <= score)'\n",
        "\n",
        "df_people['score_category'] = df_people['score'].apply(categorize_score)\n",
        "\n",
        "# 2. Count the occurrences in each category\n",
        "score_counts = df_people['score_category'].value_counts().reset_index()\n",
        "score_counts.columns = ['Category', 'Count']\n",
        "\n",
        "# 3. Define custom colors for the categories\n",
        "category_colors = {\n",
        "    'Zero (score = 0)': '#aaaa80',       # Lighter gray\n",
        "    'Low (0 < score < 0.5)': 'darkred',    # Darker red\n",
        "    'Good (0.5 <= score)': 'darkcyan'            # Cyan\n",
        "}\n",
        "\n",
        "# Ensure category order for consistent plotting\n",
        "category_order = ['Good (0.5 <= score)', 'Low (0 < score < 0.5)', 'Zero (score = 0)']\n",
        "score_counts['Category'] = pd.Categorical(score_counts['Category'], categories=category_order, ordered=True)\n",
        "score_counts = score_counts.sort_values('Category')\n",
        "\n",
        "# 5. Create the pie chart\n",
        "fig = px.pie(\n",
        "    score_counts,\n",
        "    values='Count',\n",
        "    names='Category', # Use the raw category name for slice identification\n",
        "    title='Contribution score ranges',\n",
        "    color='Category',\n",
        "    color_discrete_map=category_colors,\n",
        "    hover_data=['Count', 'Category']\n",
        ")\n",
        "\n",
        "# Update traces to show percentage and formatted name as text, outside\n",
        "fig.update_traces(textposition='outside', textinfo='percent', texttemplate=\"<b>%{label}</b><br>%{percent:.1%}<br>(%{value})\", textfont_color='black')\n",
        "\n",
        "fig.show()\n",
        "save_fig(fig, 'score_proportions_pie_chart.html')\n"
      ],
      "metadata": {
        "id": "oSlIzi-j_bOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Indices distribution"
      ],
      "metadata": {
        "id": "4gRWxSFkJkEh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVns6OnoJu2u"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "\n",
        "# Prepare the data for quartile analysis\n",
        "# Use the original activity and reward values, which are already normalized 0-1\n",
        "df_people_quartiles = df_people.copy()\n",
        "\n",
        "# Select the relevant columns for the box plots\n",
        "metrics_for_quartiles = df_people_quartiles[['github_login', 'EM', 'DM', 'activity', 'reward']]\n",
        "\n",
        "# Filter out contributors who have 0 across all these metrics for meaningful quartiles\n",
        "df_active_metrics = metrics_for_quartiles[\n",
        "    (metrics_for_quartiles['EM'] > 0) |\n",
        "    (metrics_for_quartiles['DM'] > 0) |\n",
        "    (metrics_for_quartiles['activity'] > 0) |\n",
        "    (metrics_for_quartiles['reward'] > 0)\n",
        "].copy()\n",
        "\n",
        "if df_active_metrics.empty:\n",
        "    print(\"No active contributors found to display quartile distributions.\")\n",
        "else:\n",
        "    # Melt the DataFrame to a long format suitable for px.box for side-by-side plots\n",
        "    df_melted_metrics = df_active_metrics.melt(\n",
        "        id_vars=['github_login'],\n",
        "        value_vars=['EM', 'DM', 'activity', 'reward'],\n",
        "        var_name='Metric',\n",
        "        value_name='Value'\n",
        "    )\n",
        "\n",
        "    # Rename metrics for better display in the plot\n",
        "    df_melted_metrics['Metric'] = df_melted_metrics['Metric'].replace({\n",
        "        'EM': 'EM (Engagement)',\n",
        "        'DM': 'DM (Delivery)',\n",
        "        'activity': 'Activity',\n",
        "        'reward': 'Reward'\n",
        "    })\n",
        "\n",
        "    # Define explicit order for metrics for consistent plotting\n",
        "    metric_order = ['EM (Engagement)', 'DM (Delivery)', 'Activity', 'Reward']\n",
        "    df_melted_metrics['Metric'] = pd.Categorical(df_melted_metrics['Metric'], categories=metric_order, ordered=True)\n",
        "\n",
        "    # Create the box plot\n",
        "    fig = px.box(df_melted_metrics,\n",
        "                 x='Metric',\n",
        "                 y='Value',\n",
        "                 color='Metric', # Color boxes by metric type\n",
        "                 title='Quartile distribution of tide metrics',\n",
        "                 labels={'Metric': 'Metric', 'Value': 'Value'}, # Updated labels to English\n",
        "                 hover_data={'github_login': True, 'Value': ':.2f'},\n",
        "                 points='all' # Display all individual points\n",
        "                )\n",
        "\n",
        "    # Customize layout\n",
        "    fig.update_layout(\n",
        "        xaxis_title='Metric',\n",
        "        yaxis_title='Value',\n",
        "        xaxis_tickangle=-45,\n",
        "        height=600,\n",
        "        showlegend=False, # Legend not needed as colors are explained by x-axis\n",
        "        yaxis_range=[0, 1], # Set y-axis range to 0-1\n",
        "        yaxis=dict(dtick=0.1)\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "    save_fig(fig, 'metric_quartiles_box_plot.html')\n",
        "\n",
        "    print(\"\\n--- Quartile Values for Each Metric ---\")\n",
        "    for metric in ['EM', 'DM', 'activity', 'reward']:\n",
        "        series = df_active_metrics[metric]\n",
        "        if not series.empty:\n",
        "            q1 = series.quantile(0.25)\n",
        "            median = series.quantile(0.50)\n",
        "            q3 = series.quantile(0.75)\n",
        "            max_val = series.max()\n",
        "            min_val = series.min()\n",
        "            mean_val = series.mean()\n",
        "\n",
        "            print(f\"\\nMetric: {metric.replace('EM', 'Engagement Magnitude').replace('DM', 'Delivery Magnitude')}\")\n",
        "            print(f\"  Mean: {mean_val:.2f}\")\n",
        "            print(f\"  Min: {min_val:.2f}\")\n",
        "            print(f\"  Q1 (25th percentile): {q1:.2f}\")\n",
        "            print(f\"  Median (50th percentile): {median:.2f}\")\n",
        "            print(f\"  Q3 (75th percentile): {q3:.2f}\")\n",
        "            print(f\"  Max: {max_val:.2f}\")\n",
        "        else:\n",
        "            print(f\"\\nMetric: {metric.replace('EM', 'Engagement Magnitude').replace('DM', 'Delivery Magnitude')} - No active data.\")\n"
      ],
      "metadata": {
        "id": "Iyr7mFnRPFaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OTHER INFORMATION"
      ],
      "metadata": {
        "id": "rwx4NtErqm9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inactive developers\n",
        "\n",
        "* Devs that didn't contribute dureing the period"
      ],
      "metadata": {
        "id": "hJsOxwYrxDHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Define the columns that represent contribution events\n",
        "contribution_columns = [\n",
        "    'issues_opened_count',\n",
        "    'issues_reopened_count',\n",
        "    'issue_comment_event_count',\n",
        "    'pull_requests_opened_count',\n",
        "    'pull_requests_merged_count',\n",
        "    'pull_requests_closed_count',\n",
        "    'total_additions',\n",
        "    'total_deletions'\n",
        "]\n",
        "\n",
        "# Ensure all contribution columns exist in df_people\n",
        "existing_contribution_columns = [col for col in contribution_columns if col in df_people.columns]\n",
        "\n",
        "# Calculate the total contributions for each person\n",
        "# Summing across relevant columns for each row\n",
        "df_people['total_contributions'] = df_people[existing_contribution_columns].sum(axis=1)\n",
        "\n",
        "# Filter for developers with zero total contributions\n",
        "inactive_devs_df = df_people[df_people['total_contributions'] == 0]\n",
        "\n",
        "# Extract logins of inactive developers for potential future use\n",
        "inactive_dev_logins = inactive_devs_df['github_login'].tolist()\n",
        "\n",
        "if not inactive_devs_df.empty:\n",
        "    display(Markdown(f\"**{len(inactive_devs_df)} Developer(s) with No Contributions in the Period:**\"))\n",
        "    for index, row in inactive_devs_df.iterrows():\n",
        "        dev_name = row['name'] if row['name'] != 'N/A' else 'Name not available'\n",
        "        display(Markdown(f\"- {row['github_login']} ({dev_name})\"))\n",
        "else:\n",
        "    display(Markdown(\"**All developers have made at least one contribution in the selected period!**\"))\n",
        "\n",
        "# Clean up the temporary column\n",
        "df_people.drop(columns=['total_contributions'], inplace=True, errors='ignore')\n"
      ],
      "metadata": {
        "id": "VlRkZekRxIgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVE DATAFRAME"
      ],
      "metadata": {
        "id": "RUW6B0WSYNS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Ensure output_folder and df_people are defined from previous cells\n",
        "\n",
        "output_csv_path = os.path.join(output_folder, 'developers_data.csv')\n",
        "df_people.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"DataFrame 'df_people' successfully saved to {output_csv_path}\")"
      ],
      "metadata": {
        "id": "wDyjsPjMYPcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATE REPORT"
      ],
      "metadata": {
        "id": "n2ibWwvZIh3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section descriptions"
      ],
      "metadata": {
        "id": "8yP1L3_DX17J"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "report_descriptions"
      },
      "source": [
        "# --- Report Description Variables ---\n",
        "\n",
        "# Project Summary\n",
        "summary_description = \"This section provides a high-level overview of the GitHub repository, including its basic metadata and general statistics.\"\n",
        "\n",
        "# Project Overview\n",
        "overview_description = \"This section summarizes the key demographic data for contributors and collaborators within the analyzed period.\"\n",
        "\n",
        "# Inactive Developers\n",
        "inactive_devs_description = \"This section lists developers who are either collaborators or external contributors but did not record any activity (issues opened/reopened, comments, PRs opened/merged, code changes) within the selected analysis period.\"\n",
        "inactive_devs_found_text = \"Developer(s) with No Contributions in the Period:\"\n",
        "inactive_devs_none_text = \"All developers have made at least one contribution in the selected period!\"\n",
        "\n",
        "# All Primitive Events\n",
        "primitive_events_description = \"This chart displays the raw count of various GitHub events (issues opened, issues reopened, comments, PRs opened, PRs merged, lines added, lines removed) per contributor. It provides a granular view of individual developer activity.\"\n",
        "\n",
        "# All Aggregate Events\n",
        "aggregate_events_description = \"This chart aggregates primitive events into broader categories: 'Issues Raised' (opened + reopened), 'Comments' (on issues/PRs), 'PRs Opened', 'PRs Merged', and 'Code Changes' (additions + deletions). It offers a summarized view of contribution types.\"\n",
        "\n",
        "# WB\n",
        "wbi_description = \"The WB (Workload Balance), calculated as (PRs Closed - PRs Opened) / (PRs Closed + PRs Opened), measures the balance between new work introduced (PRs opened) and work completed (PRs closed) in the project. A positive value indicates clearing the backlog, while a negative value suggests accumulation.\"\n",
        "\n",
        "# RB\n",
        "rbi_description = \"The RB (Resolution Balance), calculated as (PRs Submitted - Issues Raised) / (PRs Submitted + Issues Raised), reflects the balance between problem identification (issues raised) and solution implementation (PRs submitted). A positive value implies a focus on implementation, while a negative value suggests more issues are being identified than solutions proposed.\"\n",
        "\n",
        "# Project Performance Map\n",
        "performance_map_description = \"This map plots the Workload Balance (WB) against the Resolution Balance (RB) on a polar coordinate system. It visually represents the overall project dynamics, indicating whether the team is accumulating/clearing work and focusing on planning/implementing tasks.\"\n",
        "\n",
        "# PRs Close Time section\n",
        "pr_close_time_description = \"This section analyzes the lifecycle of Pull Requests, specifically focusing on the time taken to close them. It categorizes PRs based on their creation and closure dates relative to the analysis period.\"\n",
        "\n",
        "# Developers Overview - Engagement\n",
        "engagement_plot_description = \"This plot visualizes individual contributor engagement by mapping 'PRs Opened' against 'Discussion' (Issues Opened + Comments). It helps identify developers who are more conversational versus those focused on initiating code contributions.\"\n",
        "remi_index_description = \"EM quantifies a developer's overall engagement by taking the Euclidean norm of their normalized 'Discussion' and 'PRs Opened' scores. It intuitively measures the intensity of a developer's participatory and initiatory actions, ranging from 0 (no engagement) to 1 (maximum engagement).\"\n",
        "\n",
        "# Developers Overview - Delivery\n",
        "delivery_plot_description = \"This plot visualizes individual contributor delivery by mapping 'Code Changes' (lines added + deleted) against 'PRs Merged'. It helps identify developers who are generating significant code volume versus those focused on successfully integrating code into the main branch.\"\n",
        "rdmi_index_description = \"DM quantifies a developer's overall delivery by taking the Euclidean norm of their normalized 'Code Changes' and 'PRs Merged' scores. It intuitively measures the overall strength of a developer's effective code delivery, ranging from 0 (no delivery) to 1 (maximum delivery).\"\n",
        "\n",
        "# Engagement-Delivery Map\n",
        "engagement_delivery_map_description = \"This map plots each developer's EM against their DM score. It categorizes developers based on their relative engagement and delivery levels, helping identify different contributor profiles (e.g., highly engaged/low delivery vs. low engagement/high delivery).\"\n",
        "\n",
        "# Contribution (EM+DM) Plot\n",
        "contribution_plot_description = \"This stacked bar chart displays each developer's EM and DM scores side-by-side. It offers a combined view of individual engagement and delivery magnitudes, allowing for quick comparison across contributors.\"\n",
        "\n",
        "# Score Plot\n",
        "score_plot_description = \"The overall score for each contributor is calculated as a weighted sum of 'Activity' and 'Reward', using the formula `S * Activity + (1 - S) * Reward`. This provides a comprehensive measure between 0 and 1. 'Activity' measures minimal participation, while 'Reward' measures participation intensity based on EM and DM. This chart displays the stacked components of 'Activity' and 'Reward' contributing to the total score.\"\n",
        "score_percentiles_description = \"This box plot visualizes the distribution of individual contributor scores, showing quartiles (25th, 50th, 75th percentiles), minimum, and maximum values. It helps in understanding the spread and central tendency of scores across active developers.\"\n",
        "score_histogram_description = \"This histogram shows the frequency distribution of contributor scores, indicating how many developers fall within specific score ranges. It helps identify patterns in scoring, such as concentration around certain values or presence of multiple peaks.\"\n",
        "score_proportions_description = \"This pie chart categorizes contributors into three score ranges: 'Zero' (score = 0), 'Low' (0 < score < 0.5), and 'Good' (0.5 <= score). It illustrates the proportion of developers in each category, providing a high-level overview of the team's overall performance distribution.\"\n",
        "\n",
        "# Aggregate Indices\n",
        "metric_quartiles_description = \"This box plot displays the quartile distribution (25th, 50th, 75th percentiles) of individual EM, DM, Activity, and Reward scores across all active contributors. It provides insights into the spread and central tendency of these key metrics among developers.\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report structure"
      ],
      "metadata": {
        "id": "vQjWJKeRXws7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bd54d58"
      },
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Ensure output_folder is defined from previous cells\n",
        "# output_folder = '/content/gdrive/My Drive/devstats_report' # Uncomment if running this cell independently\n",
        "\n",
        "html_content_parts = []\n",
        "\n",
        "# Ensure analysis_run_date is available. It should be defined in a preceding cell.\n",
        "# Add a safety check for analysis_run_date\n",
        "if 'analysis_run_date' not in globals():\n",
        "    analysis_run_date = \"Analysis date not available - please run the 'Get repository info' cell.\" # Fallback\n",
        "    print(\"Warning: 'analysis_run_date' not found. Using fallback message.\")\n",
        "\n",
        "print(f\"Using analysis_run_date for report: {analysis_run_date}\")\n",
        "\n",
        "# 1. HTML Head and Styles\n",
        "html_content_parts.append(f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Project Report: {repo.name}</title>\n",
        "    <style>\n",
        "        body {{\n",
        "            font-family: 'Arial', sans-serif;\n",
        "            margin: 0;\n",
        "            padding: 20px;\n",
        "            background-color: #f4f7f6;\n",
        "            color: #333;\n",
        "            line-height: 1.6;\n",
        "        }}\n",
        "        .container {{\n",
        "            max-width: 1200px;\n",
        "            margin: 20px auto;\n",
        "            background-color: #ffffff;\n",
        "            padding: 30px;\n",
        "            border-radius: 8px;\n",
        "            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n",
        "        }}\n",
        "        h1, h2, h3, h4, h5, h6 {{\n",
        "            color: #2c3e50;\n",
        "        }}\n",
        "        h1 {{\n",
        "            text-align: center;\n",
        "            color: #2196f3;\n",
        "            margin-bottom: 30px;\n",
        "        }}\n",
        "        .section {{\n",
        "            margin-bottom: 40px;\n",
        "            border-bottom: 1px solid #eee;\n",
        "            padding-bottom: 20px;\n",
        "        }}\n",
        "        .section:last-child {{\n",
        "            border-bottom: none;\n",
        "        }}\n",
        "        .figure-container {{\n",
        "            max-width: 1000px; /* Increased max-width for centering wider iframes */\n",
        "            margin: 20px auto; /* Center the figure container itself */\n",
        "            text-align: center; /* Center inline content like h3 */\n",
        "            border: 1px solid #ddd;\n",
        "            padding: 15px;\n",
        "            border-radius: 5px;\n",
        "            background-color: #f9f9f9;\n",
        "        }}\n",
        "        .figure-container h3 {{\n",
        "            margin-top: 0;\n",
        "            color: #555;\n",
        "        }}\n",
        "        .figure-container iframe {{\n",
        "            /* Removed default width: 100%; to allow individual width or auto-sizing */\n",
        "            height: 750px; /* Adjusted default height for plotly figures to prevent flattening */\n",
        "            width: 100%; /* Allow iframe to fill its container */\n",
        "            border: none;\n",
        "            display: block; /* Treat iframe as a block element */\n",
        "            margin: 0 auto; /* Center iframe horizontally */\n",
        "            text-align: center; /* Center iframe text */\n",
        "        }}\n",
        "\n",
        "        .subsection {{\n",
        "            margin-bottom: 20px;\n",
        "            border-bottom: 1px solid #eee;\n",
        "            padding-bottom: 15px;\n",
        "        }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <h1>Project Report: {repo.name}</h1>\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Calculate total contributors\n",
        "total_contributors = num_core_devs + num_externals\n",
        "\n",
        "# 2. Project Overview (moved before Project Summary)\n",
        "html_content_parts.append(f\"\"\"\n",
        "        <div class=\"section\">\n",
        "            <h2>Analysis Period</h2>\n",
        "            <p>{overview_description}</p>\n",
        "            <p><b>Acquisition Date:</b> {analysis_run_date}</p>\n",
        "            <p><b>Interval length:</b> {selected_period_str}</p>\n",
        "        </div>\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# 3. Project Summary (moved after Project Overview)\n",
        "html_content_parts.append(f\"\"\"\n",
        "        <div class=\"section\">\n",
        "            <h2>Project Summary</h2>\n",
        "            <p>{summary_description}</p>\n",
        "            <p><b>Repository Name:</b> {repo.name}</p>\n",
        "            <p><b>Description:</b> {repo.description if repo.description else 'No description provided.'}</p>\n",
        "            <p><b>URL:</b> <a href=\"{repo.html_url}\">{repo.html_url}</a></p>\n",
        "            <p><b>Stars:</b> {repo.stargazers_count}</p>\n",
        "            <p><b>Forks:</b> {repo.forks_count}</p>\n",
        "            <p><b>Created At:</b> {repo.created_at.strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
        "            <p><b>Last Updated At:</b> {repo.updated_at.strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
        "            <p><b>Number of contributors:</b> {total_contributors}</p>\n",
        "            <ul>\n",
        "                <li>Project maintainers: {num_core_devs}</li>\n",
        "                <li>External contributors: {num_externals}</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# 5. All Primitive Events\n",
        "html_content_parts.append(f\"\"\"\n",
        "        <div class=\"section\">\n",
        "            <h2>Events</h2>\n",
        "            <p>{primitive_events_description}</p>\n",
        "            <div class=\"figure-container\">\n",
        "                <iframe src=\"./all_primitive_events.html\"></iframe>\n",
        "            </div>\n",
        "        </div>\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# 6. All Aggregate Events\n",
        "html_content_parts.append(f\"\"\"\n",
        "        <div class=\"section\">\n",
        "            <h2>Categorized events</h2>\n",
        "            <p>{aggregate_events_description}</p>\n",
        "            <div class=\"figure-container\">\n",
        "                <iframe src=\"./all_aggregate_events.html\"></iframe>\n",
        "            </div>\n",
        "        </div>\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# 7. WB\n",
        "html_content_parts.append(f\"\"\"\n",
        "        <div class=\"section\">\n",
        "            <h2>Workload Balance (WB)</h2>\n",
        "            <p>{wbi_description}</p>\n",
        "            <div class=\"figure-container\">\n",
        "                <iframe src=\"./workload_balance.html\" style=\"height: 200px; width: 880px;\"></iframe>\n",
        "            </div>\n",
        "        </div>\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# 8. RB\n",
        "html_content_parts.append(f\"\"\"\n",
        "        <div class=\"section\">\n",
        "            <h2>Resolution Balance (RB)</h2>\n",
        "            <p>{rbi_description}</p>\n",
        "            <div class=\"figure-container\">\n",
        "                <iframe src=\"./resolution_balance.html\" style=\"height: 200px; width: 880px;\"></iframe>\n",
        "            </div>\n",
        "        </div>\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# 9. Project Performance Map\n",
        "html_content_parts.append(f\"\"\"\n",
        "        <div class=\"section\">\n",
        "            <h2>Project tide chart (WB x RB)</h2>\n",
        "            <p>{performance_map_description}</p>\n",
        "            <div class=\"figure-container\">\n",
        "                <iframe src=\"./wb_rb_map.html\" style=\"height: 700px; width: 960px;\"></iframe>\n",
        "            </div>\n",
        "        </div>\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# 10. PRs Close Time section\n",
        "html_content_parts.append(f\"\"\"\n",
        "        <div class=\"section\">\n",
        "            <h2>PR completion time</h2>\n",
        "            <p>{pr_close_time_description}</p>\n",
        "            <div class=\"figure-container\">\n",
        "                <iframe src=\"./pr_close_times_histogram.html\"></iframe>\n",
        "            </div>\n",
        "            <div class=\"figure-container\">\n",
        "                <iframe src=\"./pr_close_time_scatter_plot.html\" style=\"height: 700px;\"></iframe>\n",
        "            </div>\n",
        "            <div class=\"figure-container\">\n",
        "                <iframe src=\"./pr_categories_pie_chart.html\"></iframe>\n",
        "            </div>\n",
        "        </div>\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# 11. Developers Overview - Engagement\n",
        "html_content_parts.append(f\"\"\"\n",
        "        <div class=\"section\">\n",
        "            <h2>Developers Overview</h2>\n",
        "            <div class=\"subsection\">\n",
        "                <h3>Engagement chart</h3>\n",
        "                <p>{engagement_plot_description}</p>\n",
        "                <div class=\"figure-container\">\n",
        "                    <iframe src=\"./engagement_plot.html\" style=\"height: 700px;\"></iframe>\n",
        "                </div>\n",
        "            </div>\n",
        "            <div class=\"subsection\">\n",
        "                <h3>Engagement Magnitude Index (EM)</h3>\n",
        "                <p>{remi_index_description}</p>\n",
        "                <div class=\"figure-container\">\n",
        "                    <iframe src=\"./em_index.html\"></iframe>\n",
        "                </div>\n",
        "            </div>\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# 12. Developers Overview - Delivery\n",
        "html_content_parts.append(f\"\"\"\n",
        "            <div class=\"subsection\">\n",
        "                <h3>Delivery chart</h3>\n",
        "                <p>{delivery_plot_description}</p>\n",
        "                <div class=\"figure-container\">\n",
        "                    <iframe src=\"./delivery_plot.html\" style=\"height: 900px;\"></iframe>\n",
        "                </div>\n",
        "            </div>\n",
        "            <div class=\"subsection\">\n",
        "                <h3>Delivery Magnitude Index (DM)</h3>\n",
        "                <p>{rdmi_index_description}</p>\n",
        "                <div class=\"figure-container\">\n",
        "                    <iframe src=\"./dm_index.html\"></iframe>\n",
        "                </div>\n",
        "            </div>\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# 13. Engagement-Delivery Map\n",
        "html_content_parts.append(f\"\"\"\n",
        "            <div class=\"subsection\">\n",
        "                <h3>Contribution chart</h3>\n",
        "                <p>{engagement_delivery_map_description}</p>\n",
        "                <div class=\"figure-container\">\n",
        "                    <iframe src=\"./engagement_delivery_map.html\" style=\"height: 1000px;\"></iframe>\n",
        "                </div>\n",
        "            </div>\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# 14. Contribution (EM+DM) Plot\n",
        "html_content_parts.append(f\"\"\"\n",
        "            <div class=\"subsection\">\n",
        "                <h3>Contribution rank</h3>\n",
        "                <p>{contribution_plot_description}</p>\n",
        "                <div class=\"figure-container\">\n",
        "                    <iframe src=\"./contribution_em_dm_plot.html\" style=\"height: 700px;\"></iframe>\n",
        "                </div>\n",
        "            </div>\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# 15. Score Plot\n",
        "html_content_parts.append(f\"\"\"\n",
        "            <div class=\"subsection\">\n",
        "                <h3>Contribution score</h3>\n",
        "                <p>{score_plot_description}</p>\n",
        "                <div class=\"figure-container\">\n",
        "                    <iframe src=\"./score_plot.html\"></iframe>\n",
        "                </div>\n",
        "            </div>\n",
        "            <div class=\"subsection\">\n",
        "                <h3>Contribution score percentiles</h3>\n",
        "                <p>{score_percentiles_description}</p>\n",
        "                <div class=\"figure-container\">\n",
        "                    <iframe src=\"./score_percentiles_box_plot.html\"></iframe>\n",
        "                </div>\n",
        "            </div>\n",
        "            <div class=\"subsection\">\n",
        "                <h3>Contribution score histogram</h3>\n",
        "                <p>{score_histogram_description}</p>\n",
        "                <div class=\"figure-container\">\n",
        "                    <iframe src=\"./score_histogram.html\"></iframe>\n",
        "                </div>\n",
        "            </div>\n",
        "            <div class=\"subsection\">\n",
        "                <h3>Score ranges</h3>\n",
        "                <p>{score_proportions_description}</p>\n",
        "                <div class=\"figure-container\">\n",
        "                    <iframe src=\"./score_proportions_pie_chart.html\"></iframe>\n",
        "                </div>\n",
        "            </div>\n",
        "        </div> <!-- Close Developers Overview section -->\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# 16. Aggregate Indices Header\n",
        "html_content_parts.append(f\"\"\"\n",
        "        <div class=\"section\">\n",
        "            <h2>Indices distributions</h2>\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# 18. Quartile Distribution of Key Development Metrics\n",
        "html_content_parts.append(f\"\"\"\n",
        "            <div class=\"subsection\">\n",
        "                <p>{metric_quartiles_description}</p>\n",
        "                <div class=\"figure-container\">\n",
        "                    <iframe src=\"./metric_quartiles_box_plot.html\" style=\"height: 700px;\"></iframe>\n",
        "                </div>\n",
        "            </div>\n",
        "        </div> <!-- Close Aggregate Indices section -->\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# 19. Inactive Developers (Moved to end)\n",
        "inactive_devs_html = f\"\"\"\n",
        "        <div class=\"section\">\n",
        "            <h2>Inactive Developers</h2>\n",
        "            <p>{inactive_devs_description}</p>\n",
        "\"\"\"\n",
        "if not inactive_devs_df.empty:\n",
        "    inactive_devs_html += f\"\"\"\n",
        "            <p>{len(inactive_devs_df)} {inactive_devs_found_text}</p>\n",
        "            <ul>\n",
        "\"\"\"\n",
        "    for index, row in inactive_devs_df.iterrows():\n",
        "        dev_name = row['name'] if row['name'] != 'N/A' else 'Name not available'\n",
        "        inactive_devs_html += f\"                <li>{row['github_login']} ({dev_name})</li>\\n\"\n",
        "else:\n",
        "    inactive_devs_html += f\"            <p>{inactive_devs_none_text}</p>\\n\"\n",
        "inactive_devs_html += \"        </div>\\n\"\n",
        "html_content_parts.append(inactive_devs_html)\n",
        "\n",
        "# 20. Final closing tags\n",
        "html_content_parts.append(\"\"\"\n",
        "    </div> <!-- Close container -->\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Join all parts and write to file\n",
        "final_html_content = \"\".join(html_content_parts)\n",
        "\n",
        "html_file_path = os.path.join(output_folder, 'index.html')\n",
        "\n",
        "with open(html_file_path, 'w') as f:\n",
        "    f.write(final_html_content)\n",
        "\n",
        "print(f\"Finalized HTML report created at: {html_file_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}